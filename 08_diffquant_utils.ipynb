{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp diffquant_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_condpairname(condpair):\n",
    "    return f\"{condpair[0]}_VS_{condpair[1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_middle_elem(sorted_list):\n",
    "    nvals = len(sorted_list)\n",
    "    if nvals==1:\n",
    "        return sorted_list[0]\n",
    "    middle_idx = nvals//2\n",
    "    if nvals%2==1:\n",
    "        return sorted_list[middle_idx]\n",
    "    return 0.5* (sorted_list[middle_idx] + sorted_list[middle_idx-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "def get_nonna_array(array_w_nas):\n",
    "    res = []\n",
    "    isnan_arr = np.isnan(array_w_nas)\n",
    "\n",
    "    for idx in range(len(array_w_nas)):\n",
    "        sub_res = []\n",
    "        sub_array = array_w_nas[idx]\n",
    "        na_array = isnan_arr[idx]\n",
    "        for idx2 in range(len(sub_array)):\n",
    "            if not na_array[idx2]:\n",
    "               sub_res.append(sub_array[idx2])\n",
    "        res.append(np.array(sub_res))\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "def get_non_nas_from_pd_df(df):\n",
    "    # vals = df.values\n",
    "    # result_dict = dict()\n",
    "    # pep_names = df.index.values\n",
    "    # for pep_name, sub_vals in zip(pep_names, vals):\n",
    "    #     result_dict[pep_name] = sub_vals[~np.isnan(sub_vals)]\n",
    "    # return result_dict\n",
    "    return {\n",
    "        pep_name: sub_vals[~np.isnan(sub_vals)] for pep_name, sub_vals in\n",
    "        zip( df.index.values, df.values)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def invert_dictionary(my_map):\n",
    "    inv_map = {}\n",
    "    for k, v in my_map.iteritems():\n",
    "        inv_map[v] = inv_map.get(v, []) + [k]\n",
    "    return inv_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Parsers\n",
    "The Alphaquant pipeline is run using a generic wide-table input format, as specified in the documentation. The following parsers convert long format tables as provided e.g. by Spectronaut or DIA-NN into this generic format. The configuration for the parsers is set by a yaml file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert long format to wide format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse .yaml file\n",
    "The relevant parameters for reading and reformatting the long table are stored in the \"longtable_config.yaml\" file. The functions below are for reading and reformating the config info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import yaml\n",
    "\n",
    "def get_relevant_columns(protein_cols, ion_cols, sample_ID, quant_ID, filter_dict):\n",
    "    filtcols = []\n",
    "    for filtconf in filter_dict.values():\n",
    "        filtcols.append(filtconf.get('param'))\n",
    "    relevant_cols = protein_cols + ion_cols + [sample_ID] + [quant_ID] + filtcols\n",
    "    relevant_cols = list(set(relevant_cols)) # to remove possible redudancies\n",
    "    return relevant_cols\n",
    "\n",
    "\n",
    "def retrieve_configuration(config_yaml, input_type):\n",
    "    \"\"\"collect the relevant parameters for a given type of input file (eg. DIA-NN type)\"\"\"\n",
    "    stream = open(config_yaml, 'r')\n",
    "    config_all = yaml.safe_load(stream)\n",
    "    config_dict = config_all.get(input_type)\n",
    "    return get_config_columns(config_dict)\n",
    "\n",
    "\n",
    "def get_config_columns(config_dict):\n",
    "    protein_cols = config_dict.get(\"protein_cols\")\n",
    "    ion_cols = config_dict.get(\"ion_cols\")\n",
    "    sample_ID = config_dict.get(\"sample_ID\")\n",
    "    quant_ID = config_dict.get(\"quant_ID\")\n",
    "    filter_dict = config_dict.get(\"filters\", {})\n",
    "    relevant_cols = get_relevant_columns(protein_cols, ion_cols, sample_ID, quant_ID, filter_dict)\n",
    "    return relevant_cols, protein_cols, ion_cols, sample_ID, quant_ID, filter_dict\n",
    "\n",
    "def load_config(config_yaml):\n",
    "    stream = open(config_yaml, 'r')\n",
    "    config_all = yaml.safe_load(stream)\n",
    "    return config_all\n",
    "\n",
    "def get_type2relevant_cols(config_all):\n",
    "    type2relcols = {}\n",
    "    for type in config_all.keys():\n",
    "        config_typedict = config_all.get(type)\n",
    "        relevant_cols = get_config_columns(config_typedict)[0]\n",
    "        type2relcols[type] = relevant_cols\n",
    "    return type2relcols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and reformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def filter_input(filter_dict, input):\n",
    "    for filtname,filterconf in filter_dict.items():\n",
    "        param = filterconf.get('param')\n",
    "        comparator = filterconf.get('comparator')\n",
    "        value = filterconf.get('value')\n",
    "        \n",
    "        if comparator not in [\">\",\">=\", \"<\", \"<=\", \"==\", \"!=\"]:\n",
    "            raise TypeError(f\"cannot identify the filter comparator of {filtname} given in the longtable config yaml!\")\n",
    "\n",
    "        if comparator==\"==\":\n",
    "            input = input[input[param] ==value]\n",
    "            continue\n",
    "        try:\n",
    "            input = input.astype({f\"{param}\" : \"float\"})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if comparator==\">\":\n",
    "            input = input[input[param].astype(type(value)) >value]\n",
    "\n",
    "        if comparator==\">=\":\n",
    "            input = input[input[param].astype(type(value)) >=value]\n",
    "\n",
    "        if comparator==\"<\":\n",
    "            input = input[input[param].astype(type(value)) <value]\n",
    "\n",
    "        if comparator==\"<=\":\n",
    "            input = input[input[param].astype(type(value)) <=value]\n",
    "        \n",
    "        if comparator==\"!=\":\n",
    "            input = input[input[param].astype(type(value)) !=value]\n",
    "        \n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def merge_protein_and_ion_cols(input_df, protein_cols, ion_cols):\n",
    "    input_df['protein'] = input_df.loc[:, protein_cols].astype('string').sum(axis=1)\n",
    "    input_df['ion'] = input_df.loc[:, ion_cols].astype('string').sum(axis=1)\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def reformat_longtable_according_to_config(input_file, input_type, results_folder, config_file = \"longtable_config.yaml\", sep = \"\\t\",decimal = \".\"):\n",
    "    \"\"\"Reshape a long format proteomics results table (e.g. Spectronaut or DIA-NN) to a wide format table. \n",
    "    :param file input_file: long format proteomic results table\n",
    "    :param string input_type: the configuration key stored in the config file (e.g. \"diann_precursor\")\n",
    "    \"\"\"\n",
    "    relevant_cols, protein_cols, ion_cols, sample_ID, quant_ID, filters = retrieve_configuration(config_file, input_type)\n",
    "    \n",
    "    input_df = pd.read_csv(input_file, sep = sep, decimal=decimal, usecols= relevant_cols).drop_duplicates()\n",
    "    input_df = filter_input(filters, input_df)\n",
    "    input_df = merge_protein_and_ion_cols(input_df, protein_cols, ion_cols)\n",
    "    \n",
    "    \n",
    "    input_df = input_df.astype({f'{quant_ID}': 'float'})\n",
    "    input_reshaped = pd.pivot_table(input_df, index = ['protein', 'ion'], columns = sample_ID, values = quant_ID, fill_value=0)\n",
    "    if input_reshaped.iloc[:,0].replace(0, np.nan).median() <100: #when values are small, rescale by a constant factor to prevent rounding errors in the subsequent aq analyses\n",
    "        input_reshaped = input_reshaped *10000\n",
    "    \n",
    "    input_reshaped = input_reshaped.reset_index()\n",
    "    input_reshaped = input_reshaped.set_index(\"ion\")\n",
    "    ion_level = \"fragion\" if \"fragion\" in input_file else \"precursor\"\n",
    "    input_reshaped.to_csv(f\"{input_file}.aq_reformat.{ion_level}.tsv\", index = False, sep = \"\\t\")\n",
    "    \n",
    "    return input_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_wideformat_table(peptides_tsv, config_dict):\n",
    "    input_df = pd.read_csv(peptides_tsv,sep=\"\\t\")\n",
    "    filter_dict = config_dict.get(\"filters\")\n",
    "    protein_cols = config_dict.get(\"protein_cols\")\n",
    "    ion_cols = config_dict.get(\"ion_cols\")\n",
    "    input_df = filter_input(filter_dict, input_df)\n",
    "    input_df = merge_protein_and_ion_cols(input_df,protein_cols, ion_cols)\n",
    "    input_df = input_df.set_index(\"ion\")\n",
    "    display(input_df)\n",
    "    if 'quant_prefix' in config_dict.keys():\n",
    "        quant_prefix = config_dict.get('quant_prefix')\n",
    "        headers = ['protein'] + list(filter(lambda x: x.startswith(quant_prefix), input_df.columns))\n",
    "        input_df = input_df[headers]\n",
    "        input_df = input_df.rename(columns = lambda x : x.replace(quant_prefix, \"\"))\n",
    "\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_mq_peptides_table(peptides_tsv, pepheader = \"Sequence\", protheader = \"Leading razor protein\"):\n",
    "    peps = pd.read_csv(peptides_tsv,sep=\"\\t\")\n",
    "    peps = peps[peps[\"Reverse\"] != \"+\"]\n",
    "    peps = peps[peps[\"Potential contaminant\"] != \"+\"]\n",
    "    if pepheader != None:\n",
    "        peps = peps.rename(columns = {pepheader : \"ion\"})\n",
    "    if protheader != None:\n",
    "        peps = peps.rename(columns = {protheader: \"protein\"})\n",
    "    peps = peps.set_index(\"ion\")\n",
    "    headers = ['protein'] + list(filter(lambda x: x.startswith(\"Intensity \"), peps.columns))\n",
    "    peps = peps[headers]\n",
    "    peps = peps.rename(columns = lambda x : x.replace(\"Intensity \", \"\"))\n",
    "\n",
    "    return peps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check for already processed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "def check_for_processed_runs_in_results_folder(results_folder):\n",
    "    contained_condpairs = []\n",
    "    folder_files = os.listdir(results_folder)\n",
    "    result_files = list(filter(lambda x: \"results.tsv\" in x ,folder_files))\n",
    "    for result_file in result_files:\n",
    "        res_name = result_file.replace(\".results.tsv\", \"\")\n",
    "        if ((f\"{res_name}.normed.tsv\" in folder_files) & (f\"{res_name}.results.ions.tsv\" in folder_files)):\n",
    "            contained_condpairs.append(res_name)\n",
    "    return contained_condpairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import os\n",
    "import pkg_resources\n",
    "import pathlib\n",
    "\n",
    "def import_data(input_file, results_folder, verbose=True, dashboard=False):\n",
    "    \"\"\"\n",
    "    Function to import peptide level data. Depending on available columns in the provided file,\n",
    "    the function identifies the type of input used (e.g. Spectronaut, MaxQuant, DIA-NN), reformats if necessary\n",
    "    and returns a generic wide-format dataframe\n",
    "    :param file input_file: quantified peptide/ion -level data\n",
    "    :param file results_folder: the folder where the AlphaQuant outputs are stored\n",
    "    \"\"\"\n",
    "    config_file = os.path.join(pathlib.Path(__file__).parent.absolute(), \"..\", \"longtable_config.yaml\") #the yaml config is located one directory below the python library files\n",
    "    config_dict = load_config(config_file)\n",
    "    type2relevant_columns = get_type2relevant_cols(config_dict)\n",
    "\n",
    "    \n",
    "    file_ext = os.path.splitext(input_file)[-1]\n",
    "    if file_ext=='.csv':\n",
    "        sep=','\n",
    "    if file_ext=='.tsv':\n",
    "        sep='\\t'\n",
    "    if file_ext=='.txt':\n",
    "        sep='\\t'\n",
    "\n",
    "    if 'sep' not in locals():\n",
    "        raise TypeError(f\"neither of the file extensions (.tsv, .csv, .txt) detected for file {input_file}! Your filename has to end with one of these extensions. Please modify your file name accordingly.\")\n",
    "    \n",
    "    if \"aq_reformat\" in input_file:\n",
    "        data = pd.read_csv(input_file, sep = \"\\t\")\n",
    "        return data\n",
    "\n",
    "    uploaded_data_columns = set(pd.read_csv(input_file, sep=sep, nrows=1).columns)\n",
    "    \n",
    "    for input_type in type2relevant_columns.keys():\n",
    "\n",
    "        relevant_columns = type2relevant_columns.get(input_type)\n",
    "        relevant_columns = [x for x in relevant_columns if x] #filter None values\n",
    "        print(f\"recols\\t {relevant_columns}\")\n",
    "        if set(relevant_columns).issubset(uploaded_data_columns):\n",
    "            config_dict_type =  config_dict.get(input_type)\n",
    "            format = config_dict_type.get(\"format\")\n",
    "            if verbose:\n",
    "                print(f\"{input_type} headers in format {format} detected. Importing and re-formating.\")\n",
    "            if format == \"longtable\":\n",
    "                data = reformat_longtable_according_to_config(input_file, input_type = input_type, results_folder=results_folder, sep = sep, config_file=config_file)\n",
    "            elif format == \"widetable\":\n",
    "                data = read_wideformat_table(input_file, config_dict_type)\n",
    "            else:\n",
    "                raise Exception(\"format: not specified in longtable_config.yaml\")\n",
    "            return data\n",
    "\n",
    "    #if non of the cases match, return error\n",
    "    raise TypeError(f'Input data format for {input_file} not known.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "\n",
    "def get_samplenames(data):\n",
    "    \"\"\"extracts the names of the samples of the AQ input dataframe\"\"\"\n",
    "    names = list(data.columns)\n",
    "    names.remove('protein')\n",
    "    return names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_samplemap(samplemap_file):\n",
    "    file_ext = os.path.splitext(samplemap_file)[-1]\n",
    "    if file_ext=='.csv':\n",
    "        sep=','\n",
    "    if (file_ext=='.tsv') | (file_ext=='.txt'):\n",
    "        sep='\\t'\n",
    "    \n",
    "    if 'sep' not in locals():\n",
    "        raise TypeError(f\"neither of the file extensions (.tsv, .csv, .txt) detected for file {samplemap_file}! Your filename has to end with one of these extensions. Please modify your file name accordingly.\")\n",
    "        sep = \"\\t\"\n",
    "\n",
    "    return pd.read_csv(samplemap_file, sep = sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def prepare_loaded_tables(data_df, samplemap_df):\n",
    "    \"\"\"\n",
    "    Integrates information from the peptide/ion data and the samplemap, selects the relevant columns and log2 transforms intensities.\n",
    "    \"\"\"\n",
    "    samplemap_df = samplemap_df[samplemap_df[\"condition\"]!=\"\"] #remove rows that have no condition entry\n",
    "    filtvec_not_in_data = [(x in data_df.columns) for x in samplemap_df[\"sample\"]] #remove samples that are not in the dataframe\n",
    "    samplemap_df = samplemap_df[filtvec_not_in_data]\n",
    "    headers = ['protein'] + samplemap_df[\"sample\"].to_list()\n",
    "    \n",
    "    for sample in samplemap_df[\"sample\"]:\n",
    "        data_df[sample] = np.log2(data_df[sample].replace(0, np.nan))\n",
    "    return data_df[headers], samplemap_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python385jvsc74a57bd0dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
