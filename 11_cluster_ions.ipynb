{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp cluster_ions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import scipy.spatial.distance as distance\n",
    "import scipy.cluster.hierarchy as hierarchy\n",
    "\n",
    "def find_fold_change_clusters(typenode, diffions, normed_c1, normed_c2, ion2diffDist, p2z, deedpair2doublediffdist, pval_threshold_basis, fcfc_threshold, take_median_ion):\n",
    "    \"\"\"Compares the fold changes of the ions corresponding to the nodes that are compared and returns the set of ions with consistent fold changes.\n",
    "\n",
    "    Args:\n",
    "        diffions (list[list[ionnames]]): contains the sets of ions to be tested, for example [[fragion1_precursor1, fragion2_precursor1, fragion3_precursor1],[fragion1_precursor2],[fragion1_precursor3, fragion2_precursor3]]. The ions are assumed to be similar in type (e.g. fragment, precursor)!\n",
    "        normed_c1 (ConditionBackground): [description]\n",
    "        normed_c2 (ConditionBackground): [description]\n",
    "        ion2diffDist (dict(ion : SubtractedBackground)): [description]\n",
    "        p2z ([type]): [description]\n",
    "        deedpair2doublediffdist ([type]): [description]\n",
    "        fc_threshold (float, optional): [description]. Defaults to 0.3.\n",
    "        pval_threshold_basis (float, optional): [description]. Defaults to 0.05.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(diffions)==1:\n",
    "        typenode.num_clusters = 1\n",
    "        typenode.num_mainclusts = 1\n",
    "        typenode.frac_mainclust = 1\n",
    "        return [(typenode.children[0], 0)]\n",
    "\n",
    "    diffions_idxs = [[x] for x in range(len(diffions))]\n",
    "    diffions_fcs = get_fcs_ions(diffions)\n",
    "    #mt_corrected_pval_thresh = pval_threshold_basis/len(diffions)\n",
    "    condensed_distance_matrix = distance.pdist(diffions_idxs, lambda idx1, idx2: evaluate_distance(idx1[0], idx2[0], diffions, diffions_fcs, normed_c1, normed_c2, ion2diffDist,p2z,deedpair2doublediffdist, pval_threshold_basis, fcfc_threshold, take_median_ion))\n",
    "    after_clust = hierarchy.complete(condensed_distance_matrix)\n",
    "    clustered = hierarchy.fcluster(after_clust, 0.1, criterion='distance')\n",
    "    clustered = exchange_cluster_idxs(clustered)\n",
    "    typenode.num_clusters = len(set(clustered))\n",
    "    typenode.num_mainclusts = sum([x==0 for x in clustered])\n",
    "    typenode.frac_mainclust = typenode.num_mainclusts/len(clustered)\n",
    "    \n",
    "    childnode2clust = [(typenode.children[ion_idx],clust_idx) for ion_idx, clust_idx in zip(list(range(len(clustered))),clustered)]\n",
    "    childnode2clust = sorted(childnode2clust, key = lambda x : x[0].name) #sort list for reproducibility\n",
    "\n",
    "    return childnode2clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def exchange_cluster_idxs(fclust_output_array):\n",
    "    \"\"\"The fcluster output assigns cluster numbers to the clustered elems, e.g. [1,2,1,2,2,2].\n",
    "    This function here ensures that the numbers follow size of the cluster, e.g. [1,0,1,0,0,0]\"\"\"\n",
    "    clustnum2count = {}\n",
    "    for clustnum in fclust_output_array:\n",
    "        clustnum2count[clustnum] = clustnum2count.get(clustnum, 0)+1\n",
    "    clustnums = list(clustnum2count.keys())\n",
    "    clustnums.sort(key = lambda x : clustnum2count.get(x), reverse= True)\n",
    "    clustnum_old2clustnum_new = {clustnums[idx]: idx for idx in range(len(clustnums))}\n",
    "    return [clustnum_old2clustnum_new.get(clustnum) for clustnum in fclust_output_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideÂ§\n",
    "\n",
    "def test_exchange_cluster_idxs():\n",
    "    assert exchange_cluster_idxs([1,2,1,2,2,2]) == [1,0,1,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def decide_cluster_order(node, childnode2clust_init):\n",
    "    \"\"\"ranks the clusters from 0 to n (with 0 being the best) depending on the properties/similarities of the child nodes contained in each cluster\n",
    "    \"\"\"\n",
    "    childnode2clust = order_by_score(childnode2clust_init,node, score_mapping_function=get_score_mapping_consistency_score)\n",
    "\n",
    "    return childnode2clust\n",
    "\n",
    "def get_score_mapping_consistency_score(childnode2clust):\n",
    "    clust2score = {}\n",
    "    clust2childnodes = {}\n",
    "    for childnode,clust in childnode2clust:\n",
    "        clust2score[clust] = clust2score.get(clust, 0) + childnode.fraction_consistent*len(childnode.leaves)\n",
    "        clust2childnodes[clust] = clust2childnodes.get(clust, []) + [childnode]\n",
    "    return clust2score, clust2childnodes\n",
    "\n",
    "def get_score_mapping_num_clustelems(childnode2clust):\n",
    "    clust2score = {}\n",
    "    clust2childnodes = {}\n",
    "    for childnode,clust in childnode2clust:\n",
    "        clust2score[clust] = clust2score.get(clust, 0) +1\n",
    "        clust2childnodes[clust] = clust2childnodes.get(clust, []) + [childnode]\n",
    "    return clust2score, clust2childnodes\n",
    "\n",
    "def reformat_to_childnode2clust(id2score2childnodes):\n",
    "    childnode2clust = {}\n",
    "    for clust_idx_new in range(len(id2score2childnodes)): #the new cluster has been determined by the sorting\n",
    "        for childnode in id2score2childnodes[clust_idx_new][2]:\n",
    "            childnode2clust[childnode] = clust_idx_new\n",
    "    return childnode2clust\n",
    "\n",
    "def order_by_score(childnode2clust_init, node, score_mapping_function, sort_descending_by_score = True):\n",
    "    clust2score, clust2childnodes = score_mapping_function(childnode2clust_init)\n",
    "\n",
    "    id2score2childnodes = []\n",
    "    for clust in clust2score.keys():\n",
    "        score = clust2score.get(clust)\n",
    "        childnodes = clust2childnodes.get(clust)\n",
    "        id = childnodes[0].name\n",
    "        id2score2childnodes.append((id, score, childnodes))\n",
    "    \n",
    "    id2score2childnodes = sorted(id2score2childnodes, key= lambda x : x[0])#sort by id (to ensure reproducibility)\n",
    "    id2score2childnodes = sorted(id2score2childnodes, key= lambda x : x[1], reverse= sort_descending_by_score)#then sort by score\n",
    "    node.clustscore = id2score2childnodes[0][1] #annotate the parent of the childnodes with the score of the main cluster\n",
    "    childnode2clust = reformat_to_childnode2clust(id2score2childnodes)\n",
    "    \n",
    "    return childnode2clust\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "def get_fcs_ions(diffions):\n",
    "    fcs = np.ones(len(diffions))\n",
    "    for idx in range(len(diffions)):\n",
    "        fc_ions = np.nanmedian([ion.fc for ion in diffions[idx]])\n",
    "        fcs[idx] = fc_ions\n",
    "    return fcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import statistics\n",
    "import alphaquant.doublediff_analysis as aqdd\n",
    "import numpy as np\n",
    "def evaluate_distance(idx1, idx2, diffions, fcs, normed_c1, normed_c2, ion2diffDist, p2z, deedpair2doublediffdist, pval_threshold_basis, fcfc_threshold,  take_median_ion):\n",
    "    ions1 = [x.name for x in diffions[idx1]]\n",
    "    ions2 = [x.name for x in diffions[idx2]]\n",
    "    fc1 = fcs[idx1]\n",
    "    fc2 = fcs[idx2]\n",
    "\n",
    "    if abs((fc1-fc2)) < fcfc_threshold:\n",
    "        return 0\n",
    "\n",
    "    if take_median_ion:\n",
    "        fcs_ions1 = [x.fc for x in diffions[idx1]]\n",
    "        fcs_ions2 = [x.fc for x in diffions[idx2]]\n",
    "        idx_ions1 = np.argsort(fcs_ions1)[len(fcs_ions1)//2]\n",
    "        idx_ions2 = np.argsort(fcs_ions2)[len(fcs_ions2)//2]\n",
    "        ions1 = [ions1[idx_ions1]]\n",
    "        ions2 = [ions2[idx_ions2]]\n",
    "\n",
    "    fcfc, pval = aqdd.calc_doublediff_score(ions1, ions2, normed_c1, normed_c2,ion2diffDist,p2z, deedpair2doublediffdist)\n",
    "    if (pval<pval_threshold_basis) & (abs(fcfc) > fcfc_threshold):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group and cluster ions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import anytree\n",
    "import re\n",
    "def create_hierarchical_ion_grouping(regex_patterns, gene_name, diffions):\n",
    "    #regex patterns sorted from bottom to top in the following way list(list(tuple(pattern, name))): first instance of list represents the level of the tree, second instance represents the different nodes available on this level (for example FRgIon, MS1 are on the same level)\n",
    "\n",
    "    nodes = [anytree.Node(x.name, type = \"base\", cluster = -1, is_included = True) for x in diffions]\n",
    "\n",
    "    for level in regex_patterns:\n",
    "        name2node = {}\n",
    "        for pattern2name in level:\n",
    "            for node in nodes:\n",
    "                if (re.match(pattern2name[0], node.name)):\n",
    "                    m = re.match(pattern2name[0], node.name)\n",
    "                    matching_name = m.group(1)\n",
    "                    name2node[matching_name] = name2node.get(matching_name, anytree.Node(matching_name,  type = pattern2name[1], cluster = -1, is_included = True))\n",
    "                    parent_node = name2node.get(matching_name)\n",
    "                    node.parent = parent_node\n",
    "\n",
    "        if len(name2node.keys())>0:\n",
    "            nodes = list(name2node.values())\n",
    "\n",
    "    root_node = anytree.Node(gene_name, type = \"gene\", cluster = -1, is_included = True)\n",
    "\n",
    "    for node in nodes:\n",
    "        node.parent = root_node\n",
    "\n",
    "    return root_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def get_ionlist(type_nodes, ionname2diffion, select_mainclust_ions):\n",
    "    ionlist = []\n",
    "    node2leafs = {}\n",
    "    for node in type_nodes:\n",
    "        leafs = get_leafs(node, ionname2diffion, select_mainclust_ions)\n",
    "        if len(leafs)>0:\n",
    "            ionlist.append(leafs)\n",
    "            node2leafs[node] = leafs\n",
    "\n",
    "    return ionlist, node2leafs\n",
    "\n",
    "def get_leafs(node, ionname2diffion,select_mainclust_ions):\n",
    "    \"\"\"Returns all the leafs (i.e. ions) that belong to the node. With the option to only include those that were in the main cluster of the child nodes\n",
    "\n",
    "    Args:\n",
    "        node ([type]): [description]\n",
    "        ionname2diffion ([type]): [description]\n",
    "        select_mainclust_ions ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    if node.is_leaf:\n",
    "        return [ionname2diffion.get(node.name)]\n",
    "    leafs = []\n",
    "    for child in node.children:\n",
    "        if select_mainclust_ions & (child.cluster!=0):\n",
    "            continue\n",
    "        leafs.extend([ionname2diffion.get(x.name) for x in child.leaves if x.is_included])\n",
    "            \n",
    "    return leafs\n",
    "\n",
    "\n",
    "\n",
    "def update_nodes(type_node, typefilter, type_idx, childnode2clust):\n",
    "\n",
    "    for node in type_node.children:\n",
    "        if not node.is_included:\n",
    "            continue\n",
    "        clustid =  childnode2clust.get(node)\n",
    "        node.cluster = clustid\n",
    "        leafs_included = [x for x in type_node.leaves if x.is_included]\n",
    "        no_leafs = len(leafs_included)==0\n",
    "        wrong_cluster = (clustid!=typefilter.select_cluster[type_idx]) & (typefilter.select_cluster[type_idx] !=-1)\n",
    "        #wrong_cluster = (clustid!=type_node.mostcommon_clust) & (typefilter.select_cluster[type_idx] ==-1) #all children should belong to the most common cluster\n",
    "\n",
    "        if wrong_cluster | no_leafs:\n",
    "            exclude_node(node)\n",
    "\n",
    "    filtercrit_numclust = hasattr(type_node, \"num_clusters\") and (type_node.num_clusters > typefilter.exclude_if_more_clusters_than[type_idx])\n",
    "    filtercrit_frac_mainclust = hasattr(type_node, \"frac_mainclust\") and (type_node.frac_mainclust < typefilter.exclude_if_fraction_of_mainclust_smaller_than[type_idx])\n",
    "    filtercrit_elems_mainclust = hasattr(type_node, \"num_mainclust_elems\") and(type_node.num_mainclust_elems < typefilter.exclude_if_elements_in_mainclust_less_than[type_idx])\n",
    "    filtercrit_elems_mostcommonclust = hasattr(type_node, \"num_mostcommonclust_elems\") and(type_node.num_mostcommonclust_elems < typefilter.exclude_if_elements_in_mostcommonclust_less_than[type_idx])\n",
    "    filtercrit_frac_mostcommonclust = hasattr(type_node, \"frac_mostcommonclust\") and(type_node.frac_mostcommonclust < typefilter.exclude_if_frac_mostcommonclust_less_than[type_idx])\n",
    "    filtercrit_num_mainclusts = hasattr(type_node, \"num_mainclusts\") and(type_node.num_mainclusts < typefilter.exclude_if_num_mainclusts_less_than[type_idx])\n",
    "    filtercrit_num_mostcommonclusts = hasattr(type_node, \"num_mostcommon_clusts\") and(type_node.num_mostcommon_clusts < typefilter.exclude_if_num_mostcommonclusts_less_than[type_idx])\n",
    "    no_leafs = len([x for x in node.leaves if x.is_included])==0\n",
    "    #print(\"filtercrit_numclust\\tfiltercrit_frac_mainclust\\tfiltercrit_elems_mainclust\\tfiltercrit_elems_mostcommonclust\\tfiltercrit_frac_mostcommonclust\\tfiltercrit_num_mainclusts\\tfiltercrit_num_mostcommonclusts\")\n",
    "    #print(f\"{filtercrit_numclust}\\t{filtercrit_frac_mainclust}\\t{filtercrit_elems_mainclust}\\t{filtercrit_elems_mostcommonclust}\\t{filtercrit_frac_mostcommonclust}\\t{filtercrit_num_mainclusts}\\t{filtercrit_num_mostcommonclusts}\")\n",
    "\n",
    "    if filtercrit_numclust | filtercrit_frac_mainclust | filtercrit_elems_mainclust |filtercrit_elems_mostcommonclust| filtercrit_frac_mostcommonclust |filtercrit_num_mainclusts | filtercrit_num_mostcommonclusts | no_leafs :\n",
    "        exclude_node(type_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def exclude_node(node):\n",
    "    node.is_included = False\n",
    "    for descendant in node.descendants:\n",
    "        descendant.is_included = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pickle\n",
    "import pandas as pd\n",
    "def cluster_along_specified_levels(typefilter, root_node, ionname2diffion, normed_c1, normed_c2, ion2diffDist, p2z, deedpair2doublediffdist, pval_threshold_basis, fcfc_threshold, take_median_ion):\n",
    "    #typefilter object specifies filtering and clustering of the nodes\n",
    "    assign_fcs_to_base_ions(root_node, ionname2diffion, normed_c1, normed_c2)\n",
    "    \n",
    "    for idx in range(len(typefilter.type)):\n",
    "        type_nodes = anytree.search.findall(root_node, filter_=lambda node: node.type == typefilter.type[idx])\n",
    "\n",
    "        if len(type_nodes)==0:\n",
    "            continue\n",
    "        for type_node in type_nodes:\n",
    "            child_nodes = type_node.children\n",
    "            #leaflist, node2leafs = get_ionlist(child_nodes, ionname2diffion, select_mainclust_ions=True)\n",
    "            leaflist = get_mainclust_leaves(child_nodes, ionname2diffion)\n",
    "            #print(type_node.name)\n",
    "            #print([len([y.name for y in x]) for x in leaflist])\n",
    "            if len(leaflist)==0:\n",
    "                exclude_node(type_node)\n",
    "                continue\n",
    "            childnode2clust = find_fold_change_clusters(type_node,leaflist, normed_c1, normed_c2, ion2diffDist, p2z, deedpair2doublediffdist, pval_threshold_basis, fcfc_threshold, take_median_ion) #the clustering is performed on the child nodes\n",
    "            childnode2clust = decide_cluster_order(type_node,childnode2clust)\n",
    "            annotate_mainclust_leaves(childnode2clust)\n",
    "            update_nodes(type_node, typefilter, idx, childnode2clust)\n",
    "            assign_vals_to_node(type_node,only_use_mainclust=True, use_fewpeps_per_protein=True)\n",
    "    \n",
    "    return root_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def get_mainclust_leaves(child_nodes, ionname2diffion):\n",
    "    grouped_leafs = []\n",
    "    for child in child_nodes:\n",
    "        child_leaves_mainclust = []\n",
    "        types_previous_level = {x.type for x in child.children}\n",
    "        for leafnode in child.leaves:#go through the leafs of each child\n",
    "            if hasattr(leafnode, 'inclusion_levels') and not (leafnode.inclusion_levels[-1] in types_previous_level):\n",
    "                continue\n",
    "            child_leaves_mainclust.append(leafnode)\n",
    "        child_leafs_diffions = [ionname2diffion.get(x.name) for x in child_leaves_mainclust] #map the leaf names to the diffion objetcs\n",
    "        if len(child_leafs_diffions)>0:\n",
    "            grouped_leafs.append(child_leafs_diffions)\n",
    "    return grouped_leafs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def annotate_mainclust_leaves(childnode2clust):\n",
    "    #annotate each leaf that has reached the current level with the level name, allows to visualize how the leafs are propagated\n",
    "    for child in childnode2clust.keys():\n",
    "        if childnode2clust.get(child)!=0:\n",
    "            continue\n",
    "        types_previous_level = {x.type for x in child.children}\n",
    "        for leafnode in child.leaves:#annotate the leaves of each node, if they were included at this level\n",
    "            if hasattr(leafnode, 'inclusion_levels'):\n",
    "                \n",
    "                if leafnode.inclusion_levels[-1] in types_previous_level: #only add a level if the previous level has also been included\n",
    "                    leafnode.inclusion_levels.append(child.type)\n",
    "            else:\n",
    "                leafnode.inclusion_levels = [child.type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import anytree\n",
    "import alphaquant.diff_analysis as aqdiff\n",
    "import alphaquant.diffquant_utils as aqutils\n",
    "from scipy.stats import norm\n",
    "import statistics\n",
    "import numpy as np\n",
    "\n",
    "def assign_vals_to_node(node, only_use_mainclust, use_fewpeps_per_protein):\n",
    "    \"\"\"Goes through the children and summarizes their properties to the node\n",
    "\n",
    "    Args:\n",
    "        node ([type]): [description]\n",
    "        only_use_mainclust (bool, optional): [description]. Defaults to True.\n",
    "    \"\"\"\n",
    "\n",
    "    if only_use_mainclust:\n",
    "        childs = [x for x in node.children if x.is_included & (x.cluster ==0)]\n",
    "    else:\n",
    "        childs = [x for x in node.children if x.is_included]\n",
    "\n",
    "    if use_fewpeps_per_protein and node.type == \"gene\":\n",
    "        childs = filter_fewpeps_per_protein(childs)\n",
    "\n",
    " \n",
    "    zvals = [x.z_val for x in childs]\n",
    "    fcs =  [x.fc for x in childs]\n",
    "    cvs = [x.cv for x in childs]\n",
    "    min_intensity = np.nanmedian([x.min_intensity for x in childs])\n",
    "    min_reps = np.nanmedian([x.min_reps for x in childs])\n",
    "    fraction_consistent = sum([x.fraction_consistent/len(node.children) for x in childs if x.cluster ==0])\n",
    "\n",
    "\n",
    "\n",
    "    z_sum = sum(zvals)\n",
    "    p_z = norm(0, np.sqrt(len(zvals))).cdf(z_sum)\n",
    "    z_normed = norm.ppf(p_z)\n",
    "    if z_normed <-8.2:\n",
    "        z_normed = -8.2\n",
    "    if z_normed > 8.2:\n",
    "        z_normed = 8.2\n",
    "\n",
    "    p_val = max(1e-16, 2.0 * (1.0 - norm(0, np.sqrt(len(zvals))).cdf(abs(z_sum))))\n",
    "\n",
    "    node.z_val = z_normed\n",
    "    node.p_val = p_val\n",
    "    node.fc = np.nanmedian(fcs)\n",
    "    node.fraction_consistent = fraction_consistent\n",
    "    node.cv = min(cvs)\n",
    "    node.min_intensity = min_intensity\n",
    "    node.min_reps = min_reps\n",
    "\n",
    "    if hasattr(node.children[0], 'predscore'):\n",
    "        predscores = [x.predscore for x in childs]\n",
    "        node.predscore = np.nanmedian(predscores)\n",
    "        node.cutoff = childs[0].cutoff\n",
    "        node.ml_excluded = abs(node.predscore)< node.cutoff\n",
    "\n",
    "def filter_fewpeps_per_protein(peptide_nodes):\n",
    "    peps_filtered = []\n",
    "    pepnode2pval2numleaves = []\n",
    "    for pepnode in peptide_nodes:\n",
    "        pepleaves = [x for x in pepnode.leaves if \"seq\" in x.inclusion_levels]\n",
    "        pepnode2pval2numleaves.append((pepnode, pepnode.p_val,len(pepleaves)))\n",
    "    pepnode2pval2numleaves = sorted(pepnode2pval2numleaves, key=lambda x : x[1], reverse=True)\n",
    "    numleaves_total = 0\n",
    "    for pepnode, _, numleaves in pepnode2pval2numleaves:\n",
    "        peps_filtered.append(pepnode)\n",
    "        numleaves_total+=numleaves\n",
    "        if numleaves_total>4:\n",
    "            break\n",
    "    return peps_filtered\n",
    "\n",
    "\n",
    "\n",
    "def get_diffresults_from_clust_root_node(root_node):\n",
    "    pval = root_node.p_val\n",
    "    fc = root_node.fc\n",
    "    ions_included = [x.name for x in root_node.leaves if x.is_included]\n",
    "    consistency_score = root_node.fraction_consistent * len(root_node.leaves)\n",
    "    return pval, fc, consistency_score, ions_included\n",
    "\n",
    "def get_scored_clusterselected_ions(gene_name, diffions, normed_c1, normed_c2, ion2diffDist, p2z, deedpair2doublediffdist, pval_threshold_basis, fcfc_threshold, take_median_ion):\n",
    "    #typefilter = TypeFilter('successive')\n",
    "    typefilter = init_typefilter_from_yaml('default')\n",
    "    regex_patterns = regex_frgions_isotopes\n",
    "    name2diffion = {x.name : x for x in diffions}\n",
    "    root_node = create_hierarchical_ion_grouping(regex_patterns, gene_name, diffions)\n",
    "    #print(anytree.RenderTree(root_node))\n",
    "    root_node_clust = cluster_along_specified_levels(typefilter, root_node, name2diffion, normed_c1, normed_c2, ion2diffDist, p2z, deedpair2doublediffdist, pval_threshold_basis, fcfc_threshold, take_median_ion)\n",
    "    #print(anytree.RenderTree(root_node_clust))\n",
    "    level_sorted_nodes = [[node for node in children] for children in anytree.ZigZagGroupIter(root_node_clust)]\n",
    "    level_sorted_nodes.reverse() #the base nodes are first\n",
    "\n",
    "    # for idx in range(0, len(level_sorted_nodes)):\n",
    "    #     nodes = level_sorted_nodes[idx]\n",
    "    #     if idx==0:\n",
    "    #         assign_fcs_to_base_ions(nodes, name2diffion)\n",
    "    #         continue\n",
    "    #     for node in nodes:\n",
    "    #         if not node.is_included:\n",
    "    #             continue\n",
    "    #         assign_vals_to_node(node, idx, name2diffion)\n",
    "    root_node_lvl = level_sorted_nodes[-1]\n",
    "    if len(root_node_lvl)!=1:\n",
    "        Exception(\"there should be only one root node!\")\n",
    "\n",
    "    root_node_annot =root_node_lvl[0]\n",
    "    return root_node_annot\n",
    "\n",
    "import scipy.stats\n",
    "def assign_fcs_to_base_ions(root_node, name2diffion, normed_c1, normed_c2):\n",
    "    for leaf in root_node.leaves:\n",
    "        leaf.fc = name2diffion.get(leaf.name).fc\n",
    "        leaf.z_val = name2diffion.get(leaf.name).z_val\n",
    "        leaf.fraction_consistent = 1\n",
    "        original_intensities_c1 = 2**(normed_c1.ion2nonNanvals.get(leaf.name))\n",
    "        original_intensities_c2 = 2**(normed_c2.ion2nonNanvals.get(leaf.name))\n",
    "        cv_c1 = scipy.stats.variation(original_intensities_c1)\n",
    "        cv_c2 = scipy.stats.variation(original_intensities_c2)\n",
    "        leaf.cv = min(cv_c1, cv_c2)\n",
    "        leaf.min_intensity = min(sum(original_intensities_c1)/len(original_intensities_c1), sum(original_intensities_c2)/len(original_intensities_c2))\n",
    "        leaf.min_reps = min(len(normed_c1.ion2nonNanvals.get(leaf.name)), len(normed_c2.ion2nonNanvals.get(leaf.name)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate ML scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def update_nodes_w_ml_score(protnodes):\n",
    "    typefilter = init_typefilter_from_yaml('default')\n",
    "    for prot in protnodes:\n",
    "        re_order_depending_on_predscore(prot, typefilter)\n",
    "\n",
    "\n",
    "def re_order_depending_on_predscore(protnode, typefilter):\n",
    "    for idx in range(len(typefilter.type)):\n",
    "        type_nodes = anytree.search.findall(protnode, filter_=lambda node: node.type == typefilter.type[idx])\n",
    "        if len(type_nodes)==0:\n",
    "            continue\n",
    "        for type_node in type_nodes: #go through the nodes, re-order the children. Propagate the values from the newly ordered children to the type node\n",
    "            child_nodes = type_node.children\n",
    "            had_predscore = hasattr(child_nodes[0], 'predscore')\n",
    "            if had_predscore:\n",
    "                re_order_clusters_by_predscore(child_nodes)\n",
    "                assign_vals_to_node(type_node)\n",
    "\n",
    "\n",
    "\n",
    "def re_order_clusters_by_predscore(nodes):\n",
    "    cluster2scores = {}\n",
    "    for node in nodes:\n",
    "        cluster2scores[node.cluster] = cluster2scores.get(node.cluster, [])\n",
    "        cluster2scores[node.cluster].append(node.predscore)\n",
    "    clusters = list(cluster2scores.keys())\n",
    "    clusters.sort(key = lambda x : abs(np.nanmedian(cluster2scores.get(x))))\n",
    "    clust2newclust = { clusters[x] :x for x in range(len(clusters))}\n",
    "    for node in nodes:\n",
    "        node.cluster =clust2newclust.get(node.cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "class TypeFilter():\n",
    "    def __init__(self, filttype= 'hierarchy'):\n",
    "        if filttype=='hierarchy':\n",
    "            self.type = ['frgion', 'ms1_isotopes', 'mod_seq_charge', 'mod_seq', 'seq', 'gene']\n",
    "            self.select_cluster = [-1,-1,-1,-1,-1,-1,-1]\n",
    "            self.exclude_if_more_clusters_than = [ np.inf, np.inf, np.inf, np.inf, np.inf, np.inf]\n",
    "            self.exclude_if_fraction_of_mainclust_smaller_than = [0, 0, 0, 0, 0, 0]\n",
    "            self.exclude_if_frac_mostcommonclust_less_than = [0,0,0,0,0,0]\n",
    "            self.exclude_if_elements_in_mainclust_less_than = [0, 0, 0, 0, 0, 0]\n",
    "            self.exclude_if_elements_in_mostcommonclust_less_than = [0, 0, 0, 0, 0, 0]\n",
    "            self.exclude_if_num_mainclusts_less_than = [0, 0, 0, 0, 0, 0]\n",
    "            self.exclude_if_num_mostcommonclusts_less_than = [0, 0, 0, 0, 0, 2]\n",
    "        if filttype == 'only_frgion':\n",
    "            self.type = ['frgion', 'ms1_isotopes', 'mod_seq_charge', 'mod_seq', 'seq', 'gene']\n",
    "            self.select_cluster = [0,-1,-1,-1,-1,-1,-1]\n",
    "            self.exclude_if_more_clusters_than = [ np.inf, np.inf, np.inf, np.inf, np.inf, np.inf]\n",
    "            self.exclude_if_fraction_of_mainclust_smaller_than = [0, 0, 0.2, 0, 0, 0.3]\n",
    "            self.exclude_if_frac_mostcommonclust_less_than = [0,0,0,0,0,0]\n",
    "            self.exclude_if_elements_in_mainclust_less_than = [1, 1, 1, 1, 1, 1]\n",
    "            self.exclude_if_elements_in_mostcommonclust_less_than = [1, 1, 1, 1, 1, 1]\n",
    "            self.exclude_if_num_mainclusts_less_than = [1, 1, 1, 1, 1, 1]\n",
    "            self.exclude_if_num_mostcommonclusts_less_than = [1, 1, 1, 1, 1, 2]\n",
    "        if filttype=='successive':\n",
    "            self.type = ['frgion', 'ms1_isotopes', 'mod_seq_charge', 'mod_seq', 'seq', 'gene']\n",
    "            self.select_cluster = [0, 0,-1,-1,-1,-1,-1]\n",
    "            self.exclude_if_more_clusters_than = [ np.inf, np.inf, np.inf, np.inf, np.inf, np.inf]\n",
    "            self.exclude_if_fraction_of_mainclust_smaller_than = [0, 0, 0.6, 0.6, 0, 0]\n",
    "            self.exclude_if_frac_mostcommonclust_less_than = [0,0,0,0,0,0]\n",
    "            self.exclude_if_elements_in_mainclust_less_than = [0, 0, 0, 0, 0, 0]\n",
    "            self.exclude_if_elements_in_mostcommonclust_less_than = [0, 0, 0, 0, 0, 0]\n",
    "            self.exclude_if_num_mainclusts_less_than = [0, 0, 2, 0, 0, 2]\n",
    "            self.exclude_if_num_mostcommonclusts_less_than = [0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import yaml\n",
    "import os\n",
    "import pathlib\n",
    "def init_typefilter_from_yaml(filttype):\n",
    "    typefilter_yaml = os.path.join(pathlib.Path(__file__).parent.absolute(), \"..\", \"typefilt_config.yaml\")\n",
    "    stream = open(typefilter_yaml, 'r')\n",
    "    typefilter_yaml = yaml.safe_load(stream)\n",
    "    filttype_dict = typefilter_yaml.get(filttype)\n",
    "    typefilt = TypeFilter(filttype)\n",
    "    typefilt.type = filttype_dict.get(\"type\")\n",
    "    typefilt.select_cluster = filttype_dict.get(\"select_cluster\")\n",
    "    typefilt.exclude_if_more_clusters_than = filttype_dict.get(\"exclude_if_more_clusters_than\")\n",
    "    typefilt.exclude_if_fraction_of_mainclust_smaller_than = filttype_dict.get(\"exclude_if_fraction_of_mainclust_smaller_than\")\n",
    "    typefilt.exclude_if_frac_mostcommonclust_less_than = filttype_dict.get(\"exclude_if_frac_mostcommonclust_less_than\")\n",
    "    typefilt.exclude_if_num_mainclusts_less_than = filttype_dict.get(\"exclude_if_num_mainclusts_less_than\")\n",
    "    typefilt.exclude_if_num_mostcommonclusts_less_than = filttype_dict.get(\"exclude_if_num_mostcommonclusts_less_than\")\n",
    "\n",
    "    return typefilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import numpy as np\n",
    "class NodeProperties():\n",
    "    \"\"\"Helper class to handle node properties\"\"\"\n",
    "    def __init__(self):\n",
    "        self.num_clusters = None\n",
    "        self.num_mainclust_elems = None\n",
    "        self.num_mostcommonclust_elems = None\n",
    "        self.frac_mainclust = None\n",
    "        self.frac_mostcommonclust = None\n",
    "        self.num_mainclusts = None\n",
    "        self.num_mostcommon_clusts = None\n",
    "        self.mostcommon_clust = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "regex_frgions_only = [[(\"(SEQ.*MOD.*CHARGE.*FRGION.*)\", \"frgion\")], [(\"(SEQ.*MOD.*CHARGE.*)(FRGION.*)\", \"mod_seq_charge\")], [(\"(SEQ.*MOD.*)(CHARGE.*)\", \"mod_seq\")], [(\"(SEQ.*)(MOD.*)\", \"seq\")]]\n",
    "\n",
    "regex_frgions_isotopes = [[(\"(SEQ.*MOD.*CHARGE.*FRG)(ION.*)\", \"frgion\"), (\"(SEQ.*MOD.*CHARGE.*MS1)(ISO.*)\", \"ms1_isotopes\")], [(\"(SEQ.*MOD.*CHARGE.*)(FRG.*|MS1.*)\", \"mod_seq_charge\")], [(\"(SEQ.*MOD.*)(CHARGE.*)\", \"mod_seq\")], [(\"(SEQ.*)(MOD.*)\", \"seq\")]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export/Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import anytree\n",
    "from anytree.exporter import JsonExporter\n",
    "import alphaquant.diffquant_utils as aqutils\n",
    "\n",
    "def export_roots_to_json(rootlist, condpair, results_dir):\n",
    "    \"\"\"exports all base roots for a given condition pair to a json file\"\"\"\n",
    "    condpairname = aqutils.get_condpairname(condpair)\n",
    "    condpair_node = anytree.Node(condpair) #set the condpair as node and export the whole condpair as one tree\n",
    "    for root in rootlist:\n",
    "        root.parent = condpair_node\n",
    "    results_file = f\"{results_dir}/{condpairname}.iontrees.json\"\n",
    "\n",
    "    j_exporter = JsonExporter(indent=2, sort_keys=True)\n",
    "    filehandle = open(results_file, \"w\")\n",
    "    j_exporter.write(condpair_node, filehandle)\n",
    "    filehandle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export\n",
    "# from collections import Counter\n",
    "# def propagate_clusters(diffions, ion2cluster):\n",
    "#     \"\"\"\n",
    "#     Gives basic cluster statistics for sets of ions corresponding to a node\n",
    "#      Args:\n",
    "#         diffions (list[list[ionnames]] ): contains the sets of ions to be tested, for examples [[fragion1_precursor1, fragion2_precursor1, fragion3_precursor1],[fragion1_precursor2],[fragion1_precursor3, fragion2_precursor3]]. The ions are assumed to be similar!\n",
    "#         ion2cluster: given assignment of base ion to cluster\n",
    "#     \"\"\"\n",
    "\n",
    "#     num_mainclust_elems = 0\n",
    "#     num_mostcommonclust_elems = 0\n",
    "#     ions2clust = {}\n",
    "#     all_clusters = []\n",
    "#     for ions in diffions:\n",
    "#         clusters = []\n",
    "#         for ion in ions:\n",
    "#             clusters.append(ion2cluster.get(ion))\n",
    "#         c = Counter(clusters).most_common(1)\n",
    "#         cluster = c[0][0]\n",
    "#         ions2clust[tuple(ions)] = cluster\n",
    "#         num_mainclust_elems += clusters.count(0)\n",
    "#         num_mostcommonclust_elems += clusters.count(cluster)\n",
    "#         all_clusters.append(cluster)\n",
    "#     c_mostcommon = Counter(all_clusters).most_common(1)\n",
    "#     clust_mostcommon = c_mostcommon[0][0]\n",
    "#     num_mostcommon_clusts = all_clusters.count(clust_mostcommon)\n",
    "#     num_mainclusts = all_clusters.count(0)\n",
    "#     frac_mainclust = num_mainclusts/len(all_clusters)\n",
    "#     frac_mostcommonclust =  num_mostcommon_clusts/len(all_clusters)\n",
    "#     num_clusters = len(set(all_clusters))\n",
    "    \n",
    "#     nodeprops = NodeProperties()\n",
    "#     nodeprops.num_clusters = num_clusters\n",
    "#     nodeprops.num_mainclust_elems = num_mainclust_elems #elements in the overall biggest cluster\n",
    "#     nodeprops.num_mostcommonclust_elems = num_mostcommonclust_elems #elements in the most common cluster for this particular node. \n",
    "#     #The most common cluster can only be different from the main cluster in case that clustering has been carried out outside of the set\n",
    "#     nodeprops.frac_mainclust = frac_mainclust\n",
    "#     nodeprops.frac_mostcommonclust = frac_mostcommonclust\n",
    "#     nodeprops.num_mainclusts = num_mainclusts\n",
    "#     nodeprops.num_mostcommon_clusts = num_mostcommon_clusts\n",
    "#     nodeprops.mostcommon_clust = clust_mostcommon\n",
    "\n",
    "#     return ions2clust,nodeprops\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4726\n",
      "t_ion2nonan_sw 0.011356115341186523\n",
      "t_intensity_selection 0.39522719383239746\n",
      "t_ion2nonan_sw 0.009817838668823242\n",
      "t_intensity_selection 1.9364638328552246\n",
      "diffions ['pep23_LVL0_mod0_LVL1_mod0_LVL2_mod0_LVL3_mod0', 'pep12_LVL0_mod0_LVL1_mod0_LVL2_mod0_LVL3_mod1', 'pep12_LVL0_mod0_LVL1_mod0_LVL2_mod0_LVL3_mod0', 'pep23_LVL0_mod0_LVL1_mod1_LVL2_mod0_LVL3_mod0']\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import alphaquant.background_distributions as aqbg\n",
    "import alphaquant.benchmarking as aqbm\n",
    "import alphaquant.diff_analysis as aqdiff\n",
    "import alphaquant.diffquant_utils as aqutils\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def simulate_normed_input():\n",
    "\n",
    "    sample2cond_df = pd.DataFrame({'sample' : ['A1', 'A2', 'A3', 'B1', 'B2', 'B3','B4', 'B5', 'B6', 'B7', 'B8', 'B9','B10', 'B11', 'B12'],\n",
    "    'condition' : ['A', 'A', 'A', 'B', 'B', 'B','B', 'B', 'B','B', 'B', 'B','B', 'B', 'B']})\n",
    "    unnormed_df = aqbm.generate_random_input(10000, sample2cond_df,simulate_nas=True)\n",
    "    df_c1, df_c2 = get_c1_c2_dfs(unnormed_df, sample2cond_df, [\"A\", \"B\"])\n",
    "    p2z = {}\n",
    "    normed_c1 = aqbg.ConditionBackgrounds(df_c1, p2z)\n",
    "    normed_c2 = aqbg.ConditionBackgrounds(df_c2, p2z)\n",
    "    return normed_c1, normed_c2\n",
    "\n",
    "def get_c1_c2_dfs(unnormed_df, labelmap_df, condpair, minrep = 2):\n",
    "    c1_samples = labelmap_df[labelmap_df[\"condition\"]== condpair[0]]\n",
    "    c2_samples = labelmap_df[labelmap_df[\"condition\"]== condpair[1]]\n",
    "    df_c1 = unnormed_df.loc[:, c1_samples[\"sample\"]].dropna(thresh=minrep, axis=0)\n",
    "    df_c2 = unnormed_df.loc[:, c2_samples[\"sample\"]].dropna(thresh=minrep, axis=0)\n",
    "\n",
    "    return df_c1, df_c2\n",
    "\n",
    "def generate_diffions():\n",
    "    normed_c1, normed_c2 = simulate_normed_input()\n",
    "    ion2diffDist = {}\n",
    "    p2z = {}\n",
    "    diffions = []\n",
    "    ions_to_check = normed_c1.ion2nonNanvals.keys() & normed_c2.ion2nonNanvals.keys()\n",
    "    for idx, ion in enumerate(ions_to_check):\n",
    "        if not ((\"pep12_\" in ion) | (\"pep23_\" in ion)):\n",
    "            continue\n",
    "\n",
    "        vals1 = normed_c1.ion2nonNanvals.get(ion)\n",
    "        vals2 = normed_c2.ion2nonNanvals.get(ion)\n",
    "        diffDist = aqbg.get_subtracted_bg(ion2diffDist,normed_c1, normed_c2,ion, p2z)\n",
    "        diffIon = aqdiff.DifferentialIon(vals1, vals2, diffDist, ion, outlier_correction = False)\n",
    "        diffions.append(diffIon)\n",
    "        #if idx>100:\n",
    "         #   break\n",
    "    \n",
    "    return diffions, normed_c1, normed_c2\n",
    "\n",
    "\n",
    "def add_shifts_to_diffions(ion2shift,diffions, normed_c1, normed_c2):\n",
    "    ion2diffdist = {}\n",
    "    ionname2diffion = {}\n",
    "    for ion in diffions:\n",
    "        shift = ion2shift.get(ion.name)\n",
    "        vals1 = normed_c1.ion2nonNanvals.get(ion.name)\n",
    "        vals2 = normed_c2.ion2nonNanvals.get(ion.name)\n",
    "        ion.fc = ion.fc+shift\n",
    "        if shift>0:\n",
    "            vals2 = vals2+shift\n",
    "            normed_c2.ion2nonNanvals[ion.name] = vals2\n",
    "            normed_c2.ion2allvals[ion.name] = np.array([x + shift for x in  normed_c2.ion2allvals[ion.name] if x!=0])\n",
    "            \n",
    "        if shift<0:\n",
    "            vals1 = vals1+shift\n",
    "            normed_c1.ion2nonNanvals[ion.name] = vals1\n",
    "            normed_c1.ion2allvals[ion.name] =  np.array([x + shift for x in  normed_c1.ion2allvals[ion.name] if x!=0])\n",
    "        diffDist = aqbg.get_subtracted_bg(ion2diffdist,normed_c1, normed_c2,ion.name, {})\n",
    "\n",
    "        diffIon = aqdiff.DifferentialIon(vals1, vals2, diffDist, ion.name, outlier_correction = False)\n",
    "        ionname2diffion[ion.name] = diffIon\n",
    "        #if idx>100:\n",
    "         #   break\n",
    "\n",
    "    return ionname2diffion\n",
    "    \n",
    "\n",
    "\n",
    "import alphaquant.benchmarking as aqbench\n",
    "import alphaquant.diffquant_utils as aqutils\n",
    "\n",
    "def test_tree_construction():\n",
    "\n",
    "    diffions, normed_c1, normed_c2 = generate_diffions()\n",
    "    regex_patterns = [[(\"(.*_LVL0.*_LVL1.*_LVL2.*_LVL3)(_mod[0-1])\",\"frgion\"), (\"(.*_LVL0.*_LVL1.*_LVL2.*_LVL3)(_mod[2-3])\", \"ms1_isotopes\")], [(\"(.*_LVL0.*_LVL1.*_LVL2)(.*_LVL3)\", \"mod_seq_charge\")], [(\"(.*_LVL0.*_LVL1)(.*_LVL2)\", \"mod_seq\")], \n",
    "    [(\"(.*)(.*_LVL0.*_LVL1.*)\", \"seq\")]]\n",
    "    node = create_hierarchical_ion_grouping(regex_patterns,\"testgene\",diffions)\n",
    "    #print(anytree.RenderTree(node))\n",
    "    #dotexporter.UniqueDotExporter(node).to_picture(\"tmp.png\")\n",
    "    print(f'diffions {[x.name for x in diffions]}')\n",
    "    typefilter = TypeFilter()\n",
    "\n",
    "    deedpair2doublediffdist = {}\n",
    "    dpair2diffdist = {}\n",
    "    p2z = {}\n",
    "    aqbench.add_perturbations_to_proteins([node])\n",
    "    ion2shift = {x.name:x.applied_shift for x in node.leaves}\n",
    "    ionname2diffion = add_shifts_to_diffions(ion2shift, diffions, normed_c1, normed_c2)\n",
    "    node_clust = cluster_along_specified_levels(typefilter,node,ionname2diffion, normed_c1, normed_c2,dpair2diffdist,p2z, deedpair2doublediffdist, pval_threshold_basis=0.05, fcfc_threshold=0, take_median_ion=False)\n",
    "    aqbench.count_correctly_excluded([node], [node])\n",
    "    #logging.info(anytree.RenderTree(node_clust))\n",
    "    #print(anytree.RenderTree(node_clust))\n",
    "\n",
    "\n",
    "\n",
    "test_tree_construction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized within conditions\n",
      "median 0.00729314330089359, mode 0.0009959526163560994\n",
      "using median for shift\n",
      "shift cond 2 by -0.00729314330089359\n",
      "t_ion2nonan_sw 0.04566192626953125\n",
      "t_intensity_selection 1.5283470153808594\n",
      "t_ion2nonan_sw 0.046247243881225586\n",
      "t_intensity_selection 1.4687960147857666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import anytree\n",
    "import sklearn.metrics\n",
    "import random\n",
    "\n",
    "\n",
    "def select_shifts_for_ions(diffions):\n",
    "    ion2shift = {}\n",
    "    for diffion in diffions:\n",
    "        perturb = np.random.uniform(0, 1) < 0.3 #randomly select ~30% of the samples for perturbation\n",
    "        if perturb:\n",
    "            applied_shift = random.sample([-2, 2],1)[0]\n",
    "            ion2shift[diffion.name] = applied_shift\n",
    "        else:\n",
    "            ion2shift[diffion.name] = 0\n",
    "    return ion2shift\n",
    "\n",
    "def compare_clustered_and_shifted(ion2shift, ion2clust):\n",
    "    is_shifted_vec = []\n",
    "    clust_nonzero_vec = []\n",
    "    for ion in ion2shift:\n",
    "        is_shifted = ion2shift.get(ion)!=0\n",
    "        clust_nonzero = ion2clust.get(ion)!=0\n",
    "        if (not is_shifted) and (not clust_nonzero):\n",
    "            continue\n",
    "        clust_nonzero_vec.append(clust_nonzero)\n",
    "        is_shifted_vec.append(is_shifted)\n",
    "    \n",
    "    \n",
    "    accuracy = sklearn.metrics.accuracy_score(y_true = is_shifted_vec, y_pred = clust_nonzero_vec)\n",
    "    recall = sklearn.metrics.recall_score(y_true=is_shifted_vec, y_pred = clust_nonzero_vec)\n",
    "    precision = sklearn.metrics.precision_score(y_true=is_shifted_vec, y_pred = clust_nonzero_vec)\n",
    "    length = len(clust_nonzero_vec)\n",
    "    #print(f\"accuracy is {accuracy}\")\n",
    "    #print(f\"precision {precision}\\trecall {recall}\")\n",
    "\n",
    "    return accuracy, precision, recall, length\n",
    "\n",
    "\n",
    "\n",
    "def check_correct_clustering(diffions, normed_c1, normed_c2):\n",
    "    diffions_grouped = [[x] for x in diffions]\n",
    "    ion2shift = select_shifts_for_ions(diffions)\n",
    "    ionname2diffion = add_shifts_to_diffions(ion2shift, diffions, normed_c1, normed_c2)\n",
    "    type_node = anytree.Node(\"root\")\n",
    "    for diffion in ionname2diffion.values():\n",
    "        anytree.Node(diffion.name, parent=type_node)\n",
    "    childnode2clust = find_fold_change_clusters(type_node, diffions_grouped, normed_c1, normed_c2, {}, {}, {},0.05, 0,False) #the clustering is performed on the child nodes\n",
    "    chilnodename2clust = {x.name : y for x,y in childnode2clust}\n",
    "    return compare_clustered_and_shifted(ion2shift, chilnodename2clust)\n",
    "\n",
    "\n",
    "diffions_real, normed_c1_real, normed_c2_real = aqbench.load_real_example_ions(\"test_data/cluster_ions/filtered_fragions.aq_reformat.tsv\",\n",
    "\"test_data/cluster_ions/samples.map.tsv\",num_ions=1000)\n",
    "\n",
    "diffions_subsetted = [diffions_real[i:i + 10] for i in range(0, len(diffions_real), 10)]\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "for diffions in diffions_subsetted:\n",
    "    accuracy,precison,recall, length = check_correct_clustering(diffions, normed_c1_real, normed_c2_real)\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precison)\n",
    "    recalls.append(recall)\n",
    "\n",
    "plt.hist(accuracies, cumulative=True, density=True, histtype='step', label='accuracy')\n",
    "plt.hist(precisions,cumulative=True, density=True, histtype='step', label='precision')\n",
    "plt.hist(recalls,cumulative=True, density=True, histtype='step', label='recall')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#diffions_simul, normed_c1_simul, normed_c2_simul =generate_diffions()\n",
    "#check_correct_clustering(diffions_simul, normed_c1_simul, normed_c2_simul)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
