{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "The normalization step of MS-EmpiRe aims at reducing systematic biases between samples. Such biases can for example occur when more material is pipetted in one of the samples. In principle, two steps are performed:\n",
    "\n",
    "1. Normalize between samples of the same condition (i.e. replicates)\n",
    "\n",
    "2. Normalize between different conditions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Within-condition normalization\n",
    "It is common practice and highly recommended to measure multiple samples of a given condition. This ensures that observed changes between conditions are not just due to random variation. Examples of samples within the same condition could be biological replicates, but also patients with the same clinical condition. \n",
    "We want to ensure that systematic changes between within-condition samples are corrected for as follows:\n",
    "\n",
    "* Our assumed input values are log2 transformed peptide ion intensities, which are stored in a 2d numpy array called \"samples\". Each row in samples represents a peptide and each column represents a sample\n",
    "\n",
    "* In a first step, we determine the all pairwise distances between the samples (details explained below)\n",
    "* We then choose the pair of samples with the closest distance between each other\n",
    "* We randomly choose one \"anchor\" sample and one \"shift\" sample and we subtract the distance between the samples from each peptide intensity measured in the \"shift\" sample. This is equivalent to rescaling the intensities of the original sample by a constant factor such that the distributions are aligned\n",
    "* We then construct a virtual \"merged\" sample by computing the average intensities of anchor and shift sample\n",
    "* We repeat the steps above until all samples are merged. Keeping track of the shift factors allows us then to determine an ideal shift for each sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def normalize_withincond(samples):##row is the sample column is the features\n",
    "\n",
    "    \"finds optimal scaling factors for samples measured in the same condition and corrects the samples by these scaling factors. Takes a 2d numpy array as input  \"\n",
    "    num_samples = samples.shape[0]\n",
    "    mergedsamples = np.copy(samples) #the virtual \"merged\" samples will be stored in this array\n",
    "    sampleidx2shift = dict(zip(range(num_samples), np.zeros(num_samples))) #the scaling factors applied to the samples are stored here\n",
    "    sampleidx2counts = dict(zip(range(num_samples), np.ones(num_samples)))#keeps track of how many distributions are merged\n",
    "    sampleidx2anchoridx = {} #keeps track of the shifted samples\n",
    "    exclusion_set = set() #already clustered samples are stored here\n",
    "    distance_matrix = create_distance_matrix(samples)\n",
    "    variance_matrix = create_distance_matrix(samples, metric = 'variance')\n",
    "    #print(f\"distance matrix start\\n{distance_matrix}\")\n",
    "\n",
    "    for rep in range(num_samples-1):\n",
    "        #anchor_idx, shift_idx, min_distance = get_bestmatch_pair(mergedsamples, exclusion_set, sampleidx2counts)\n",
    "        anchor_idx, shift_idx, min_distance = get_bestmatch_pair(distance_matrix,variance_matrix, sampleidx2counts)\n",
    "        \n",
    "        # #determine the closest pair of samples (one \"shift\" sample to be shifted and one \"anchor sample which stays the same\") and the distance between this pair\n",
    "        #update the sets\n",
    "\n",
    "        if(anchor_idx == None):\n",
    "            break\n",
    "        sampleidx2anchoridx.update({shift_idx : anchor_idx})\n",
    "        sampleidx2shift.update({shift_idx : min_distance })\n",
    "        exclusion_set.add(shift_idx)\n",
    "\n",
    "        anchor_sample = mergedsamples[anchor_idx]\n",
    "        shift_sample = samples[shift_idx]\n",
    "        shifted_sample = shift_sample + min_distance\n",
    "\n",
    "        #print(f\"\\n\\nanchor {anchor_sample}\\nshift {shift_sample}\\nshifted sample{shifted_sample}\\nshiftfactor {min_distance}\\tsamples {shift_idx}\\t{anchor_idx}\")\n",
    "        merged_sample = merge_distribs(anchor_sample, shifted_sample, sampleidx2counts[anchor_idx], sampleidx2counts[shift_idx])\n",
    "        mergedsamples[anchor_idx] = merged_sample\n",
    "\n",
    "        #print(f\"shift, anchor: {shift_idx}, {anchor_idx} achtual shift {distance_matrix[shift_idx][anchor_idx]} or {distance_matrix[anchor_idx][shift_idx]}\")\n",
    "        #print(f\"distance matrix before\\n{distance_matrix}\")\n",
    "\n",
    "\n",
    "        update_distance_matrix(variance_matrix, mergedsamples, anchor_idx, shift_idx, metric='variance')\n",
    "        update_distance_matrix(distance_matrix, mergedsamples, anchor_idx, shift_idx)\n",
    "\n",
    "        #print(f\"distance matrix after\\n{distance_matrix}\")\n",
    "        sampleidx2counts[anchor_idx]+=1\n",
    "\n",
    "    for i in exclusion_set:\n",
    "        shift = get_total_shift(sampleidx2anchoridx, sampleidx2shift, i)\n",
    "        samples[i] = samples[i]+shift\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best matching pair\n",
    "Take all pairs of the columns in the \"samples\" array that have not been already merged and compute the distance between the pairs as follows:\n",
    "* Subtract sample1 from sample2 (or sample2 from sample1, the order does not matter)\n",
    "* This results in a distribution of differences. As the samples array contains log2 intensities, this corresponds to taking log2 fold changes\n",
    "* Take the median of the distribution, this is a good approximation for the change between the two distributions\n",
    "* Select the two samples with the lowest absolute change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_bestmatch_pair(distance_matrix, variance_matrix, sample2counts):\n",
    "    \n",
    "    i,j = np.unravel_index(np.argmin(variance_matrix, axis=None), variance_matrix.shape)\n",
    "    min_distance = distance_matrix[i,j]\n",
    "    #print(f\"idxs are {i}, {j} median is {distance_matrix[i][j]} variance is {variance_matrix[i][j]}\")\n",
    "    if(min_distance == np.inf):\n",
    "        return None, None, None\n",
    "    anchor_idx, shift_idx, min_distance = determine_anchor_and_shift_sample(sample2counts,i, j, min_distance) #direction flip of distance if necessary\n",
    "    return anchor_idx, shift_idx, min_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_distance_matrix(samples, metric = 'median'):\n",
    "    num_samples = samples.shape[0]\n",
    "    distance_matrix = np.full((num_samples, num_samples), np.inf)\n",
    "    for i in range(num_samples):\n",
    "        for j in range(i+1, num_samples):#do every comparison once\n",
    "            distance_matrix[i,j] = calc_distance(metric, samples[i], samples[j]) #the median of the shifted distribution is taken as the distance measure\n",
    "            \n",
    "    return distance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def calc_distance(metric, samples_1, samples_2):\n",
    "    res = None\n",
    "\n",
    "    if metric == 'median':\n",
    "        res = np.nanmedian(get_fcdistrib(samples_1, samples_2))#the median of the shifted distribution is taken as the distance measure\n",
    "    if(metric == 'variance'):\n",
    "        fcdist = get_fcdistrib(samples_1, samples_2)\n",
    "        #if sum(~np.isnan(fcdist))<2:\n",
    "         #   return np.nan\n",
    "        res = np.nanvar(fcdist)\n",
    "    if res == None:\n",
    "        raise Exception(f\"distance metric {metric} not implemented\")\n",
    "    if(np.isnan(res)):\n",
    "        return np.inf\n",
    "    else:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def update_distance_matrix(distance_matrix, merged_samples, merged_sample_idx, shift_idx,metric ='median'):\n",
    "    \"determine the distances to the newly merged sample\"\n",
    "    for i in range(0, merged_sample_idx):#update rows of distance matrix\n",
    "        if distance_matrix[i, merged_sample_idx]==np.inf:#do not compare already merged samples\n",
    "            continue\n",
    "        distance = calc_distance(metric,merged_samples[i], merged_samples[merged_sample_idx])\n",
    "        distance_matrix[i, merged_sample_idx] = distance\n",
    "    \n",
    "    for j in range(merged_sample_idx+1, merged_samples.shape[0]):#update columns of distance matrix\n",
    "        if distance_matrix[merged_sample_idx, j] == np.inf:\n",
    "            continue\n",
    "        distance = calc_distance(metric,merged_samples[merged_sample_idx], merged_samples[j])\n",
    "        distance_matrix[merged_sample_idx, j] = distance\n",
    "    \n",
    "    distance_matrix[shift_idx] = np.inf #shifted samples are excluded by setting distance to infinity\n",
    "    distance_matrix[:, shift_idx] = np.inf\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_fcdistrib(logvals_rep1, logvals_rep2):\n",
    "    \"generates difference distribution between two samples\"\n",
    "    dist = np.subtract(logvals_rep1, logvals_rep2)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def determine_anchor_and_shift_sample(sample2counts, i_min, j_min, min_distance):\n",
    "    \"given two samples, declare the sample with fewer merges as the shift\"\n",
    "    counts_i = sample2counts[i_min]\n",
    "    counts_j = sample2counts[j_min]\n",
    "    anchor_idx = i_min if counts_i>=counts_j else j_min\n",
    "    shift_idx = j_min if anchor_idx == i_min else i_min\n",
    "    flip = 1 if anchor_idx == i_min else -1\n",
    "    return anchor_idx, shift_idx, flip*min_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting samples\n",
    "When we have computed the distance between two samples, we want to correct one of the samples by this distance. This results in two distributions with the same median value. We always shift the sample which has been merged from fewer distributions (see below for details). The sample to which the shift is applied is call \"shift\" sample and the sample which is not shifted is called \"anchor\" sample.\n",
    "A \"total shift\" is calculated after all samples are merged, just by following up how many shifts have been applied to a sample in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def shift_samples(samples, sampleidx2anchoridx, sample2shift):\n",
    "    for sample_idx in range(samples.shape[0]):\n",
    "        samples[sample_idx] = samples[sample_idx]+get_total_shift(sampleidx2anchoridx, sample2shift, sample_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_total_shift(sampleidx2anchoridx, sample2shift,sample_idx):\n",
    "\n",
    "    total_shift = 0.0\n",
    "\n",
    "    while(True):\n",
    "        total_shift +=sample2shift[sample_idx]\n",
    "        if sample_idx not in sampleidx2anchoridx: #every shifted sample has an anchor\n",
    "            break\n",
    "        sample_idx = sampleidx2anchoridx[sample_idx]\n",
    "\n",
    "    return total_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "' #hide\\ndef test_shift_samples():\\n    #assume we shift sample 1 to 2, and then sample 2 to 3\\n    sampleidx2anchoridx_test = {0:1, 1:2}\\n    #the first shift is 5 and the second shift is 10\\n    sample2shift_test = {0:5, 1:5,2:0}\\n    samples_test = np.array([[1,1,1], [6,6,6], [11,11,11]])\\n    print(shift_samples(samples_test, sampleidx2anchoridx_test,sample2shift_test))\\n    assert shift_samples(samples_test, sampleidx2anchoridx_test,sample2shift_test) == np.array([[1,1,1], [1,1,1], [1,1,1]])\\n\\ntest_shift_samples() '"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" #hide\n",
    "def test_shift_samples():\n",
    "    #assume we shift sample 1 to 2, and then sample 2 to 3\n",
    "    sampleidx2anchoridx_test = {0:1, 1:2}\n",
    "    #the first shift is 5 and the second shift is 10\n",
    "    sample2shift_test = {0:5, 1:5,2:0}\n",
    "    samples_test = np.array([[1,1,1], [6,6,6], [11,11,11]])\n",
    "    print(shift_samples(samples_test, sampleidx2anchoridx_test,sample2shift_test))\n",
    "    assert shift_samples(samples_test, sampleidx2anchoridx_test,sample2shift_test) == np.array([[1,1,1], [1,1,1], [1,1,1]])\n",
    "\n",
    "test_shift_samples() \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging distributions\n",
    "After we shift two distributions on top of each other, we calculate a \"merged\" distribution. Each intensity in the merged distribution is the average of the intensity in both distributions. For the merging we have to take into account the following: If for example the anchor sample has already been merged from 10 samples, and the shift distribution has not been merged at all, we want to weigh the distribution coming from many samples higher. We hence multiply each sample by the number of merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def merge_distribs(anchor_distrib, shifted_distrib,counts_anchor_distrib, counts_shifted_distrib):\n",
    "    \"Calculate the average peptide intensities to merge two peptide distributions\"\n",
    "\n",
    "    t_alt = time.time()\n",
    "    res = np.zeros(len(anchor_distrib))\n",
    "\n",
    "    nans_anchor = np.isnan(anchor_distrib)\n",
    "    nans_shifted = np.isnan(shifted_distrib)\n",
    "    nans_anchor_and_shifted = nans_anchor & nans_shifted\n",
    "    nans_only_anchor = nans_anchor & ~nans_shifted\n",
    "    nans_only_shifted = nans_shifted &~nans_anchor\n",
    "    no_nans = ~nans_anchor & ~nans_shifted\n",
    "\n",
    "    idx_anchor_and_shifted = np.where(nans_anchor_and_shifted)\n",
    "    idx_only_anchor = np.where(nans_only_anchor)\n",
    "    idx_only_shifted = np.where(nans_only_shifted)\n",
    "    idx_no_nans = np.where(no_nans)\n",
    "\n",
    "    res[idx_anchor_and_shifted] = np.nan\n",
    "    res[idx_only_anchor] = shifted_distrib[idx_only_anchor]\n",
    "    res[idx_only_shifted] = anchor_distrib[idx_only_shifted]\n",
    "    res[idx_no_nans] = (anchor_distrib[idx_no_nans] *counts_anchor_distrib + shifted_distrib[idx_no_nans]*counts_shifted_distrib)/(counts_anchor_distrib+counts_shifted_distrib)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_merged_distribs():\n",
    "    anchor_distrib = np.array([1, 1, 1, 1, 1])\n",
    "    shift_distrib = np.array([2, 2, 2, 2, 2])\n",
    "    counts_anchor_distrib = 4\n",
    "    counts_shifted_distib = 1\n",
    "    assert (merge_distribs(anchor_distrib, shift_distrib, counts_anchor_distrib, counts_shifted_distib)== np.array([1.2, 1.2, 1.2, 1.2, 1.2])).any()\n",
    "\n",
    "test_merged_distribs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def generate_randarrays(number_arrays,size_of_array):\n",
    "    randarray = []\n",
    "    for i in range(number_arrays):\n",
    "        shift = np.random.uniform(low=-10, high=+10)\n",
    "        randarray.append(np.random.normal(loc=shift, size=size_of_array))\n",
    "    return np.array(randarray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDev 0.9913432587600196\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_sampleshift(samples):\n",
    "    num_samples = samples.shape[0]\n",
    "    merged_sample = []\n",
    "    for i in range(num_samples):\n",
    "        plt.hist(samples[i])\n",
    "        merged_sample.extend(samples[i])\n",
    "    stdev = np.std(merged_sample)\n",
    "    print(f\"STDev {stdev}\")\n",
    "    assert (stdev <=1.2) \n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "randarray = generate_randarrays(5, 1000)\n",
    "normalized_randarray = normalize_withincond(randarray)\n",
    "test_sampleshift(normalized_randarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "import pandas as pd\n",
    "\n",
    "def mode_normalization(x):\n",
    "\n",
    "    x = np.sort(x)\n",
    "\n",
    "    cumul_counts = np.linspace(0, len(x), len(x)) #cut away the most extreme fold changes\n",
    "    cumul_counts_rel = cumul_counts/cumul_counts[-1]\n",
    "    thresh = 0.05\n",
    "    subset_vec = (cumul_counts_rel>thresh/2) & (cumul_counts_rel<1-thresh/2)\n",
    "    x = x[subset_vec]\n",
    "\n",
    "    x_min = min(x)\n",
    "    x_max = max(x)\n",
    "    num_bins = int((x_max-x_min)*50)\n",
    "    bins = np.linspace(x_min, x_max, num_bins)\n",
    "    hist = np.histogram(x, bins)\n",
    "    x = hist[0]\n",
    "    fcs =  0.5*(hist[1][1:]+hist[1][:-1]) #get middle of each fc bin\n",
    "    cumul_x = np.cumsum(x)\n",
    "\n",
    "    peaks2  = find_peaks(x, prominence=1)\n",
    "    lbase = peaks2[1]['left_bases']\n",
    "    rbase = peaks2[1]['right_bases']\n",
    "    cumul_heights = cumul_x[rbase] - cumul_x[lbase]\n",
    "    max_cumul_idx = np.argmax(cumul_heights)#returns the peak with the highest probabilty mass\n",
    "    max_idx = peaks2[0][max_cumul_idx]\n",
    "    shift_fc = hist[1][max_idx]\n",
    "\n",
    "    return shift_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def get_betweencond_shift(df_c1_normed, df_c2_normed):\n",
    "\n",
    "    both_idx = df_c1_normed.index.intersection(df_c2_normed.index)\n",
    "    df1 = df_c1_normed.loc[both_idx]\n",
    "    df2 = df_c2_normed.loc[both_idx]\n",
    "    df1 = df1.median(axis = 1, skipna = True).to_frame()\n",
    "    df2 = df2.median(axis = 1, skipna = True).to_frame()\n",
    "    col1 = df1.columns[0]\n",
    "    col2 = df2.columns[0]\n",
    "\n",
    "    diff_fcs = df1[col1].to_numpy() - df2[col2].to_numpy()\n",
    "    median = np.nanmedian(diff_fcs)\n",
    "    mode = mode_normalization(diff_fcs)\n",
    "    if(abs(median-mode) <0.05):\n",
    "        print(f\"using median for shift\")\n",
    "        return -median\n",
    "    else:\n",
    "        print(f\"using mode for shift\")\n",
    "        return -mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import alphaquant.visualizations as aqviz\n",
    "def get_normalized_dfs(df_c1, df_c2,  c1_samples, c2_samples, minrep, runtime_plots = True,prenormed_file = None): #labelmap_df, unnormed_df,condpair,\n",
    "\n",
    "    #c1_samples = labelmap_df[labelmap_df[\"condition\"]== condpair[0]]\n",
    "    #c2_samples = labelmap_df[labelmap_df[\"condition\"]== condpair[1]]\n",
    "    # df_c1 = unnormed_df.loc[:, c1_samples[\"sample\"]].dropna(thresh=minrep, axis=0)\n",
    "    # df_c2 = unnormed_df.loc[:, c2_samples[\"sample\"]].dropna(thresh=minrep, axis=0)\n",
    "    df_c1_normed = None\n",
    "    df_c2_normed = None\n",
    "\n",
    "    if prenormed_file is not None:\n",
    "        print(\"using pre-normalized data - skipping normalization\")\n",
    "        df_prenormed = pd.read_csv(prenormed_file, sep=\"\\t\",index_col = \"ion\")\n",
    "        df_c1_normed = df_prenormed[c1_samples[\"sample\"]].dropna(thresh=minrep, axis=0)\n",
    "        df_c2_normed = df_prenormed[c2_samples[\"sample\"]].dropna(thresh=minrep, axis=0)\n",
    "    else:\n",
    "        df_c1_normed = pd.DataFrame(normalize_withincond(df_c1.to_numpy().T).T, index = df_c1.index, columns = c1_samples[\"sample\"])\n",
    "        df_c2_normed = pd.DataFrame(normalize_withincond(df_c2.to_numpy().T).T, index = df_c2.index, columns = c2_samples[\"sample\"])\n",
    "\n",
    "    if runtime_plots:\n",
    "        aqviz.plot_betweencond_fcs(df_c1, df_c2, True)\n",
    "    print(f\"normalized within conditions\")\n",
    "    shift_between_cond = get_betweencond_shift(df_c1_normed, df_c2_normed)\n",
    "    if(prenormed_file is not None):\n",
    "        shift_between_cond = -0.18\n",
    "    print(f\"shift cond 2 by {shift_between_cond}\")\n",
    "    df_c2_normed = df_c2_normed-shift_between_cond\n",
    "    #compare_normalization(\"./test_data/normed_intensities.tsv\", df_c1_normed, df_c2_normed)\n",
    "    if runtime_plots:\n",
    "        aqviz.plot_betweencond_fcs(df_c1_normed, df_c2_normed, False)\n",
    "        aqviz.plot_betweencond_fcs(df_c1_normed, df_c2_normed, True)\n",
    "    return df_c1_normed, df_c2_normed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python385jvsc74a57bd0dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
