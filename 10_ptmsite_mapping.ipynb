{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp ptmsite_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ms_empire.background_distributions import *\n",
    "from ms_empire.normalization import *\n",
    "from ms_empire.diff_analysis import *\n",
    "from ms_empire.visualizations import *\n",
    "from ms_empire.benchmarking import *\n",
    "from ms_empire.diffquant_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "class ModifiedPeptide():\n",
    "    \"\"\"\n",
    "    helper class for convenient access of modified peptide variables\n",
    "    \"\"\"\n",
    "    def __init__(self, df_line, protein_sequence, id_thresh, excl_thresh, modification_type = \"[Phospho (STY)]\"):\n",
    "\n",
    "        self.id = df_line[\"IonID\"]\n",
    "        self.ionname = df_line[\"FG.Id\"]\n",
    "        self.sample = df_line[\"R.Label\"]\n",
    "        self.seq = df_line[\"PEP.GroupingKey\"]\n",
    "        self.prot = df_line[\"PG.UniProtIds\"]\n",
    "        self.start_idx = protein_sequence.find(self.seq)\n",
    "        positions_parsed = np.array(df_line[f\"EG.PTMPositions {modification_type}\"].split(\";\")).astype(\"int\")\n",
    "        probabilities_parsed =  np.array(df_line[f\"EG.PTMProbabilities {modification_type}\"].split(\";\")).astype(\"float\")\n",
    "        self.positions = scale_site_idxs_to_protein(protein_sequence, self.seq, positions_parsed)\n",
    "        self.num_sites = get_num_sites(probabilities_parsed)\n",
    "        self.probabilities = probabilities_parsed\n",
    "        self.encoded_probs = None#encode_probabilities(probabilities_parsed, id_thresh, excl_thresh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group ions and reduce redundancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import copy\n",
    "def merge_samecond_modpeps(ions, sample2cond, id_thresh, excl_thresh):\n",
    "    \"\"\"\n",
    "    identical ions from the same condition are merged and their site localization probabilities are averaged\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    condid2ionids = {}\n",
    "    \n",
    "    condion2modpeps = {}\n",
    "    for ion in ions:\n",
    "        condid = f\"{sample2cond.get(ion.sample)}{ion.ionname}\"\n",
    "        condion2modpeps[condid] = condion2modpeps.get(condid, []) + [ion]\n",
    "        condid2ionids[condid] = condid2ionids.get(condid, []) + [ion.id]\n",
    "    \n",
    "    for condid,modpeps in condion2modpeps.items():\n",
    "        modpep_selected = copy.deepcopy(modpeps[0])\n",
    "        allprobs = [x.probabilities for x in modpeps]\n",
    "        meanprobs = np.mean(allprobs, axis = 0)\n",
    "        modpep_selected.id = condid\n",
    "        modpep_selected.probabilities = meanprobs\n",
    "        modpep_selected.encoded_probs = encode_probabilities(meanprobs, id_thresh, excl_thresh)\n",
    "        res.append(modpep_selected)\n",
    "    return res, condid2ionids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def scale_site_idxs_to_protein(protseq, pepseq, localization_array):\n",
    "    \"\"\"align peptide sequence along protein, express idxs relative to start\"\"\"\n",
    "    start_idx = protseq.find(pepseq)\n",
    "    localization_array = localization_array + start_idx\n",
    "    return localization_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_num_sites(probabilities_parsed):\n",
    "    return round(sum(probabilities_parsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def group_by_nummods_posv(ions):\n",
    "    \"\"\"ions with identical position vector and number of modifications are grouped together\"\"\"\n",
    "    nmod2ions = {}\n",
    "    for ion in ions:\n",
    "        nmodposv = f\"{ion.num_sites}_{ion.positions}\"\n",
    "        nmod2ions[nmodposv] = nmod2ions.get(nmodposv, []) + [ion]\n",
    "    return list(nmod2ions.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def condense_ions(ions):\n",
    "    \"\"\"\n",
    "    group ions together, which have identical sequence and encoded probabilities. This way you only need to \n",
    "    compare these in the distance matrix\n",
    "    \"\"\"\n",
    "    key2equivions = {}\n",
    "    for ion in ions:\n",
    "        key = f\"{ion.seq}_{ion.encoded_probs}\"\n",
    "        key2equivions[key] = key2equivions.get(key, []) + [ion]\n",
    "    ion2equiv_ions = {gr_ions[0] : gr_ions for gr_ions in key2equivions.values()}\n",
    "    representative_ions = list(ion2equiv_ions.keys())\n",
    "    return representative_ions, ion2equiv_ions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def encode_probabilities(probabilties_parsed, id_thresh, excl_thresh):\n",
    "    prob_copy = probabilties_parsed.copy()\n",
    "    prob_copy[prob_copy>id_thresh] = 5\n",
    "    prob_copy[prob_copy < excl_thresh] = 3\n",
    "    prob_copy[(prob_copy!=3) & (prob_copy!=5)] = 0\n",
    "\n",
    "    return prob_copy.astype('int')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare and cluster ions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_ion_similarities(ions):\n",
    "    seqs = np.array([x.seq for x in ions])\n",
    "    encoded = np.array([x.encoded_probs for x in ions])\n",
    "    #distances = np.array([(8 in (x[0].encoded_probs + x[1].encoded_probs)) &(x[1].seq in x[0].seq) for x in itertools.combinations(ions, 2)]).astype('int')\n",
    "    distances = get_condensed_matrix(seqs, encoded)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def cluster_ions(ions):\n",
    "    res = []\n",
    "    nmod_posv_grouped = group_by_nummods_posv(ions)\n",
    "    for candidates in nmod_posv_grouped:\n",
    "        if len(candidates)==1: #check if only one ion, then no pairwise comparison needed\n",
    "            res.extend([candidates])\n",
    "            continue\n",
    "        \n",
    "        representative_ions, ion2equiv_ions = condense_ions(candidates) \n",
    "        if len(representative_ions)==1:#check if only one condensed ion, then also no pairwise comparison needed\n",
    "            equiv_ions = ion2equiv_ions.get(representative_ions[0])\n",
    "            res.extend([equiv_ions])\n",
    "            continue\n",
    "\n",
    "        ionclustered = cluster_ions_pairwise(representative_ions) #if multiple ions to compare, do pairwise comparisons\n",
    "        for cluster in ionclustered:\n",
    "            clust_copy = cluster.copy()\n",
    "            for ion in clust_copy:\n",
    "                equiv_ions = ion2equiv_ions.get(ion)\n",
    "                if len(equiv_ions)>1:\n",
    "                    cluster.extend(equiv_ions)\n",
    "        res.extend(ionclustered)\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import scipy.cluster.hierarchy as hierarchy\n",
    "\n",
    "def cluster_ions_pairwise(ions):\n",
    "    \"\"\"form complete linkage clusters (every ion is a neighbor to every ion in the cluster) for a given set of ions. Distance matrix define in 'compare ion similarities'\"\"\"\n",
    "    ions.sort(key = lambda x : len(x.seq),reverse = True)\n",
    "    condensed_distance_matrix = compare_ion_similarities(ions)\n",
    "    after_clust = hierarchy.complete(condensed_distance_matrix)\n",
    "    clustered = hierarchy.fcluster(after_clust, 0.1, criterion='distance')\n",
    "    clust2ions = {}\n",
    "    for i in range(len(clustered)):\n",
    "       clustions = clust2ions.get(clustered[i],list())\n",
    "       clustions.append(ions[i])\n",
    "       clust2ions[clustered[i]] = clustions\n",
    "\n",
    "    return list(clust2ions.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def compare_ion_similarities(ions):\n",
    "    \"\"\"returns a condensed distance matrix for a given set of ions. Distances are calculated based on the encoded site localization probabilities, as described below\"\"\"\n",
    "    seqs = np.array([x.seq for x in ions])\n",
    "    encoded = np.array([x.encoded_probs for x in ions])\n",
    "    distances = get_condensed_matrix(seqs, encoded)\n",
    "\n",
    "    return distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_condensed_matrix(seqs, encoded):\n",
    "    \"\"\"checks pairwise occpancy vectors based on the following encoding: 3 == clearly not occupied, 5 == clearly occupied. \n",
    "    If a sum=vec1+vec2 contains 3+5=8, this means it is dissimilar and is assigned distance = 1, distance =0 otherwise\n",
    "    \"\"\"\n",
    "    res = np.zeros(int(len(seqs) * (len(seqs)-1)/2))\n",
    "    count = 0\n",
    "    for i in range(len(seqs)):\n",
    "        for j in range(i+1, len(seqs)):\n",
    "            seq1 = seqs[i]\n",
    "            seq2 = seqs[j]\n",
    "            if seq2 in seq1:\n",
    "                encode1 = encoded[i]\n",
    "                encode2 = encoded[j]\n",
    "                summed = encode1 + encode2\n",
    "                if 8 in summed:\n",
    "                    res[count] = 1\n",
    "            count+=1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and reformat input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_idmap_column(protgroups, swissprots):\n",
    "    \"\"\"go through protein groups and map to swissprot ID if possible\"\"\"\n",
    "    res = []\n",
    "    for protgroup in protgroups:\n",
    "        mapped = False\n",
    "        proteins = list(protgroup.split(\";\"))\n",
    "        for protein in proteins:\n",
    "            if protein in swissprots:\n",
    "                res.append(protein)\n",
    "                mapped = True\n",
    "                break\n",
    "        if not mapped:\n",
    "            res.append(proteins[0])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "def get_site_prob_overview(modpeps, refprot, refgene):\n",
    "    \"\"\"reformats the modified peptide objects for a given protein. The returned series objects contain the mean probabilities for a given site and experimental sample\"\"\"\n",
    "    site2sample2probs = {}\n",
    "    for modpep in modpeps:\n",
    "        for idx in range(len(modpep.positions)):\n",
    "            site = modpep.positions[idx]\n",
    "            prob = modpep.probabilities[idx]\n",
    "            sample = modpep.sample\n",
    "            site2sample2probs[site] = site2sample2probs.get(site, {}) #.update({sample:[]})\n",
    "            site2sample2probs.get(site)[sample] = site2sample2probs.get(site).get(sample, []) + [prob]\n",
    "    \n",
    "    series_collected = []\n",
    "    for site in site2sample2probs.keys():\n",
    "        sample2probs = site2sample2probs.get(site)\n",
    "        header = list(sample2probs.keys())\n",
    "        probs = [np.mean(sample2probs.get(x)) for x in header]\n",
    "        site_series = pd.Series(probs, index=header)\n",
    "        site_series = site_series.append(pd.Series([int(site)], index=[\"site\"]))\n",
    "        site_series = site_series.append(pd.Series(refprot, index= [\"REFPROT\"]))\n",
    "        site_series = site_series.append(pd.Series(refgene, index= [\"gene\"]))\n",
    "        series_collected.append(site_series)\n",
    "\n",
    "    return series_collected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign all ions for a given protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def assign_protein(modpeps,condid2ionids, refprot):\n",
    "    \"\"\"go through ions of a given protein, cluster if necessary and map each ion to a ptm_site ID\"\"\"\n",
    "    id2groupid = {}\n",
    "    id2normedid = {}\n",
    "\n",
    "    if len(modpeps) == 1:\n",
    "        grouped_ions = [modpeps]\n",
    "    else:\n",
    "        grouped_ions = cluster_ions(modpeps)\n",
    "\n",
    "    for group in grouped_ions:\n",
    "        summed_probs = sum([x.probabilities for x in group])\n",
    "        num_sites = group[0].num_sites\n",
    "        idx_most_likely = np.argpartition(summed_probs, -num_sites)[-num_sites:]\n",
    "        positions = np.sort(group[0].positions[idx_most_likely])\n",
    "        ptm_group_id = positions#f\"{refprot}_{positions}\"\n",
    "        ptm_group_id_normed = f\"{refprot}_{positions-group[0].start_idx}\"\n",
    "        all_ions = sum([condid2ionids.get(x.id) for x in group], [])#the condition-level merged ions are mapped back to the existin ion-level IDs\n",
    "        id2groupid.update({x:ptm_group_id for x in all_ions})\n",
    "\n",
    "        id2normedid.update({x:ptm_group_id_normed for x in all_ions})\n",
    "\n",
    "\n",
    "    return id2groupid, id2normedid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def assign_dataset(ptmprob_file, id_thresh = 0.75, excl_thresh =0.15, samplemap = 'samples.map',swissprot_file = 'swissprot_mapping.tsv', sequence_file='uniprot_mapping.tsv', modification_type = \"[Phospho (STY)]\",sep = \"\\t\"):\n",
    "\n",
    "    \"\"\"wrapper function reformats inputs tables and iterates through the whole dataset. Output needs to contain \"\"\"\"\"\n",
    "    input_df = pd.read_csv(ptmprob_file, sep = sep).drop_duplicates()\n",
    "    _,sample2cond = initialize_sample2cond(samplemap)\n",
    "    len_before = len(input_df.index)\n",
    "    input_df = input_df[~input_df[f\"EG.PTMProbabilities {modification_type}\"].isna()]\n",
    "    print(f\"filtered PTM peptides from {len_before} to {len(input_df.index)}\")\n",
    "    swissprot_ids = set(pd.read_csv(swissprot_file, sep = \"\\t\", usecols = [\"Entry\"])[\"Entry\"])\n",
    "    sequence_df = pd.read_csv(sequence_file, sep = \"\\t\", usecols = [\"Entry\", \"Sequence\", \"Gene names\"])\n",
    "    sequence_map = dict(zip(sequence_df[\"Entry\"], sequence_df[\"Sequence\"]))\n",
    "    sequence_df = sequence_df.dropna()\n",
    "    \n",
    "    refgene_map = dict(zip(sequence_df[\"Entry\"], [x.split(\" \")[0] for x in sequence_df[\"Gene names\"]]))\n",
    "\n",
    "    input_df[\"REFPROT\"] = get_idmap_column(input_df[\"PG.UniProtIds\"],swissprot_ids)\n",
    "    input_df[\"IonID\"] = input_df[\"R.Label\"] + input_df['FG.Id']\n",
    "    input_df = input_df.set_index(\"REFPROT\")\n",
    "    input_df.sort_index(inplace=True)\n",
    "    #input_df.to_csv(f\"{ptmprob_file}.sorted\", sep = \"\\t\")\n",
    "    site_ids = []\n",
    "    fg_ids = []\n",
    "    run_ids = []\n",
    "    prot_ids = []\n",
    "    gene_ids = []\n",
    "    ptmlocs = []\n",
    "    locprobs = []\n",
    "    siteprobs = []\n",
    "    \n",
    "    count_peps = 0\n",
    "    one_fraction = int(len(input_df.index)/100)\n",
    "    for prot in input_df.index.unique():#input_df[\"REFPROT\"].unique():\n",
    "\n",
    "        if count_peps%one_fraction==0:\n",
    "            print(f\"assigned {count_peps} of {len(input_df.index)} {count_peps/len(input_df.index)}\")\n",
    "        \n",
    "        #filtvec = [prot in x for x in input_df[\"REFPROT\"]]\n",
    "\n",
    "        protein_df = input_df.loc[[prot]].copy()#input_df[filtvec].copy()\n",
    "        protein_df = protein_df.reset_index()\n",
    "\n",
    "        sequence = sequence_map.get(prot)\n",
    "        if sequence == None:\n",
    "            continue\n",
    "        gene = refgene_map.get(prot)\n",
    "        count_peps+= len(protein_df)\n",
    "\n",
    "        modpeps_per_sample = [ModifiedPeptide(protein_df.loc[x], sequence, id_thresh, excl_thresh) for x in protein_df.index]\n",
    "        merged_siteprobs = get_site_prob_overview(modpeps_per_sample, prot, gene)\n",
    "        siteprobs.extend(merged_siteprobs)\n",
    "        modpeps, condid2ionids = merge_samecond_modpeps(modpeps_per_sample, sample2cond, id_thresh, excl_thresh) #all ions coming from the same condition are merged\n",
    "        ionid2ptmid,_ = assign_protein(modpeps, condid2ionids, prot)##after clustering, conditions are mapped back to the original run\n",
    "\n",
    "        ptm_ids_prot = [ionid2ptmid.get(x) for x in protein_df[\"IonID\"]]\n",
    "        \n",
    "        ptmlocs.extend([x for x in protein_df[f\"EG.PTMPositions {modification_type}\"]])\n",
    "        locprobs.extend([x for x in protein_df[f\"EG.PTMProbabilities {modification_type}\"]])\n",
    "        site_ids.extend(ptm_ids_prot)\n",
    "        fg_ids.extend(protein_df[\"FG.Id\"].tolist())\n",
    "        run_ids.extend(protein_df[\"R.Label\"].tolist())\n",
    "        prot_ids.extend([prot for x in range(len(ptm_ids_prot))])\n",
    "        gene_ids.extend([gene for x in range(len(ptm_ids_prot))])\n",
    "\n",
    "    \n",
    "    conditions = [sample2cond.get(x) for x in run_ids]\n",
    "    mapped_df = pd.DataFrame({\"R.Label\" : run_ids, \"conditions\" : conditions, \"FG.Id\" : fg_ids, \"REFPROT\" : prot_ids, \"gene\" : gene_ids,\"site\" : site_ids, \"ptmlocs\":ptmlocs ,\"locprob\" : locprobs})\n",
    "    mapped_df.to_csv(f\"{ptmprob_file}.ptm_ids\", sep = \"\\t\", index = None)\n",
    "    \n",
    "    siteprob_df = pd.DataFrame(siteprobs)\n",
    "    siteprob_df = siteprob_df.astype({\"site\" : \"int\"})\n",
    "    siteprob_df.set_index([\"REFPROT\", \"site\"], inplace=True)\n",
    "    siteprob_df = siteprob_df.sort_index().reset_index()\n",
    "    siteprob_df.to_csv(f\"{ptmprob_file}.siteprobs\", sep = \"\\t\", index = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def detect_site_occupancy_change(cond1, cond2, samplemap_file, ptmsite_map, minrep = 2, threshold_prob = 0.05):\n",
    "    \"\"\"\n",
    "    reads a PTMsite table with headers \"REFPROT\", \"gene\",\"site\", and headers for sample1, sample2, etc and determines\n",
    "    whether a site appears/dissappears between conditions based on some probability threshold\n",
    "    \"\"\"\n",
    "    samplemap_df, _ = initialize_sample2cond(samplemap_file)\n",
    "    ptmsite_df = pd.read_csv(ptmsite_map, sep = \"\\t\")\n",
    "    ptmsite_df[\"site_id\"] = ptmsite_df[\"REFPROT\"] + ptmsite_df[\"site\"].astype(\"str\")\n",
    "    ptmsite_df = ptmsite_df.set_index(\"site_id\").sort_index()\n",
    "    cond1_samples = list(set(samplemap_df[(samplemap_df[\"condition\"]==cond1)][\"sample\"]).intersection(set(ptmsite_df.columns)))\n",
    "    cond2_samples = list(set(samplemap_df[(samplemap_df[\"condition\"]==cond2)][\"sample\"]).intersection(set(ptmsite_df.columns)))\n",
    "\n",
    "    regulated_sites = []\n",
    "    count = 0\n",
    "    for ptmsite in ptmsite_df.index.unique():\n",
    "\n",
    "        site_df = ptmsite_df.loc[[ptmsite]]\n",
    "        count+=len(site_df.index)\n",
    "  \n",
    "        cond1_vals = site_df[cond1_samples].to_numpy()\n",
    "        cond2_vals = site_df[cond2_samples].to_numpy()\n",
    "\n",
    "        cond1_vals = cond1_vals[~np.isnan(cond1_vals)]\n",
    "        cond2_vals = cond2_vals[~np.isnan(cond2_vals)]\n",
    "\n",
    "        numrep_c1 = len(cond1_vals)\n",
    "        numrep_c2 = len(cond2_vals)\n",
    "\n",
    "        if(numrep_c1<minrep) | (numrep_c2 < minrep):\n",
    "            continue\n",
    "\n",
    "\n",
    "        cond1_prob = np.mean(cond1_vals)\n",
    "        cond2_prob = np.mean(cond2_vals)\n",
    "        \n",
    "\n",
    "        unlikely_c1 = cond1_prob<threshold_prob\n",
    "        unlikely_c2 = cond2_prob<threshold_prob\n",
    "        likely_c1 = cond1_prob>1-threshold_prob\n",
    "        likely_c2 = cond2_prob>1-threshold_prob\n",
    "        direction = 0\n",
    "\n",
    "        if(unlikely_c1&likely_c2):\n",
    "            direction = -1\n",
    "        if(unlikely_c2&likely_c1):\n",
    "            direction = 1\n",
    "        \n",
    "        if direction!=0:\n",
    "            refprot = site_df[\"REFPROT\"].values[0]\n",
    "            gene = site_df[\"gene\"].values[0]\n",
    "            site = site_df[\"site\"].values[0]\n",
    "            regulated_sites.append([refprot, gene, site, direction, cond1_prob, cond2_prob, numrep_c1, numrep_c2])\n",
    "        \n",
    "\n",
    "    df_occupancy_change = pd.DataFrame(regulated_sites, columns=[\"REFPROT\", \"gene\", \"site\", \"direction\", \"c1_meanprob\", \"c2_meanprob\", \"c1_nrep\", \"c2_nrep\"])\n",
    "    return df_occupancy_change\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python385jvsc74a57bd0dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
