{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from alphaquant.diff_analysis_manager import run_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "def get_tps_fps(result_df, prot2org_file, thresh = 0.05, fc_thresh = 0.3):\n",
    "    annotated = annotate_dataframe(result_df, prot2org_file)\n",
    "    condpairs = result_df[\"condpair\"].drop_duplicates()\n",
    "\n",
    "\n",
    "    for condpair in condpairs:\n",
    "        annotated_condpair = annotated[annotated[\"condpair\"]==condpair]\n",
    "        num_tps = sum(annotated_condpair[\"TP\"])\n",
    "        num_fps = sum(annotated_condpair[\"FP\"])\n",
    "        annotated_fcfilt = annotated_condpair[annotated[\"log2fc\"] >fc_thresh]\n",
    "        num_regulated_prots = sum(annotated_fcfilt[\"fdr\"]<thresh)\n",
    "        num_true_positives = sum(annotated_fcfilt[\"TP\"] &(annotated_fcfilt[\"fdr\"]<0.05))\n",
    "        num_false_positives = sum(annotated_fcfilt[\"FP\"] &(annotated_fcfilt[\"fdr\"]<0.05))\n",
    "        fpr = num_false_positives/num_regulated_prots\n",
    "\n",
    "        print(f'condpair {condpair}')\n",
    "        print(f\"total TPs {num_tps}\")\n",
    "        print(f\"total FPs {num_fps}\")\n",
    "        print(f'regulated {num_regulated_prots}')\n",
    "        print(f'false positives {num_false_positives}')\n",
    "        print(f'true positives {num_true_positives}')\n",
    "        print(f'regulated control {num_false_positives+num_true_positives}')\n",
    "        print(f'FPR {fpr}')\n",
    "\n",
    "        assert fpr < 0.06\n",
    "\n",
    "\n",
    "def annotate_dataframe(result_df, prot2org_file):\n",
    "    prot2org = pd.read_csv(prot2org_file, sep = \"\\t\")\n",
    "    prot2org[\"FP\"] = (prot2org[\"organism\"] == \"Homo sapiens\")\n",
    "    prot2org[\"TP\"] = (prot2org[\"organism\"] == \"Saccharomyces cerevisiae\")\n",
    "    prot2org = prot2org[(prot2org[\"FP\"] | prot2org[\"TP\"])]\n",
    "    print(f\"df size before {len(result_df.index)}\")\n",
    "    annotated = pd.merge(result_df, prot2org, how='inner', on = \"protein\")\n",
    "    print(f\"df size after {len(annotated.index)}\")\n",
    "    return annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "def compare_to_reference(result_df, reference_file, condpair):#put in condpair as tuple\n",
    "    result_df = result_df[result_df[\"condpair\"]==condpair]\n",
    "\n",
    "    ref_df = pd.read_csv(reference_file, sep = \"\\t\")\n",
    "    merged = pd.merge(result_df, ref_df, how='inner', on = \"protein\",suffixes = [\"\", \"_ref\"])\n",
    "    ax_p = merged.plot.scatter(x='pval_ref',y='pval')\n",
    "    plt.show()\n",
    "    ax_fc = merged.plot.scatter(x='log2FC_ref',y='fc')\n",
    "    plt.show()\n",
    "    ax_fdr = merged.plot.scatter(x='fdr_ref',y='fdr')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "def compare_normalization(ref_normalization_file, norm1_df, norm2_df):\n",
    "    ref_normed = pd.read_csv(ref_normalization_file, sep =\"\\t\").set_index('peptide')\n",
    "\n",
    "    merged = pd.merge(norm1_df, norm2_df, how='inner',  left_index = True, right_index = True)\n",
    "    columns = merged.columns\n",
    "    merged = pd.merge(ref_normed, merged, how='inner', left_index = True, right_index = True, suffixes = [\"_ref\", \"\"])\n",
    "\n",
    "    for i in range(len(columns)):\n",
    "        sample1 = columns[i]\n",
    "        sample2 = sample1+\"_ref\"\n",
    "        ax_p = merged.plot.scatter(x=sample1,y=sample2)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import alphaquant.visualizations as aqviz\n",
    "\n",
    "def compare_to_reference(peptide_detail_file, result_df, peptide_df, protref_file, outdir):\n",
    "    protein_ref = pd.read_csv(peptide_detail_file, sep=\"\\t\", usecols=[\"protein\", \"protein_pval\", \"protein_fc\"]).drop_duplicates().rename(columns = {\"protein_pval\" : \"pval_ref\", \"protein_fc\": \"log2fc_ref\"})\n",
    "    peptide_ref = pd.read_csv(peptide_detail_file, sep='\\t', usecols = [\"peptide\", \"protein\", \"peptide_pval\",\"peptide_fc\"]).rename(columns = {\"peptide_pval\" :\"peptide_pval_ref\", \"peptide_fc\" : \"peptide_fc_ref\"})\n",
    "    aqviz.compare_peptid_protein_overlaps(protein_ref, result_df, peptide_ref, peptide_df, peptide_name = \"peptide\")\n",
    "    compare_significant_proteins(result_df, protref_file)\n",
    "\n",
    "    print_nonref_hits(protein_ref, result_df, peptide_ref, peptide_df, outdir)\n",
    "    prots_merged = pd.merge(protein_ref, result_df, on = \"protein\", how='inner')\n",
    "    peps_per_prot_ref = pd.DataFrame(peptide_ref.groupby(by=[\"protein\"])['peptide'].count()).rename(columns = {\"peptide\":\"num_peptides_ref\"}).reset_index()\n",
    "    prots_merged = pd.merge(prots_merged, peps_per_prot_ref, on = \"protein\", how='inner')\n",
    "\n",
    "    peptides_merged = pd.merge(peptide_ref, peptide_df, on = \"peptide\", how='inner')\n",
    "\n",
    "    peptides_merged[\"peptide_pval_diff\"] = ( peptides_merged[\"peptide_pval\"]/peptides_merged[\"peptide_pval_ref\"]).abs()\n",
    "    peptides_merged = peptides_merged.sort_values(by=['peptide_pval_diff'], ascending = False)\n",
    "    display(peptides_merged.head(10))\n",
    "    peptides_merged.to_csv(f\"{outdir}/merged_peptides.tsv\", sep = \"\\t\", index = False)\n",
    "    aqviz.scatter_df_columns(prots_merged)\n",
    "    aqviz.scatter_df_columns(peptides_merged)\n",
    "\n",
    "    prots_merged[\"pvaldiff\"] = (np.log2(prots_merged[\"pval\"]) - np.log2(prots_merged[\"pval_ref\"])).abs()\n",
    "    prots_merged = prots_merged.sort_values(by=['pvaldiff'], ascending = False)\n",
    "    peptides_merged.to_csv(f\"{outdir}/merged_proteins.tsv\", sep = \"\\t\", index = False)\n",
    "    display(prots_merged.head(10))\n",
    "    display(peptides_merged)\n",
    "\n",
    "\n",
    "    prots_merged[\"numpep_diff\"] = (prots_merged[\"num_peptides\"] - prots_merged[\"num_peptides_ref\"]).abs()\n",
    "    prots_merged = prots_merged.sort_values(by=['numpep_diff'], ascending = False)\n",
    "    display(prots_merged.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from matplotlib_venn import venn2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def compare_significant_proteins(result_df, protref_file):\n",
    "    protein_ref = pd.read_csv(protref_file, sep=\"\\t\")\n",
    "    sigprots_ref = protein_ref[protein_ref[\"fdr\"]<0.05]\n",
    "    sigprots = result_df[result_df[\"fdr\"]<0.05]\n",
    "    prots_ref = set(sigprots_ref[\"protein\"].to_list())\n",
    "    prots = set(sigprots[\"protein\"].to_list())\n",
    "    print(f\"in ref only {prots_ref - prots}\")\n",
    "    print(f\"in AP only {prots-prots_ref}\")\n",
    "    venn2([prots_ref, prots], ('sigprots_ref', 'sigprots'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_nonref_hits(protein_ref, protein_df, peptide_ref, peptide_df, outdir):\n",
    "    prots_nonref_df =  protein_df[~(protein_df[\"protein\"].isin(protein_ref[\"protein\"].to_list()))]#the tilde inverts the boolean vector\n",
    "    peps_nonref_df = peptide_df[~(peptide_df[\"peptide\"].isin(peptide_ref[\"peptide\"].to_list()))]\n",
    "    prots_nonref_df.to_csv(f\"{outdir}/nonref_proteins.tsv\", sep = \"\\t\", index = False)\n",
    "    peps_nonref_df.to_csv(f\"{outdir}/nonref_peptides.tsv\", sep = \"\\t\", index = False)\n",
    "    #display(peps_nonref_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "import random\n",
    "\n",
    "def test_run_pipeline():\n",
    "\n",
    "    sample2cond_df = pd.DataFrame({'sample' : ['A1', 'A2', 'A3', 'B1', 'B2', 'B3','B4', 'B5', 'B6', 'B7', 'B8', 'B9','B10', 'B11', 'B12'],\n",
    "    'condition' : ['A', 'A', 'A', 'B', 'B', 'B','B', 'B', 'B','B', 'B', 'B','B', 'B', 'B']})\n",
    "    unnormed_df = generate_random_input(10000, sample2cond_df)\n",
    "    res_df, pep_df = run_pipeline(unnormed_df, sample2cond_df, 2, False)\n",
    "    plot_pvals(pep_df)\n",
    "\n",
    "def generate_random_input(num_pep,sample2cond_df , simulate_nas = False):\n",
    "    pepnames = generate_peptide_list(num_pep, [2, 3, 5, 3]) #gives uuid strings for each peptide\n",
    "    print(len(pepnames))\n",
    "    protnames = generate_protein_list(pepnames)\n",
    "    nrep_1 = 3\n",
    "    nrep_2 = 12\n",
    "    randarrays1 = 10+ 1.5*np.random.randn(len(pepnames),nrep_1)\n",
    "    randarrays2 = 10+ 3.5*np.random.randn(len(pepnames),nrep_2)\n",
    "\n",
    "    if simulate_nas:\n",
    "        idxs_1 = np.unique(np.random.randint(0, nrep_1, size= int(len(randarrays1)/3)))\n",
    "        randarrays1[idxs_1] = np.nan\n",
    "        idxs_2 = np.unique(np.random.randint(0, nrep_2, size= int(len(randarrays2)/3)))\n",
    "        randarrays2[idxs_2] = np.nan\n",
    "\n",
    "    randarrays = np.concatenate((randarrays1, randarrays2), axis = 1)\n",
    "    df_intens = pd.DataFrame(randarrays, columns= sample2cond_df[\"sample\"].tolist())\n",
    "    df_intens.insert(1,\"protein\", protnames)\n",
    "    df_intens.insert(0, \"ion\", pepnames )\n",
    "    df_intens = df_intens.set_index(\"ion\")\n",
    "    return df_intens\n",
    "\n",
    "def generate_peptide_list(num_peps, levels ):\n",
    "    \"\"\"levels is list of ints, each int inidcates, how many potential possibilities there are on this level\"\"\"\n",
    "    pepcount = 0\n",
    "    count = 0\n",
    "    peptides = []\n",
    "    while count < num_peps:\n",
    "        list = [f'pep{pepcount}']\n",
    "\n",
    "        for levelidx, level in enumerate(levels):\n",
    "            num_events = np.random.randint(1,level)\n",
    "            new_list = []\n",
    "            for elem in list:\n",
    "                for idx in range(num_events):\n",
    "                    new_list.append(elem + f\"_LVL{levelidx}_mod{idx}\")\n",
    "                    count+=1\n",
    "            list = new_list\n",
    "        peptides.extend(list)\n",
    "        pepcount+=1\n",
    "\n",
    "    return peptides\n",
    "\n",
    "\n",
    "\n",
    "def generate_protein_list(pepnames):\n",
    "    res = []\n",
    "    assigned = 0\n",
    "    protcount = 0\n",
    "    while assigned < len(pepnames):\n",
    "        protstring = f\"P{protcount}\"\n",
    "        num_peps = random.randint(2,10)\n",
    "        for i in range(num_peps):\n",
    "            res.append(protstring)\n",
    "        assigned+=num_peps\n",
    "        protcount+=1\n",
    "    res = res[:len(pepnames)]\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions to compare against other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def annotate_fcs_to_wideformat_table(wideformat_df, columns_intens_c1, columns_intens_c2, num_reps = None):\n",
    "    wideformat_df[columns_intens_c1+columns_intens_c2] = wideformat_df[columns_intens_c1+columns_intens_c2].replace(0, np.nan)\n",
    "    prots_c1 = wideformat_df[columns_intens_c1]\n",
    "    prots_c2 = wideformat_df[columns_intens_c2]\n",
    "\n",
    "    prots_c1 = prots_c1.dropna(thresh = num_reps) #if None then no nans, i.e. all replicates\n",
    "    prots_c2 = prots_c2.dropna(thresh = num_reps)\n",
    "    both_idx = prots_c1.index.intersection(prots_c2.index)\n",
    "    wideformat_df[\"median_int_c1\"] = prots_c1.loc[both_idx].median(axis = 1, skipna = True)\n",
    "    wideformat_df[\"median_int_c2\"] = prots_c2.loc[both_idx].median(axis = 1, skipna = True)\n",
    "    wideformat_df[\"median_intensity\"] = (wideformat_df[\"median_int_c1\"] + wideformat_df[\"median_int_c2\"])/2\n",
    "    wideformat_df = wideformat_df.loc[both_idx]\n",
    "    wideformat_df[f\"log2fc\"] = np.log2(wideformat_df[f\"median_int_c1\"]) - np.log2(wideformat_df[f\"median_int_c2\"])\n",
    "    return wideformat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def prepare_mq_table(mq_df, columns_intens_c1, columns_intens_c2):\n",
    "    mq_df = mq_df[mq_df[\"Species\"]!= np.nan]\n",
    "    mq_df = mq_df[mq_df['Reverse']!= \"+\"]\n",
    "    mq_df = mq_df[mq_df['Potential contaminant'] != \"+\"]\n",
    "    mq_df = mq_df.rename(columns = {'Species' : \"PG.Organisms\", 'Protein IDs' : 'protein'})\n",
    "    mq_df[columns_intens_c1+columns_intens_c2] = mq_df[columns_intens_c1+columns_intens_c2].replace(0, np.nan)\n",
    "    prots_c1 = mq_df[columns_intens_c1]\n",
    "    prots_c2 = mq_df[columns_intens_c2]\n",
    "\n",
    "    prots_c1 = prots_c1.dropna(thresh = 2)\n",
    "    prots_c2 = prots_c2.dropna(thresh = 2)\n",
    "    both_idx = prots_c1.index.intersection(prots_c2.index)\n",
    "\n",
    "    mq_df[\"median_int_c1\"] = prots_c1.loc[both_idx].median(axis = 1, skipna = True)\n",
    "    mq_df[\"median_int_c2\"] = prots_c2.loc[both_idx].median(axis = 1, skipna = True)\n",
    "    mq_df = mq_df.loc[both_idx]\n",
    "    mq_df[f\"log2fc\"] = np.log2(mq_df[f\"median_int_c1\"]) - np.log2(mq_df[f\"median_int_c2\"])\n",
    "    mq_df[\"method\"] = [\"MaxQuant\" for x in range(len(mq_df.index))]\n",
    "    return mq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System-wide benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def cluster_selected_proteins(protnames, quant_df, normed_c1, normed_c2, pval_threshold_basis = 0.05, fcfc_threshold = 0, take_median_ion=False):\n",
    "    pep2prot = dict(zip(quant_df.index, quant_df['protein']))\n",
    "    ions_to_check = normed_c1.ion2nonNanvals.keys() & normed_c2.ion2nonNanvals.keys()\n",
    "    deedpair2doublediffdist = {}\n",
    "    bgpair2diffDist = {}\n",
    "    p2z = {}\n",
    "    prot2diffions = {}\n",
    "    root_node = anytree.Node('parent')\n",
    "    for ion in ions_to_check:\n",
    "        protein = pep2prot.get(ion)\n",
    "        if protein not in protnames:\n",
    "            continue\n",
    "        vals1 = normed_c1.ion2nonNanvals.get(ion)\n",
    "        vals2 = normed_c2.ion2nonNanvals.get(ion)\n",
    "        diffDist = aqbg.get_subtracted_bg(bgpair2diffDist,normed_c1, normed_c2,ion, p2z)\n",
    "\n",
    "        diffIon = aqdiff.DifferentialIon(vals1, vals2, diffDist, ion,outlier_correction=False)\n",
    "\n",
    "        \n",
    "        prot_ions = prot2diffions.get(protein, list())\n",
    "        prot_ions.append(diffIon)\n",
    "        prot2diffions[protein] = prot_ions\n",
    "    \n",
    "    for prot in prot2diffions.keys():\n",
    "        ions = prot2diffions.get(prot)\n",
    "        clustered_root_node = aqclust.get_scored_clusterselected_ions(prot, ions, normed_c1, normed_c2, bgpair2diffDist, p2z, deedpair2doublediffdist, pval_threshold_basis = pval_threshold_basis, fcfc_threshold = fcfc_threshold, take_median_ion=take_median_ion)\n",
    "        clustered_root_node.parent = root_node\n",
    "    \n",
    "    return root_node\n",
    "\n",
    "import alphaquant.normalization as aqnorm\n",
    "def create_background_dists_from_prepared_files(samplemap_file, quant_file, cond1, cond2):\n",
    "\n",
    "    quant_df = pd.read_csv(quant_file, sep = \"\\t\",index_col='ion')\n",
    "    samplemap_df = aqutils.load_samplemap(samplemap_file)\n",
    "\n",
    "    df_c1, df_c2, c1_samples, c2_samples = get_c1_c2_dfs(quant_df, samplemap_df, [cond1, cond2])\n",
    "\n",
    "    df_c1, df_c2 = aqnorm.get_normalized_dfs(df_c1, df_c2, c1_samples, c2_samples, minrep= min(len(df_c1.columns), len(df_c2.columns)), runtime_plots = False)#filter for no missing values\n",
    "    p2z = {}\n",
    "    normed_c1 = aqbg.ConditionBackgrounds(df_c1, p2z)\n",
    "    normed_c2 = aqbg.ConditionBackgrounds(df_c2, p2z)\n",
    "    return quant_df, normed_c1, normed_c2\n",
    "\n",
    "\n",
    "def get_c1_c2_dfs(unnormed_df, labelmap_df, condpair, minrep = 2):\n",
    "    c1_samples = labelmap_df[labelmap_df[\"condition\"]== condpair[0]]\n",
    "    c2_samples = labelmap_df[labelmap_df[\"condition\"]== condpair[1]]\n",
    "    df_c1 = unnormed_df.loc[:, c1_samples[\"sample\"]].dropna(thresh=minrep, axis=0)\n",
    "    df_c2 = unnormed_df.loc[:, c2_samples[\"sample\"]].dropna(thresh=minrep, axis=0)\n",
    "\n",
    "    return df_c1, df_c2, c1_samples, c2_samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import alphaquant.diff_analysis_manager as aqmgr\n",
    "import alphaquant.normalization as aqnorm\n",
    "import alphaquant.cluster_ions as aqclust\n",
    "import alphaquant.diffquant_utils as aqutils\n",
    "import anytree\n",
    "import math\n",
    "import os\n",
    "import alphaquant.diff_analysis_manager as aqdiffmgr\n",
    "import alphaquant.background_distributions as aqbg\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_real_example_ions(input_file, samplemap_file, num_ions = 20):\n",
    "    p2z = {}\n",
    "    samplemap_df = aqutils.load_samplemap(samplemap_file)\n",
    "    fragion_df = pd.read_csv(input_file, sep = \"\\t\")\n",
    "    _, samplemap_df = aqutils.prepare_loaded_tables(fragion_df, samplemap_df)\n",
    "    fragion_df = fragion_df.set_index('ion')\n",
    "\n",
    "    df_c1, df_c2, c1_samples, c2_samples = format_condpair_input(samplemap_df = samplemap_df, input_file=input_file,condpair = ('S1', 'S2'), minrep= 4)\n",
    "    #df_c1_normed, df_c2_normed = aqnorm.normalize_if_specified(df_c1, df_c2, c1_samples, c2_samples, minrep=4, runtime_plots = False)\n",
    "    normed_c1 = aqbg.ConditionBackgrounds(df_c1, p2z)\n",
    "    normed_c2 = aqbg.ConditionBackgrounds(df_c2, p2z)\n",
    "    diffions = get_subset_of_diffions(normed_c1, normed_c2, num_ions)\n",
    "    return diffions, normed_c1, normed_c2\n",
    "\n",
    "\n",
    "\n",
    "def format_condpair_input(samplemap_df, condpair, minrep, input_file):\n",
    "    print(condpair)\n",
    "    samples_c1, samples_c2 = aqutils.get_samples_used_from_samplemap_df(samplemap_df, condpair[0], condpair[1])\n",
    "    input_df_local = aqdiffmgr.get_unnormed_df_condpair(input_file = input_file, samplemap_df = samplemap_df, condpair = condpair)\n",
    "    df_c1, df_c2 = aqdiffmgr.get_per_condition_dataframes(samples_c1, samples_c2, input_df_local, minrep)\n",
    "    return df_c1, df_c2, samples_c1, samples_c2\n",
    "\n",
    "def get_filtered_protnodes(condpair, results_dir_unfiltered):\n",
    "    condpairtree = aqutils.read_condpair_tree(condpair[0], condpair[1], results_dir_unfiltered)\n",
    "    protnodes = condpairtree.children\n",
    "    selected_protnodes = []\n",
    "    for protnode in protnodes:\n",
    "        filtered_protnode = filter_check_protnode(protnode)\n",
    "        if filtered_protnode == None:\n",
    "            continue\n",
    "        if (filtered_protnode.fc >0.2) or (filtered_protnode.p_val < 0.05) :\n",
    "            continue\n",
    "        selected_protnodes.append(filtered_protnode)\n",
    "    \n",
    "    return selected_protnodes\n",
    "\n",
    "\n",
    "\n",
    "def filter_check_protnode(protnode):\n",
    "\n",
    "    # filter the base nodes, prepare the ms1 and fragion nodes\n",
    "    for base_node in protnode.leaves:\n",
    "        if base_node.cluster != 0:\n",
    "            base_node.parent = None\n",
    "    frgion_ms1_nodes = anytree.search.findall(protnode, filter_=lambda node:  (node.type == 'frgion') or (node.type == 'ms1_isotopes'))\n",
    "\n",
    "    check_nodes = set()\n",
    "    #annotate if fragion_ms1 has enough leafs\n",
    "    for frg_ms1_node in frgion_ms1_nodes:\n",
    "        if len(frg_ms1_node.leaves) <3:\n",
    "            frg_ms1_node.parent = None\n",
    "        else:\n",
    "            check_nodes.add(frg_ms1_node)\n",
    "\n",
    "\n",
    "    type2required_children = {\"mod_seq_charge\":2, \"mod_seq\":2, \"seq\":1, \"gene\":3}\n",
    "    nodetypes = [\"mod_seq_charge\", \"mod_seq\", \"seq\", \"gene\"]\n",
    "\n",
    "    for nodetype in nodetypes:\n",
    "        check_nodes =  anytree.search.findall(protnode, filter_=lambda node:  node.type == nodetype) #for each level, check if there are enough children\n",
    "        for check_node in check_nodes:\n",
    "            num_children = len([x for x in check_node.children if (x.cluster ==0)])\n",
    "            if (num_children<type2required_children.get(check_node.type)) or (check_node.fc > 0.1):\n",
    "                check_node.parent = None\n",
    "                if nodetype == \"gene\":\n",
    "                    return None\n",
    "            else:\n",
    "                check_node.has_enough = True\n",
    "    if len(check_nodes)==0:\n",
    "        return None\n",
    "    else:\n",
    "        return list(check_nodes)[0]\n",
    "\n",
    "\n",
    "import alphaquant.diff_analysis as aqdiff\n",
    "\n",
    "def get_subset_of_diffions(normed_c1, normed_c2, num_ions):\n",
    "    ion2diffDist = {}\n",
    "    p2z = {}\n",
    "    diffions = []\n",
    "    ions_to_check = normed_c1.ion2nonNanvals.keys() & normed_c2.ion2nonNanvals.keys()\n",
    "    count_ions = 0\n",
    "    for idx, ion in enumerate(ions_to_check):\n",
    "        if count_ions==num_ions:\n",
    "            break\n",
    "        vals1 = normed_c1.ion2nonNanvals.get(ion)\n",
    "        vals2 = normed_c2.ion2nonNanvals.get(ion)\n",
    "        diffDist = aqbg.get_subtracted_bg(ion2diffDist,normed_c1, normed_c2,ion, p2z)\n",
    "        diffIon = aqdiff.DifferentialIon(vals1, vals2, diffDist, ion, outlier_correction = False)\n",
    "        diffions.append(diffIon)\n",
    "        count_ions+=1\n",
    "    \n",
    "    return diffions\n",
    "\n",
    "\n",
    "\n",
    "def add_perturbations_to_proteins(protnodes):\n",
    "    #go through each protein and randomly add perturbations at different levels, if a perturbation is added, propagate it to the children etc.\n",
    "    for protnode in protnodes:\n",
    "        for level_nodes in anytree.LevelOrderGroupIter(protnode, filter_= lambda x : 'gene' not in x.type): #iterate through all levels below protein\n",
    "            for nodes_of_interest in group_level_nodes_by_parents(level_nodes):\n",
    "                perturb = np.random.uniform(0, 1) < 0.3 #randomly select ~30% of the samples for perturbation\n",
    "                num_perturb = math.ceil(len(nodes_of_interest)*0.2) if len(nodes_of_interest)>2 else 0\n",
    "                perturb_idxs = random.sample(list(range(len(nodes_of_interest))), num_perturb)\n",
    "                for sub_idx in range(len(nodes_of_interest)):\n",
    "                    node_of_interest = nodes_of_interest[sub_idx]\n",
    "                    applied_shift_parent = 0 if not hasattr(node_of_interest.parent, 'applied_shift') else node_of_interest.parent.applied_shift #check if the parent of the node already had a shift applied, if yes, add this shift\n",
    "                    node_of_interest.applied_shift = applied_shift_parent\n",
    "                    node_of_interest.applied_shift_local = 0\n",
    "                    if (sub_idx in perturb_idxs) and perturb:\n",
    "                        applied_shift = np.random.uniform(-2, 2)\n",
    "                        node_of_interest.applied_shift += applied_shift\n",
    "                        node_of_interest.applied_shift_local = applied_shift\n",
    "\n",
    "\n",
    "def group_level_nodes_by_parents(nodes_of_interest):\n",
    "    parent2nodes = {}\n",
    "    for node in nodes_of_interest:\n",
    "        parent2nodes[node.parent] = parent2nodes.get(node.parent, []) + [node]\n",
    "    return list(parent2nodes.values())\n",
    "\n",
    "def get_filtered_intensity_df(fragion_df, protnodes):    \n",
    "\n",
    "    ions_included = []\n",
    "    for protnode in protnodes:\n",
    "        ions_included.extend([x.name  for x in protnode.leaves if x.type == 'base'])\n",
    "    \n",
    "    #drop the unincluded ions\n",
    "    fragion_df = fragion_df.loc[ions_included]\n",
    "\n",
    "    return fragion_df\n",
    "\n",
    "\n",
    "def get_perturbed_intensity_df(fragion_df, samplemap, protnodes):    \n",
    "    \n",
    "    ion2shift = {}\n",
    "    for protnode in protnodes:\n",
    "        ion2shift.update({x.name : x.applied_shift for x in protnode.leaves if x.type =='base'})\n",
    "    #drop the unincluded ions\n",
    "    fragion_df = fragion_df.loc[list(ion2shift.keys())]\n",
    "\n",
    "    #determine the factors to be added\n",
    "    shifts_up = np.array([np.array([abs(ion2shift.get(x)) if ion2shift.get(x)> 0 else 0 for x in fragion_df.index])])\n",
    "    shifts_down = np.array([np.array([abs(ion2shift.get(x)) if ion2shift.get(x)< 0 else 0 for x in fragion_df.index])])\n",
    "\n",
    "    s1_samples = list(samplemap[samplemap[\"condition\"]==\"S1\"][\"sample\"])\n",
    "    s2_samples = list(samplemap[samplemap[\"condition\"]==\"S2\"][\"sample\"])\n",
    "\n",
    "    fragion_df[s1_samples] =fragion_df[s1_samples] +shifts_up.T\n",
    "    fragion_df[s2_samples] =fragion_df[s2_samples]+shifts_down.T\n",
    "    \n",
    "\n",
    "    return fragion_df\n",
    "\n",
    "\n",
    "def run_perturbation_test(input_file, samplemap, input_file_filtered = None, input_file_perturbed = None, run_diffanalysis_benchm_set = False, run_filtered = True,run_perturbed = True, run_perturbed_no_iontree = True, cluster_threshold_pval_perturbed = 0.01, runtime_plots = True):\n",
    "    condpair_combinations = [(\"S1\", \"S2\")]\n",
    "    results_dir = \"results\"\n",
    "    results_dir_filtered = \"results_filtered\"\n",
    "    results_dir_perturbed = \"results_perturbed\"\n",
    "    results_dir_perturbed_unclustered = \"results_perturbed_unclustered\"\n",
    "    fragion_df = aqutils.import_data(input_file)\n",
    "    samplemap = aqutils.load_samplemap(samplemap)\n",
    "    fragion_df, samplemap = aqutils.prepare_loaded_tables(fragion_df, samplemap)\n",
    "    \n",
    "    \n",
    "    #run the diffanalysis of the basic dataset\n",
    "    if run_diffanalysis_benchm_set:\n",
    "        aqutils.store_method_parameters({'input_file': str(input_file)}, results_dir)\n",
    "        aqmgr.run_pipeline(fragion_df, samplemap, condpair_combinations=condpair_combinations, minrep = 9, runtime_plots=runtime_plots, cluster_threshold_pval=0.05, cluster_threshold_fcfc=0,results_dir=results_dir)\n",
    "    \n",
    "    #filter the analyzed results for consistent, low-FC proteins\n",
    "    if (not os.path.exists(f\"{results_dir}/S1_filtered_VS_S2_filtered.iontrees.json\")) or (input_file_filtered == None):\n",
    "        protnodes_filt = get_filtered_protnodes(condpair_combinations[0], results_dir_unfiltered=results_dir)\n",
    "        fragion_df_only_filt = get_filtered_intensity_df(fragion_df, protnodes_filt)\n",
    "        aqclust.export_roots_to_json(protnodes_filt,(\"S1_filtered\", \"S2_filtered\"), results_dir)\n",
    "        fragion_df_only_filt.reset_index().to_csv(\"filtered_fragions.tsv\", sep = \"\\t\", index = None)\n",
    "    else:\n",
    "        protnodes_filt = aqutils.read_condpair_tree(\"S1_filtered\", \"S2_filtered\", results_dir).children\n",
    "        fragion_df_only_filt = pd.read_csv(input_file_filtered, sep = \"\\t\",index_col='ion')\n",
    "    \n",
    "    #add perturbations to the filtered proteins\n",
    "    if (not os.path.exists(f\"{results_dir_perturbed}/S1_annot_VS_S2_annot.iontrees.json\") or (input_file_perturbed == None)):\n",
    "        add_perturbations_to_proteins(protnodes_filt)\n",
    "        if not os.path.exists(results_dir_perturbed):\n",
    "            os.makedirs(results_dir_perturbed)\n",
    "        aqclust.export_roots_to_json(protnodes_filt,(\"S1_annot\", \"S2_annot\"), results_dir_perturbed)\n",
    "        fragion_df_perturbed = get_perturbed_intensity_df(fragion_df, samplemap, protnodes_filt)\n",
    "        fragion_df_perturbed.reset_index().to_csv(\"perturbed_fragions.tsv\", sep = \"\\t\", index = None)\n",
    "    else:\n",
    "        protnodes_filt = aqutils.read_condpair_tree(\"S1_annot\", \"S2_annot\", results_dir_perturbed).children\n",
    "        fragion_df_perturbed = pd.read_csv(input_file_perturbed, sep = \"\\t\",index_col='ion')\n",
    "    \n",
    "    if run_filtered:\n",
    "        aqmgr.run_pipeline(fragion_df_only_filt, samplemap, condpair_combinations=condpair_combinations, minrep = 9, normalize=True, runtime_plots=runtime_plots, use_iontree_if_possible=False, results_dir= results_dir_filtered)\n",
    "    \n",
    "    if run_perturbed_no_iontree:\n",
    "        aqmgr.run_pipeline(fragion_df_perturbed, samplemap, condpair_combinations=condpair_combinations, minrep = 9,  normalize=True, runtime_plots=runtime_plots,use_iontree_if_possible=False,results_dir=results_dir_perturbed_unclustered)\n",
    "    \n",
    "    if run_perturbed:\n",
    "        aqmgr.run_pipeline(fragion_df_perturbed, samplemap, condpair_combinations=condpair_combinations, minrep = 9,  normalize=True, runtime_plots=runtime_plots, cluster_threshold_pval=cluster_threshold_pval_perturbed, cluster_threshold_fcfc=0,results_dir=results_dir_perturbed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import alphaquant.visualizations as aqviz\n",
    "import alphaquant.diffquant_utils as aqutils\n",
    "import sklearn.metrics\n",
    "\n",
    "\n",
    "def compare_cluster_to_benchmarks(results_dir_unperturbed, results_dir_perturbed, results_dir_perturbed_unclustered):\n",
    "    aqviz.compare_fcs_unperturbed_vs_perturbed_and_clustered(results_dir_unperturbed,results_dir_perturbed, results_dir_perturbed_unclustered)\n",
    "\n",
    "\n",
    "def evaluate_per_level(level2annotated_shift, level2classified_shift):\n",
    "    for level in level2annotated_shift.keys():\n",
    "        y_true = level2annotated_shift.get(level)\n",
    "        y_pred = level2classified_shift.get(level)\n",
    "        metrics = sklearn.metrics.precision_recall_fscore_support(y_true=y_true, y_pred=y_pred)\n",
    "        accuracy = sklearn.metrics.accuracy_score(y_true=y_true, y_pred= y_pred)\n",
    "        print(f\"level {level}\")\n",
    "        print(f\"accuracy:{accuracy}\\tprecision:{metrics[0]}\\trecall{metrics[1]}\\tfscore{metrics[2]}\")\n",
    "\n",
    "\n",
    "def count_correctly_excluded(protnodes_annotated, protnodes_clustered):\n",
    "    level2annotated_shift = {}\n",
    "    level2classified_shift = {}\n",
    "    name2node_annot = {x.name : x for x in protnodes_annotated}\n",
    "    name2node_clustered = {x.name : x for x in protnodes_clustered}\n",
    "    for name in name2node_annot.keys():\n",
    "        protnode_annotated = name2node_annot.get(name)\n",
    "        protnode_clustered = name2node_clustered.get(name)\n",
    "        for annot_nodes in anytree.LevelOrderGroupIter(protnode_annotated, filter_= lambda x : 'gene' not in x.type):\n",
    "            for annot_node in annot_nodes:\n",
    "                clustered_node = anytree.find(protnode_clustered, filter_= lambda x : annot_node.name == x.name)\n",
    "                annot_shifted = annot_node.applied_shift_local!=0\n",
    "                cluster_nonzero = clustered_node.cluster != 0\n",
    "                if (not annot_shifted) and (not cluster_nonzero):\n",
    "                    continue\n",
    "                level2annotated_shift[annot_node.type] = level2annotated_shift.get(annot_node.type, [])\n",
    "                level2classified_shift[annot_node.type] = level2classified_shift.get(clustered_node.type, [])\n",
    "                level2annotated_shift[annot_node.type].append(annot_shifted)\n",
    "                level2classified_shift[annot_node.type].append(cluster_nonzero)\n",
    "    evaluate_per_level(level2annotated_shift, level2classified_shift)\n",
    "\n",
    "\n",
    "def eval_clustered_results(results_perturbed):\n",
    "    protnodes_annot = aqutils.read_condpair_tree(\"S1_annot\", \"S2_annot\", results_folder=results_perturbed).children\n",
    "    protnodes_perturbed = aqutils.read_condpair_tree(\"S1\", \"S2\", results_folder=results_perturbed).children\n",
    "    count_correctly_excluded(protnodes_annot, protnodes_perturbed)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Benchmarking Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pyopenms import ProteaseDigestion, AASequence\n",
    "import pyfasta\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_and_filter_output_table_to_single_organism(input_table, fastas,desired_organism , software_filter_function = None):\n",
    "\n",
    "    undesired_peptides = get_peptides_set(fastas)\n",
    "    print(\"got undesired peptides\")\n",
    "    pd.DataFrame({\"peptide\" : list(undesired_peptides)}).to_csv(f\"{input_table}.undesired_peptides.tsv\", sep = \"\\t\")\n",
    "    print(\"start filtering\")\n",
    "    if software_filter_function == None:\n",
    "        software_filter_function = decide_filter_function(input_table = input_table)\n",
    "    tableit = pd.read_csv(input_table, sep = \"\\t\", chunksize=1000_000)\n",
    "    tables = []\n",
    "    for table_df in tableit:\n",
    "        table_df = software_filter_function(table_df, undesired_peptides, desired_organism)\n",
    "        tables.append(table_df)\n",
    "\n",
    "    yeast_df = pd.concat(tables, ignore_index = True)\n",
    "    return yeast_df\n",
    "    \n",
    "\n",
    "def get_peptides_set(fastas):\n",
    "    peps_merged = set()\n",
    "    for fasta in fastas:\n",
    "        # try:\n",
    "        #     peps = set(pd.read_csv(f\"{fasta}.all_peptides.tsv\", sep = \"\\t\")[\"peptide\"])\n",
    "        # except:\n",
    "            #print(\"could not find digested version of the fasta, try to digest\")\n",
    "        peps = retrieve_all_peptides_from_fasta_and_save(fasta)\n",
    "        peps_merged = peps_merged.union(peps)\n",
    "    return peps_merged\n",
    "\n",
    "def retrieve_all_peptides_from_fasta_and_save(fasta):\n",
    "\n",
    "    digestor = ProteaseDigestion()\n",
    "    digestor.setEnzyme('Trypsin/P')\n",
    "\n",
    "    digestor.setMissedCleavages(2)\n",
    "\n",
    "    f = pyfasta.Fasta(fasta)\n",
    "\n",
    "    all_results = []\n",
    "    for key in f.keys():\n",
    "        protseq = str(f.get(key))\n",
    "        peptides = get_peptides_from_protein_sequence(protseq=protseq, digestor=digestor)\n",
    "        all_results.extend(peptides)\n",
    "    \n",
    "    df = pd.DataFrame({'peptide' : all_results})\n",
    "    df.to_csv(f\"{fasta}.all_peptides.tsv\", index = None)\n",
    "    return set(df[\"peptide\"])\n",
    "\n",
    "\n",
    "def get_peptides_from_protein_sequence(protseq, digestor):\n",
    "    val = AASequence.fromString(protseq)\n",
    "    peptides = []\n",
    "    digestor.digest(val, peptides, 4, 60)\n",
    "    for pep in peptides:\n",
    "        hass_pref = pep.hasPrefix(pep)\n",
    "        if not hass_pref:\n",
    "            print(pep)\n",
    "    peptides = [str(x) for x in peptides]\n",
    "    if len(peptides)>0:\n",
    "        n_terminal_peptide = peptides[0]\n",
    "        m_removed_peptides = get_m_replaced_peps(peptides)\n",
    "        peptides += m_removed_peptides #add the m-removed peptides\n",
    "    return peptides\n",
    "\n",
    "\n",
    "def get_m_replaced_peps(peptides):\n",
    "    m_removed_peptides = []\n",
    "    for peptide in peptides:\n",
    "        m_removed_peptide = peptide[0].replace(\"M\", \"\") + peptide[1:]\n",
    "        m_removed_peptides.append(m_removed_peptide)\n",
    "    return m_removed_peptides\n",
    "\n",
    "\n",
    "\n",
    "def spectronaut_filtering(table_df, undesired_peptides, desired_organism):\n",
    "    table_df = table_df[[(x not in undesired_peptides) for x in table_df['PEP.StrippedSequence']]]\n",
    "    if desired_organism is not None:\n",
    "        table_df = table_df[table_df[\"PG.Organisms\"] == desired_organism]\n",
    "\n",
    "    return table_df\n",
    "\n",
    "def diann_filtering(table_df, undesired_peptides, desired_organism):\n",
    "    table_df = table_df[[(x not in undesired_peptides) for x in table_df['Stripped.Sequence']]]\n",
    "    if desired_organism is not None:\n",
    "        table_df = table_df[[(desired_organism in x) for x in table_df['Protein.Names']]]\n",
    "    return table_df\n",
    "\n",
    "\n",
    "def decide_filter_function(input_table):\n",
    "    columns_table = pd.read_csv(input_table, sep = \"\\t\", nrows=2).columns\n",
    "    if \"PG.Organisms\" in columns_table:\n",
    "        software_filter_function = spectronaut_filtering\n",
    "    elif 'Protein.Names' in columns_table:\n",
    "        software_filter_function = diann_filtering\n",
    "    else:\n",
    "        raise Exception(\"file for filtering does not have the needed columns!\")\n",
    "    return software_filter_function\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_protein_digestion(seq, peptide):\n",
    "    \n",
    "    digestor = ProteaseDigestion()\n",
    "    digestor.setEnzyme('Trypsin/P')\n",
    "\n",
    "    digestor.setMissedCleavages(2)\n",
    "    peptides = get_peptides_from_protein_sequence(seq, digestor)\n",
    "    assert peptide in peptides\n",
    "\n",
    "test_protein_digestion(\"MAKPCGVRLSGEARKQAEEFLYRFLPQKIIYLNQLLQEDSLNVADLTSLRAPLDIPIPDP\", \"AKPCGVR\")\n",
    "\n",
    "test_protein_digestion(seq = \"MAADVSVTHRPPLSPKSGAEVEAGDAAERRAPEEELPPLDPEEIRKRLEHTERQFRNRRK\", peptide = \"AADVSVTHRPPLSPK\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spike-in Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import alphaquant.diffquant_utils as aqutils\n",
    "import seaborn as sns\n",
    "import alphaquant.visualizations as aqplot\n",
    "import os.path\n",
    "import anytree\n",
    "import copy\n",
    "import numpy as np\n",
    "    \n",
    "\n",
    "def compare_aq_to_reference(protein_nodes, expected_log2fc, condpair, software_used, name, original_input_file, samplemap,quant_level_aq, quant_level_reference, tolerance_interval, xlim_lower, xlim_upper, savedir, predscore_cutoff, ml_exclude, percentile_to_retain, num_reps):\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows = 3, ncols = 3, figsize=(15,15))\n",
    "    fig.suptitle(f\"{software_used}, {aqutils.get_condpairname(condpair)}\")\n",
    "    nodes_precursors = generate_precursor_nodes_from_protein_nodes(protein_nodes, type=quant_level_aq)\n",
    "    shift_to_expected_fc(nodes_precursors, expected_log2fc)\n",
    "    \n",
    "    #aqplot.plot_fc_dist_of_test_set(fcs = [x.fc for x in nodes_precursors], ax = ax[2][2])\n",
    "    \n",
    "    true_falses, predscores, reference_scores, fcs = aqplot.get_true_false_to_predscores(nodes_precursors, expected_log2fc)\n",
    "    \n",
    "    aqplot.plot_true_false_fcs_of_test_set(fcs=fcs, true_falses=true_falses, ax= ax[0][0])\n",
    "    aqplot.plot_predictability_roc_curve(true_falses=true_falses, predscores=predscores, reference_scores=reference_scores, ax = ax[0][1], percentile_cutoff_indication=percentile_to_retain)\n",
    "    aqplot.plot_predictability_precision_recall_curve(true_falses=true_falses, predscores=predscores, reference_scores=reference_scores, ax=ax[0][2], percentile_cutoff_indication=percentile_to_retain)\n",
    "\n",
    "    \n",
    "    if predscore_cutoff is not None:\n",
    "        nodes_precursors = [x for x in nodes_precursors if abs(x.predscore)<predscore_cutoff]\n",
    "    if (predscore_cutoff is None) and (ml_exclude):\n",
    "        nodes_precursors = [x for x in nodes_precursors if not x.ml_excluded]\n",
    "\n",
    "\n",
    "    \n",
    "    original_df_reformat = get_original_input_df( c1 = condpair[0], c2 = condpair[1], input_file = original_input_file, samplemap_file = samplemap, input_type = quant_level_reference, num_reps = num_reps)\n",
    "    node_df = get_node_df(nodes_precursors = nodes_precursors)\n",
    "\n",
    "    \n",
    "    if percentile_to_retain is not None:\n",
    "        rough_tpr_cutoff = get_rough_tpr_cutoff(percentile_to_retain, true_falses)\n",
    "        original_df_reformat, node_df = filter_top_qualityscore_percentiles(df_original=original_df_reformat, df_nodes=node_df, nodes_precursors=nodes_precursors, percentile=rough_tpr_cutoff, method=software_used)\n",
    "        doublecheck_df_reformat = filter_score_from_original_df(original_input_file=original_input_file, input_type= quant_level_reference, c1 = condpair[0], c2 = condpair[1], samplemap_file=samplemap,percentile_to_use= rough_tpr_cutoff,minrep=num_reps)\n",
    "\n",
    "    \n",
    "    frac_outliers = aqutils.count_fraction_outliers_from_expected_fc(original_df_reformat, tolerance_interval, expected_log2fc)\n",
    "    aqplot.plot_fc_intensity_scatter(original_df_reformat, f\"{software_used} ({frac_outliers:.2f})\", expected_log2fc = expected_log2fc, tolerance_interval = tolerance_interval, xlim_lower=xlim_lower, xlim_upper = xlim_upper, ax = ax[1][0])\n",
    "    aqplot.plot_fc_intensity_scatter(doublecheck_df_reformat, f\"{software_used} doublecheck\", expected_log2fc = expected_log2fc, tolerance_interval = tolerance_interval, xlim_lower=xlim_lower, xlim_upper = xlim_upper, ax = ax[1][2])\n",
    "    frac_outliers_aq = aqutils.count_fraction_outliers_from_expected_fc(node_df, tolerance_interval, expected_log2fc)\n",
    "    aqplot.plot_fc_intensity_scatter(node_df, f\"AlphaQuant ({frac_outliers_aq:.2f})\", expected_log2fc = expected_log2fc, tolerance_interval = tolerance_interval, xlim_lower=xlim_lower, xlim_upper = xlim_upper, ax = ax[1][1])\n",
    "\n",
    "\n",
    "    aqplot.plot_violin_plots_log2fcs([software_used, 'AlphaQuant'], [original_df_reformat, node_df], ax = ax[2][0])\n",
    "    aqplot.plot_beeswarm_plot_log2fcs([software_used, 'AlphaQuant'], [original_df_reformat, node_df], ax = ax[2][1])\n",
    "\n",
    "    aqplot.plot_outlier_fraction(node_df, reference_df = original_df_reformat, expected_log2fc=expected_log2fc, outlier_thresholds=[1.0, 0.5, 0.3], ax = ax[2][2])\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f\"{savedir}/{name}_{software_used}.pdf\")\n",
    "\n",
    "    ax[0][0].figure.savefig(f\"{savedir}/{name}_fc_dist.pdf\")\n",
    "    ax[0][1].figure.savefig(f\"{savedir}/{name}_predictability_roc_curve.pdf\")\n",
    "    ax[0][2].figure.savefig(f\"{savedir}/{name}_predictability_precision_recall_curve.pdf\")\n",
    "    ax[1][0].figure.savefig(f\"{savedir}/{name}_{software_used}_fc_intensity_scatter.pdf\")\n",
    "    ax[1][1].figure.savefig(f\"{savedir}/{name}_AlphaQuant_fc_intensity_scatter.pdf\")\n",
    "    ax[2][0].figure.savefig(f\"{savedir}/{name}_violin_plot.pdf\")\n",
    "    ax[2][1].figure.savefig(f\"{savedir}/{name}_beeswarm_plot.pdf\")\n",
    "    ax[2][1].figure.savefig(f\"{savedir}/{name}_fraction_outliers.pdf\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def shift_to_expected_fc(nodes_precursors, expected_log2fc):\n",
    "    median_fc = np.median([x.fc for x in nodes_precursors])\n",
    "    diff = expected_log2fc - median_fc\n",
    "    for node in nodes_precursors:\n",
    "        node.fc +=diff\n",
    "\n",
    "\n",
    "def get_rough_tpr_cutoff(percentile_to_retain, true_false_vec):\n",
    "    fraction_true = sum(true_false_vec)/len(true_false_vec)\n",
    "    return fraction_true*percentile_to_retain\n",
    "\n",
    "\n",
    "\n",
    "def get_top_percentile_node_df(nodes, percentile, node_filterfunction = None):\n",
    "    \n",
    "    if node_filterfunction is not None:\n",
    "        nodes = [x for x in nodes if node_filterfunction(x)]\n",
    "    nodes_sorted = sorted(nodes,key= lambda x : abs(x.predscore))\n",
    "    nodes_sorted = nodes_sorted[:int(len(nodes_sorted)*percentile)]\n",
    "    return get_node_df(nodes_sorted)\n",
    "\n",
    "\n",
    "def filter_top_qualityscore_percentiles(df_original, df_nodes, nodes_precursors, percentile, method, node_filterfunction = None):\n",
    "    top_precursors_aqscore, top_precursors_default_quality_score = get_top_percentile_peptides(nodes_precursors=nodes_precursors, percentile = percentile, method = method, node_filterfunction = node_filterfunction)\n",
    "    df_original = df_original[[x in top_precursors_default_quality_score for x in df_original[\"ion\"]]]\n",
    "    df_nodes = df_nodes[[x in top_precursors_aqscore for x in df_nodes[\"ion\"]]]\n",
    "    \n",
    "    return df_original, df_nodes\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "def get_top_percentile_peptides(nodes_precursors, percentile, method, node_filterfunction = None):\n",
    "    if node_filterfunction is not None:\n",
    "        nodes_precursors = [x for x in nodes_precursors if node_filterfunction(x)]\n",
    "    nodes_aqscore_sorted = sorted(nodes_precursors, key = lambda x : abs(x.predscore))\n",
    "    nodes_default_quality_score_sorted = sorted(nodes_precursors, key = lambda x : abs(x.default_quality_score), reverse=True) #the quality scores are higher is better, the predscore is lower is better\n",
    "\n",
    "    #get the percentiles\n",
    "    nodes_default_quality_score_sorted = nodes_default_quality_score_sorted[:int(percentile*len(nodes_default_quality_score_sorted))]\n",
    "    nodes_aqscore_sorted = nodes_aqscore_sorted[:int(percentile*len(nodes_aqscore_sorted))]\n",
    "\n",
    "    #get the precursor names\n",
    "    if method == \"Spectronaut\":\n",
    "        treename2simplename = convert_tree_ionname_to_simple_ionname_sn(nodes_precursors)\n",
    "    if method ==\"DIANN\":\n",
    "        treename2simplename = convert_tree_ionname_to_simple_ionname_diann(nodes_precursors)\n",
    "\n",
    "    precursors_default_quality_score = {treename2simplename.get(x.name) for x in nodes_default_quality_score_sorted}\n",
    "    precursors_aqscore = {x.name for x in nodes_aqscore_sorted}\n",
    "    return precursors_aqscore, precursors_default_quality_score\n",
    "\n",
    "def compare_aq_w_method(nodes_precursors, c1, c2, spectronaut_file, samplemap_file, expected_log2fc = None, threshold = 0.5, input_type = \"spectronaut_precursor\", num_reps = None, method_name = \"Spectronaut\", tolerance_interval = 1, xlim_lower = -1, xlim_upper = 3.5):\n",
    "    specnaut_reformat = get_original_input_df( c1 = c1, c2 = c2, input_file = spectronaut_file, samplemap_file = samplemap_file, input_type = input_type, num_reps = num_reps, expected_log2fc=expected_log2fc)\n",
    "    node_df = get_node_df(nodes_precursors = nodes_precursors)\n",
    "    aqutils.count_fraction_outliers_from_expected_fc(specnaut_reformat, threshold, expected_log2fc)\n",
    "    aqplot.plot_fc_intensity_scatter(specnaut_reformat, method_name, expected_log2fc = expected_log2fc, tolerance_interval = tolerance_interval, xlim_lower=xlim_lower, xlim_upper = xlim_upper)\n",
    "    aqutils.count_fraction_outliers_from_expected_fc(node_df, threshold, expected_log2fc)\n",
    "    aqplot.plot_fc_intensity_scatter(node_df, \"AlphaQuant\", expected_log2fc = expected_log2fc)\n",
    "\n",
    "def import_input_file_in_specified_format(input_file, input_type):\n",
    "    print(f\"use input type {input_type}\")\n",
    "    reformat_file = f\"{input_file}.{input_type}.aq_reformat.tsv\"\n",
    "    if os.path.isfile(reformat_file):\n",
    "        specnaut_reformat = pd.read_csv(reformat_file, sep = \"\\t\", encoding ='latin1')\n",
    "    else:\n",
    "        specnaut_reformat = aqutils.import_data(input_file, input_type_to_use=input_type)\n",
    "    return specnaut_reformat\n",
    "\n",
    "def get_original_input_df(c1, c2, input_file, samplemap_file, num_reps, expected_log2fc  = None,input_type = \"spectronaut_precursor\"):\n",
    "    specnaut_reformat = import_input_file_in_specified_format(input_file=input_file, input_type=input_type)\n",
    "    samplemap_df = aqutils.load_samplemap(samplemap_file)\n",
    "    c1_samples = list(samplemap_df[samplemap_df[\"condition\"]==c1][\"sample\"])\n",
    "    c2_samples = list(samplemap_df[samplemap_df[\"condition\"]==c2][\"sample\"])\n",
    "    specnaut_reformat = annotate_fcs_to_wideformat_table(specnaut_reformat,c1_samples, c2_samples, num_reps = num_reps)\n",
    "    if expected_log2fc is not None:\n",
    "        specnaut_reformat = correct_fcs_to_expected(specnaut_reformat, expected_log2fc)\n",
    "\n",
    "    return specnaut_reformat\n",
    "\n",
    "import numpy as np\n",
    "def correct_fcs_to_expected(specnaut_reformat, expected_log2fc):\n",
    "    log2fcs = specnaut_reformat[\"log2fc\"]\n",
    "    median_fc = np.median(log2fcs)\n",
    "    diff = expected_log2fc-median_fc\n",
    "    specnaut_reformat[\"log2fc\"] = [x+diff for x in log2fcs]\n",
    "    return specnaut_reformat\n",
    "\n",
    "\n",
    "def get_node_df(nodes_precursors):\n",
    "    node_info_dict = {'ion': [x.name for x in nodes_precursors], 'log2fc' : [x.fc for x in nodes_precursors], \"median_intensity\" : [x.min_intensity for x in nodes_precursors]}\n",
    "    node_df = pd.DataFrame(node_info_dict)\n",
    "    return node_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_precursor_nodes_from_protein_nodes(protein_nodes, shift_fc = None, type = \"mod_seq_charge\"):\n",
    "    all_precursors = []\n",
    "    for protein in protein_nodes:\n",
    "        precursors = anytree.findall(protein, filter_= lambda x : (x.type == type))\n",
    "        all_precursors.extend(precursors)\n",
    "    all_precursors = [copy.copy(x) for x in all_precursors]\n",
    "    if shift_fc is not None:\n",
    "        for precursor in all_precursors:\n",
    "            precursor.fc +=shift_fc\n",
    "    return all_precursors\n",
    "\n",
    "import re\n",
    "def convert_tree_ionname_to_simple_ionname_sn(nodes):\n",
    "    tree2simple = {}\n",
    "    for node in nodes:\n",
    "        groups = re.match(\"(.*MOD_)(.*)(_CHARGE_)(.*)(_.*)\",node.name)\n",
    "        tree2simple[node.name] = f\"{groups[2]}.{groups[4]}\"\n",
    "    return tree2simple\n",
    "\n",
    "def convert_tree_ionname_to_simple_ionname_diann(nodes):\n",
    "    tree2simple = {}\n",
    "    for node in nodes:\n",
    "        groups = re.match(\"(.*MOD_)(.*)(_CHARGE_)(.*)(_.*)\",node.name)\n",
    "        tree2simple[node.name] = f\"{groups[2]}{groups[4]}\"\n",
    "    return tree2simple\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def filter_score_from_original_df(original_input_file, input_type, c1, c2, samplemap_file, percentile_to_use, minrep):\n",
    "    _, config_dict, _ = aqutils.get_input_type_and_config_dict(input_file=original_input_file, input_type_to_use=input_type)\n",
    "\n",
    "    sample_id = config_dict.get(\"sample_ID\")\n",
    "    precursor_id = config_dict.get(\"ion_cols\")[0]\n",
    "    \n",
    "\n",
    "    #load samples\n",
    "    samplemap = aqutils.load_samplemap(samplemap_file)\n",
    "    samples_c1 = list(samplemap[[x ==c1 for x in samplemap[\"condition\"]]][\"sample\"])\n",
    "    samples_c2 = list(samplemap[[x ==c2 for x in samplemap[\"condition\"]]][\"sample\"])\n",
    "\n",
    "    #retrieve ions used by AlphaQuant\n",
    "    aq_df = aqbench.import_input_file_in_specified_format(input_file = original_input_file, input_type= input_type)\n",
    "    aq_df = aqutils.filter_df_to_minrep(aq_df, samples_c1, samples_c2, minrep)\n",
    "    ions_used_aq = set(aq_df[\"ion\"])\n",
    "\n",
    "    samplemap = samplemap[[x == c1 or x == c2 for x in samplemap[\"condition\"]]] #only the condition samples remain\n",
    "    condition_samples = set(samplemap[\"sample\"])\n",
    "    reference_df_it = pd.read_csv(original_input_file, sep = \"\\t\", chunksize= 100_000)\n",
    "    reference_dfs = []\n",
    "    for df_chunk in reference_df_it:\n",
    "        df_chunk = df_chunk.drop_duplicates(subset = [sample_id, precursor_id])\n",
    "        df_chunk = df_chunk[[x in ions_used_aq for x in df_chunk[precursor_id]]]\n",
    "        df_chunk = df_chunk[[x in condition_samples for x in df_chunk[sample_id]]]\n",
    "        reference_dfs.append(df_chunk)\n",
    "\n",
    "    reference_df = pd.concat(reference_dfs, ignore_index=True)\n",
    "    \n",
    "    quality_id = aqutils.get_quality_score_column(reference_df)\n",
    "    \n",
    "    reference_df_filtered = filter_top_percentile_reference_df(reference_df, precursor_id, quality_id, percentile_to_use)\n",
    "    filename = f\"{original_input_file}_{aqutils.get_condpairname((c1, c2))}_scorefilt_{percentile_to_use :.2f}.tsv\"\n",
    "    reference_df_filtered.to_csv(filename, sep = \"\\t\", index = None)\n",
    "    reformated_df = read_reformat_filtered_df(filtered_file=filename, input_type_to_use=input_type, samplemap_file=samplemap_file, c1 = c1, c2 = c2, num_rep= minrep)\n",
    "    \n",
    "    return reformated_df\n",
    "\n",
    "\n",
    "def filter_top_percentile_reference_df(reference_df, precursor_id, quality_id, percentile_to_use):\n",
    "    groupedref = reference_df.groupby([precursor_id]).mean().reset_index()\n",
    "    groupedref = groupedref.sort_values(by=quality_id, ascending=False)\n",
    "    first_sample = list(groupedref[precursor_id][:3])\n",
    "\n",
    "    cutoff_threshold = int(percentile_to_use * len(groupedref.index))\n",
    "    best_ranked_ions = list(groupedref[precursor_id][:cutoff_threshold])\n",
    "\n",
    "    assert (first_sample == best_ranked_ions[:3])\n",
    "\n",
    "    filtered_df = reference_df[[x in best_ranked_ions for x in reference_df[precursor_id]]]\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def read_reformat_filtered_df(filtered_file, input_type_to_use, samplemap_file, c1, c2, num_rep):\n",
    "    samplemap_df = aqutils.load_samplemap(samplemap_file)\n",
    "    input_df = aqutils.import_data(filtered_file, input_type_to_use=input_type_to_use)\n",
    "    #input_df, samplemap_df = aqutils.prepare_loaded_tables(input_df, samplemap_df)\n",
    "\n",
    "    c1_samples = list(samplemap_df[samplemap_df[\"condition\"]==c1][\"sample\"])\n",
    "    c2_samples = list(samplemap_df[samplemap_df[\"condition\"]==c2][\"sample\"])\n",
    "    annotated_df = aqbench.annotate_fcs_to_wideformat_table(input_df,c1_samples, c2_samples, num_reps = num_rep)\n",
    "    return annotated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import alphaquant.benchmarking as aqbench\n",
    "import alphaquant.visualizations as aqplot\n",
    "import numpy as np\n",
    "import alphaquant.classify_ions as aqclass\n",
    "import alphaquant.diffquant_utils as aqutils\n",
    "import alphaquant.cluster_ions as aqclust\n",
    "import anytree\n",
    "\n",
    "\n",
    "def benchmark_configs_and_datasets(*,results_dir, expected_log2fcs,condpairs_to_check, original_input_file, samplemap_reference,  software_used, quant_levels_reference, quant_levels_aq = ['mod_seq_charge'], replace_nans = [True], distort_every_nth_precursor = [5, np.inf], \n",
    "predscore_cutoff = None, ml_exclude = False, percentile_to_retain = 0.7, num_reps = 9, num_splits_ml_set = 5):\n",
    "    \"\"\"obtain \"\"\"\n",
    "    \n",
    "    for idx_condpair in range(len(condpairs_to_check)):\n",
    "        condpair = condpairs_to_check[idx_condpair]\n",
    "        for replace_nan in replace_nans:\n",
    "\n",
    "            for distort_modulo in distort_every_nth_precursor:\n",
    "                name_analysis_level = get_benchmark_setting_name(condpair = condpair, replace_nan=replace_nan, distort_number=distort_modulo)\n",
    "                protein_nodes = load_tree_assign_predscores(c1 = condpair[0], c2 = condpair[1], samplemap=samplemap_reference,name= name_analysis_level, results_folder = results_dir, replace_nans= replace_nan,distort_precursor_modulo = distort_modulo, \n",
    "                re_run_assignment=True, num_splits_ml_set = num_splits_ml_set)\n",
    "                for quant_idx in range(len(quant_levels_aq)):\n",
    "                    \n",
    "                    quant_level_aq = quant_levels_aq[quant_idx]\n",
    "                    quant_level_reference = quant_levels_reference[quant_idx]\n",
    "                    name = name_analysis_level+quant_level_reference\n",
    "                    print(f\"TESTING: {name}\")\n",
    "                    \n",
    "                    compare_aq_to_reference(protein_nodes, expected_log2fcs[idx_condpair], condpair=condpair, software_used=software_used, name = name, original_input_file=original_input_file, samplemap=samplemap_reference, quant_level_aq=quant_level_aq, quant_level_reference=quant_level_reference, \n",
    "                    tolerance_interval = 1, xlim_lower = -1, xlim_upper = 3.5,savedir = results_dir,predscore_cutoff = predscore_cutoff, ml_exclude = ml_exclude, percentile_to_retain=percentile_to_retain, num_reps = num_reps)\n",
    "\n",
    "\n",
    "def load_tree_assign_predscores(c1, c2, samplemap,name,results_folder, re_run_assignment  = False, results_folder_diann = None, replace_nans = False, distort_precursor_modulo = np.inf, num_splits_ml_set = 5):\n",
    "    \"\"\"retrieve the predictability scores from a previously run differential analysis. Re-run the predictability score analysis in case they are not available, or if specified\"\"\"\n",
    "    s1, s2 = aqutils.get_samples_used_from_samplemap_file(samplemap, c1, c2)\n",
    "    cpair_tree = aqutils.read_condpair_tree(c1, c2, results_folder=results_folder)\n",
    "    cpair_tree.type = \"asd\"\n",
    "    protnodes = anytree.findall(cpair_tree, filter_= lambda x : (x.type == \"gene\"),maxlevel=2)\n",
    "\n",
    "    if hasattr(protnodes[0],'predscore') and not re_run_assignment:\n",
    "        return protnodes\n",
    "\n",
    "    aqclass.assign_predictability_scores(protnodes,results_folder,name = name, samples_used=s1+s2, precursor_cutoff=2, fc_cutoff=0, number_splits=num_splits_ml_set, plot_predictor_performance=True, replace_nans=replace_nans, distort_precursor_modulo = distort_precursor_modulo)\n",
    "    if results_folder_diann is None:\n",
    "        aqclust.update_nodes_w_ml_score(protnodes)\n",
    "    return protnodes\n",
    "\n",
    "def intersect_with_diann(c1, c2, protnodes,results_folder, results_folder_diann):\n",
    "    diann_intersect = aqclass.get_intersect_sn_diann_precursors(c1, c2, results_folder, results_folder_diann)\n",
    "    for protein in protnodes:\n",
    "        precursors = anytree.findall(protein, filter_= lambda x : (x.type == \"mod_seq_charge\"))\n",
    "        for precursor in precursors:\n",
    "            if precursor.name not in diann_intersect:\n",
    "                precursor.parent = None\n",
    "\n",
    "\n",
    "def get_benchmark_setting_name(condpair, replace_nan, distort_number, diann_intersect_dir = None):\n",
    "    name = f\"{condpair[0]}_{condpair[1]}_distort_every{distort_number}_\"\n",
    "    if replace_nan:\n",
    "        name+=\"nanreplace_\"\n",
    "    if diann_intersect_dir:#in the case that e.g. Spectronaut results are overlayed with DIANN results\n",
    "        if \"no_mbr\" in diann_intersect_dir:\n",
    "            name+=\"diann_intersect_no_mbr\"\n",
    "        else:\n",
    "            name+=\"diann_intersect_mbr\"\n",
    "    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import functools\n",
    "\n",
    "class ResultsTable():\n",
    "    def __init__(self, input_file, input_name, fdr_threshold = 0.05):\n",
    "        self._input_file = input_file\n",
    "        self.input_name = input_name\n",
    "        self.fdr_threshold = fdr_threshold\n",
    "        self.protein_column = \"protein\"\n",
    "        self.called_column = \"called\"\n",
    "        self.species_column = \"species\"\n",
    "        self.formated_dataframe = self.__reformat_input_file_to_default_dataframe()\n",
    "    \n",
    "    def get_proteins(self):\n",
    "        return self.formated_dataframe[self.protein_column]\n",
    "\n",
    "    def subset_to_relevant_columns(self):\n",
    "        return self.formated_dataframe[[self.protein_column, self.called_column]]\n",
    "    \n",
    "    def __reformat_input_file_to_default_dataframe(self):\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ResultsTableSpectronaut(ResultsTable):\n",
    "    def __init__(self, input_file, input_name, fdr_threshold = 0.05):\n",
    "        super().__init__(input_file=input_file, input_name=input_name, fdr_threshold=fdr_threshold)\n",
    "        self.formated_dataframe = self.__reformat_input_file_to_default_dataframe()\n",
    "        self.formated_dataframe = super().subset_to_relevant_columns()\n",
    "        \n",
    "    \n",
    "    def __reformat_input_file_to_default_dataframe(self):\n",
    "        results_df = self.__read_and_rename_input_file()\n",
    "        results_df = self.__determine_called_proteins(results_df)\n",
    "        return results_df\n",
    "\n",
    "    def __read_and_rename_input_file(self):\n",
    "        results_df = pd.read_excel(self._input_file, sheet_name=2)\n",
    "        results_df = results_df.rename(mapper = {'Protein': self.protein_column}, axis=1)\n",
    "        return results_df\n",
    "\n",
    "    def __determine_called_proteins(self, results_df):\n",
    "        results_df[self.called_column] = [x<self.fdr_threshold for x in results_df[\"adjusted.pvalue\"]]\n",
    "        return results_df\n",
    "\n",
    "\n",
    "class ResultsTableAlphaQuant(ResultsTable):\n",
    "    def __init__(self, input_file, input_name, fdr_threshold = 0.05, pre_calculated_table = None):\n",
    "        super().__init__(input_file=input_file, input_name=input_name, fdr_threshold=fdr_threshold)\n",
    "        self.predscore_column = \"predscore\"\n",
    "        self.consistencyscore_column = \"consistency_score\"\n",
    "        if input_file is not None:\n",
    "            results_df = self.__read_input_file()\n",
    "        else:\n",
    "            results_df = pre_calculated_table\n",
    "        self.formated_dataframe = self.__reformat_to_default_dataframe(results_df)\n",
    "        self._formated_dataframe_nofilter = self.formated_dataframe\n",
    "        self.formated_dataframe = self.__subset_to_relevant_columns()\n",
    "\n",
    "    def reduce_formatted_df_to_best_available_score_quantile(self, percentile_to_retain):\n",
    "        if self.predscore_column in self.formated_dataframe.columns:\n",
    "            self.reduce_formatted_df_to_predscore_quantile(percentile_to_retain)\n",
    "        else:\n",
    "            self.reduce_formatted_df_to_consistency_score_quantile(percentile_to_retain)\n",
    "    \n",
    "    def reduce_formatted_df_to_consistency_score_quantile(self, percentile_to_retain):\n",
    "        sorted_df = self.__sort_dataframe_descending_by_consistency_score(self._formated_dataframe_nofilter)\n",
    "        self.__subset_formated_df_to_top_rows(sorted_df, percentile_to_retain)\n",
    "\n",
    "    \n",
    "    def reduce_formatted_df_to_predscore_quantile(self, percentile_to_retain):\n",
    "        sorted_df = self.__sort_by_predscore()\n",
    "        self.__subset_formated_df_to_top_rows(sorted_df, percentile_to_retain)\n",
    "\n",
    "\n",
    "    def __sort_by_predscore(self):\n",
    "        df = self.__set_predscore_values_absolute()\n",
    "        df = self.__sort_dataframe_ascending_by_predscore(df)\n",
    "        return df\n",
    "\n",
    "    def __subset_formated_df_to_top_rows(self, sorted_df, percentile_to_retain):\n",
    "        sorted_df = self.__return_top_rows(sorted_df, percentile_to_retain)\n",
    "        self.formated_dataframe = sorted_df\n",
    "        self.formated_dataframe = self.__subset_to_relevant_columns()\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def __return_top_rows(df, percentile_to_retain):\n",
    "        return df.iloc[:int(percentile_to_retain*len(df.index))]\n",
    "\n",
    "    def __sort_dataframe_descending_by_consistency_score(self, df):\n",
    "        return df.sort_values(by = self.consistencyscore_column, ascending = False).reset_index()\n",
    "\n",
    "    def __sort_dataframe_ascending_by_predscore(self, df):\n",
    "        return df.sort_values(by = self.predscore_column, ascending = True).reset_index()\n",
    "    \n",
    "    def __set_predscore_values_absolute(self):\n",
    "        df = self._formated_dataframe_nofilter\n",
    "        df[self.predscore_column] = abs(self._formated_dataframe_nofilter[self.predscore_column])\n",
    "        return df\n",
    "\n",
    "\n",
    "    def __reformat_to_default_dataframe(self, results_df):\n",
    "        \n",
    "        results_df = self.__determine_called_proteins(results_df)\n",
    "        return results_df\n",
    "\n",
    "    def __read_input_file(self):\n",
    "        results_df = pd.read_csv(self._input_file, sep = \"\\t\")\n",
    "        return results_df\n",
    "\n",
    "    def __determine_called_proteins(self, results_df):\n",
    "        results_df[self.called_column] = [x<self.fdr_threshold for x in results_df[\"fdr\"]]\n",
    "        return results_df\n",
    "\n",
    "    def __subset_to_relevant_columns(self):\n",
    "        df = self.formated_dataframe[[self.protein_column, self.called_column]]\n",
    "        return df\n",
    "\n",
    "\n",
    "import functools\n",
    "class MergedResultsTable(ResultsTable):\n",
    "    def __init__(self, list_of_results_tables):\n",
    "        self._list_of_results_tables = list_of_results_tables\n",
    "        self._list_of_result_dataframes = self.__get_list_of_result_dataframes()\n",
    "        self.protein_column = self.__get_protein_column()\n",
    "        self.called_column = self.__get_called_column()\n",
    "        self.species_column = self.__get_species_column()\n",
    "        self.fdr_threshold = self.__get_fdr_threshold()\n",
    "        self._merge_column = self.protein_column\n",
    "        self.formated_dataframe = self.merge_result_tables()\n",
    "    \n",
    "    def __get_protein_column(self):\n",
    "        return self._list_of_results_tables[0].protein_column\n",
    "    \n",
    "    def __get_called_column(self):\n",
    "        return self._list_of_results_tables[0].called_column\n",
    "\n",
    "    def __get_species_column(self):\n",
    "        return self._list_of_results_tables[0].species_column\n",
    "\n",
    "    def __get_fdr_threshold(self):\n",
    "        return self._list_of_results_tables[0].fdr_threshold\n",
    "    \n",
    "    def __get_list_of_result_dataframes(self):\n",
    "        return [x.formated_dataframe for x in self._list_of_results_tables]\n",
    "    \n",
    "    def merge_result_tables(self):\n",
    "        #self.__find_and_remove_redundant_columns()\n",
    "        merged_table = self.__join_prepared_tables()\n",
    "        merged_table = self.__replace_nans_with_false(merged_table)\n",
    "        return merged_table\n",
    "\n",
    "    def __join_prepared_tables(self):\n",
    "        df_final = functools.reduce(lambda left,right: self.__specify_merge_params(left, right), self._list_of_results_tables)\n",
    "        return df_final\n",
    "\n",
    "    def __specify_merge_params(self, left_resultstable, right_resultstable):\n",
    "        return pd.merge(left_resultstable.formated_dataframe, right_resultstable.formated_dataframe, on=self._merge_column, how = 'outer', suffixes= (\"_\"+left_resultstable.input_name, \"_\"+right_resultstable.input_name))\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def __replace_nans_with_false(merged_table):\n",
    "        return merged_table.fillna(value=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class SpeciesAnnotator():\n",
    "    def __init__(self, mapping_file, protein_column = 'PG.ProteinGroups', species_colum = 'PG.Organisms'):\n",
    "        self._mapping_file = mapping_file\n",
    "        self._protein_column = protein_column\n",
    "        self._species_column = species_colum\n",
    "        self._protein_species_mapping_df = self.__load_reduce_mapping_dataframe()\n",
    "\n",
    "    def annotate_table_with_species(self, results_table):\n",
    "        species_column = results_table.species_column\n",
    "        protein_column = results_table.protein_column\n",
    "\n",
    "        results_df = self.__add_organism_column(results_table.formated_dataframe, species_column, protein_column)\n",
    "        results_df = self.__filter_non_matching_proteins(results_df, species_column)\n",
    "        \n",
    "        results_table.formated_dataframe = results_df\n",
    "\n",
    "    def save_protein_species_map(self, outfile):\n",
    "        self._protein_species_mapping_df.to_csv(outfile, sep = \"\\t\", index = None)\n",
    "    \n",
    "    \n",
    "    def __load_reduce_mapping_dataframe(self):\n",
    "        mapping_df = pd.read_csv(self._mapping_file, sep = \"\\t\", usecols=[self._protein_column, self._species_column], encoding='latin1').drop_duplicates()\n",
    "        mapping_df = self.__filter_double_mapping_species(mapping_df)\n",
    "        return mapping_df\n",
    "    \n",
    "    def __filter_double_mapping_species(self, protein2species_df):\n",
    "        protein2species_df = protein2species_df[[\";\" not in x for x in protein2species_df[self._species_column]]] #a semicolon seperates different species entries\n",
    "        return protein2species_df\n",
    "    \n",
    "\n",
    "    def __add_organism_column(self, results_df,species_column, protein_column):\n",
    "        protein2species_dict = self.__get_protein2species_dict()\n",
    "        results_df[species_column] = [protein2species_dict.get(x) for x in results_df[protein_column]]\n",
    "        return results_df\n",
    "\n",
    "    def __get_protein2species_dict(self):\n",
    "        protein2species = dict(zip(self._protein_species_mapping_df[self._protein_column], self._protein_species_mapping_df[self._species_column]))\n",
    "        return protein2species\n",
    "    \n",
    "    @staticmethod\n",
    "    def __filter_non_matching_proteins(results_df, species_column):\n",
    "        results_df = results_df[[x is not None for x in results_df[species_column]]]\n",
    "        return results_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ClassificationBenchmarker():\n",
    "    def __init__(self, resultstable):\n",
    "        self._resultstable = resultstable\n",
    "        self._protein_column = resultstable.protein_column\n",
    "        self._species_column = resultstable.species_column\n",
    "        self.speciescount_table = self.__get_table_counting_detected_proteins_per_species()\n",
    "        self.variable2falsecountthreshold = self.__calculate_acceptable_number_of_false_calls_per_variable()\n",
    "    \n",
    "    def plot_detected_proteins_per_species(self):\n",
    "        ax = sns.barplot(data=self.speciescount_table, x = self._species_column, y = \"value\", hue=\"variable\")\n",
    "        self.__rotate_x_labels_of_barplot(ax)\n",
    "        self.__annotate_plot_with_acceptable_false_id_numbers(ax)\n",
    "        return ax\n",
    "\n",
    "    def __annotate_plot_with_acceptable_false_id_numbers(self, ax):\n",
    "        for variable in self.variable2falsecountthreshold:\n",
    "            self.__annotate_plot_with_acceptable_false_id_for_given_variable(variable, ax)\n",
    "    \n",
    "    def __annotate_plot_with_acceptable_false_id_for_given_variable(self, variable, ax):\n",
    "        threshold = self.variable2falsecountthreshold.get(variable)\n",
    "        ax.axhline(threshold)\n",
    "    \n",
    "\n",
    "    def __calculate_acceptable_number_of_false_calls_per_variable(self):\n",
    "        variables = self.__get_variable_names()\n",
    "        return self.__get_variable2threshold(variables)\n",
    "    \n",
    "    def __get_variable_names(self):\n",
    "        return self.speciescount_table[\"variable\"].drop_duplicates()\n",
    "\n",
    "    def __get_variable2threshold(self, variables):\n",
    "        variable2threshold = {}\n",
    "        fdr = self._resultstable.fdr_threshold\n",
    "        for variable in variables:\n",
    "            called_proteins = self.__get_number_called_proteins_for_variable(variable)\n",
    "            variable2threshold[variable] = self.__calculate_acceptable_protein_number(called_proteins, fdr)\n",
    "        return variable2threshold\n",
    "\n",
    "    def __get_number_called_proteins_for_variable(self, variable):\n",
    "        subset_variables = self.speciescount_table.set_index(\"variable\").loc[variable]\n",
    "        num_proteins = sum(subset_variables[\"value\"])\n",
    "        return num_proteins\n",
    "\n",
    "    def __test_get_number_called_proteins_for_variable(self):\n",
    "        assert self.__get_number_called_proteins_for_variable(\"called_AlphaQuant\") == 1448\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def __calculate_acceptable_protein_number(called_proteins, fdr):\n",
    "        return int(called_proteins *fdr)\n",
    "\n",
    "    @staticmethod\n",
    "    def __rotate_x_labels_of_barplot(ax):\n",
    "        ax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n",
    "\n",
    "    def __get_table_counting_detected_proteins_per_species(self):\n",
    "        df_melted = self._resultstable.formated_dataframe.melt(id_vars=[self._protein_column, self._species_column])\n",
    "        df_grouped = df_melted.groupby([self._species_column, \"variable\"]).sum().reset_index()\n",
    "        return df_grouped\n",
    "    \n",
    "    def run_tests(self):\n",
    "        self.__test_get_number_called_proteins_for_variable()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
