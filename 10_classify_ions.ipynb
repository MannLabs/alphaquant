{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp classify_ions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "def collect_node_parameters(all_nodes, w_annot = True):\n",
    "    ion2param2val = {}\n",
    "    all_headers = set()\n",
    "    for node in all_nodes:\n",
    "\n",
    "        param2val = get_param2val(node)\n",
    "        if w_annot:\n",
    "            param2val.update({\"ion\" : node.name})\n",
    "        if node.type == 'mod_seq_charge':\n",
    "            if len(node.children) ==2:\n",
    "                fcfc_diff = abs(node.children[0].fc - node.children[1].fc)\n",
    "                param2val.update({\"ms1_ms2_fcfc_diff\" : fcfc_diff})\n",
    "\n",
    "            for child in node.children:\n",
    "                name_frac_mainclust = f\"child_type_{child.type}_frac_mainclust\"\n",
    "                name_num_mainclusts = f\"child_type_{child.type}_num_mainclusts\"\n",
    "                name_variance = f\"child_type_{child.type}_cv_fcs\"\n",
    "                name_replicate_cv = f\"child_type_{child.type}_replicate_cv\"\n",
    "                param2val.update({name_frac_mainclust: child.frac_mainclust, name_num_mainclusts : child.num_mainclusts, name_variance : calc_variance_for_node(child), name_replicate_cv : child.cv})\n",
    "                clusterstats_dict = calc_cluster_stats(child)\n",
    "                param2val.update(clusterstats_dict)\n",
    "        all_headers.update(param2val.keys())\n",
    "        ion2param2val[node.name] = param2val\n",
    "\n",
    "    return get_dataframe(all_nodes,ion2param2val, all_headers)\n",
    "\n",
    "\n",
    "def get_param2val(node):\n",
    "    headers_of_interest = [\"frac_mainclust\", \"num_mainclusts\", \"fraction_consistent\",  \"cv\", \"min_intensity\", \"min_reps\"]\n",
    "    param_dict = node.__dict__\n",
    "    param2val = {x: param_dict.get(x) for x in headers_of_interest}\n",
    "    param2val[\"num_leaves\"] = len(node.leaves)\n",
    "    if len(node.children)>0:\n",
    "        param2val[\"num_children\"] = len(node.children)\n",
    "        param2val[\"cv_fcs\"] = calc_variance_for_node(node)\n",
    "    return param2val\n",
    "\n",
    "def get_dataframe(all_nodes,ion2param2val, all_headers):\n",
    "    all_headers = list(all_headers)\n",
    "    rows = [[ion2param2val.get(node.name).get(header, np.nan) for header in all_headers] for node in all_nodes]\n",
    "    df = pd.DataFrame(rows, columns = all_headers)\n",
    "    return df\n",
    "\n",
    "\n",
    "def calc_cluster_stats(node):\n",
    "    num_elems_secondclust = 0\n",
    "    fcs_clust0 = []\n",
    "    fcs_clust1 = []\n",
    "    intensities = []\n",
    "    for child in node.children:\n",
    "        if child.cluster ==0:\n",
    "            fcs_clust0.append(child.fc)\n",
    "        if child.cluster ==1:\n",
    "            num_elems_secondclust+=1\n",
    "            fcs_clust1.append(child.fc)\n",
    "        intensities.append(child.min_intensity)\n",
    "\n",
    "    if len(fcs_clust1)>0:\n",
    "        betweenclust_fcfc = abs(np.mean(fcs_clust0) - np.mean(fcs_clust1))\n",
    "    else:\n",
    "        betweenclust_fcfc = 8\n",
    "\n",
    "    stats_dict = {f\"child_type_{node.type}_num_elems_secondclust\" : num_elems_secondclust, f\"child_type_{node.type}_num_clusters_total\" : node.num_clusters, f\"child_type_{node.type}_betweenclust_fcfc\" : betweenclust_fcfc,\n",
    "    f\"child_type_{node.type}_num_elems_secondclust\" : np.mean(intensities)}\n",
    "    return stats_dict\n",
    "\n",
    "def calc_variance_for_node(node):\n",
    "\n",
    "    fcs_children = [x.fc for x in node.children]\n",
    "    min_fc = min(fcs_children)\n",
    "    fcs_children = [x.fc - min_fc for x in node.children]\n",
    "    #print(f\"fcs children are {fcs_children}, variance is {np.var(fcs_children)}\")\n",
    "\n",
    "    return scipy.stats.variation(fcs_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import anytree\n",
    "\n",
    "def get_positive_negative_samples(condpairtree,type):\n",
    "    all_nodes = []\n",
    "    for protein_node in condpairtree.children:\n",
    "        if protein_node.num_mainclusts <3:\n",
    "            continue\n",
    "        if protein_node.fraction_consistent < 0.7:\n",
    "            continue\n",
    "        type_nodes = anytree.search.findall(protein_node, filter_=lambda node: node.type == type)\n",
    "        for type_node in type_nodes:\n",
    "            fcdiff = abs(type_node.fc - protein_node.fc)\n",
    "\n",
    "            if fcdiff<0.3:\n",
    "                type_node.positive_example = True\n",
    "                all_nodes.append(type_node)\n",
    "                continue\n",
    "            if fcdiff>2:\n",
    "                type_node.positive_example = False\n",
    "                all_nodes.append(type_node)\n",
    "    return all_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from sklearn.utils import resample\n",
    "import alphaquant.classify_ions as aqclass\n",
    "\n",
    "def get_tp_fp_nodes(condpairtree, nodetype, substantially_off_thresholds, close_to_target_threshold, precursor2fc = None):\n",
    "    res_nodes =[]\n",
    "    condpairtree.type = 'condpair'\n",
    "    nodes = anytree.search.findall(condpairtree, filter_=lambda node:  (node.type == nodetype))\n",
    "    if precursor2fc is not None:\n",
    "        node2fc = get_node2fc(precursor2fc, nodes)\n",
    "\n",
    "    for node in nodes:\n",
    "        if precursor2fc is not None:\n",
    "            fc = node2fc.get(node)\n",
    "        else:\n",
    "            fc = node.fc\n",
    "        if (abs(fc) > substantially_off_thresholds[0]) & (abs(fc) < substantially_off_thresholds[1]):\n",
    "            node.positive_example = False\n",
    "            res_nodes.append(node)\n",
    "        if abs(fc) < close_to_target_threshold:\n",
    "            node.positive_example = True\n",
    "            res_nodes.append(node)\n",
    "    return res_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import re\n",
    "def get_node2fc(precursor2fc, nodes):\n",
    "    node2fc = {}\n",
    "    pattern = \"(SEQ_.*_MOD_)(.*)(_CHARGE_)(.*)(_)\"\n",
    "    if nodes[0].name in precursor2fc.keys():\n",
    "        rename_precursor= False\n",
    "    else:\n",
    "        rename_precursor = True\n",
    "    for node in nodes:\n",
    "        if rename_precursor:\n",
    "            matched = re.search(pattern,node.name)\n",
    "            precursor = f\"{matched.group(2)}.{matched.group(4)}\"\n",
    "        else:\n",
    "            precursor = node.name\n",
    "        fc = precursor2fc.get(precursor)\n",
    "        node2fc[node] = fc\n",
    "\n",
    "    return node2fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import alphaquant.visualizations as aqplot\n",
    "def generate_ml_input(c1, c2, neg_thresholds =[1.5, 3], pos_threshold =  0.2, features_to_exclude = [], precursor2fc = None, results_folder=\"results_renamed\"):\n",
    "    tree = aqplot.read_condpair_tree(c1, c2, results_folder)\n",
    "    pos_negs = get_tp_fp_nodes(tree, 'mod_seq_charge', neg_thresholds, pos_threshold, precursor2fc)\n",
    "    df_precursor_features = aqclass.collect_node_parameters(pos_negs)\n",
    "    df_precursor_features = df_precursor_features.drop(columns = features_to_exclude)\n",
    "\n",
    "    df_precursor_features = balance_classes(df_precursor_features)\n",
    "    #df_precursor_features = df_precursor_features.dropna()\n",
    "    ionnames = df_precursor_features[\"ion\"]\n",
    "    df_precursor_features = df_precursor_features.drop(columns = \"ion\")\n",
    "    df_precursor_features = df_precursor_features.astype('float')\n",
    "    df_precursor_features = df_precursor_features.replace(np.nan, -1)\n",
    "\n",
    "    X_outliers = df_precursor_features.drop(columns=[\"positive_example\"]).to_numpy()\n",
    "    print(\"shapes:\")\n",
    "    print(X_outliers.shape)\n",
    "    y_outliers = df_precursor_features[\"positive_example\"]\n",
    "    print(y_outliers.shape)\n",
    "    featurenames = list(df_precursor_features.drop(columns=[\"positive_example\"]).columns)\n",
    "    return X_outliers, y_outliers, featurenames, ionnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import anytree\n",
    "def ml_filter_and_plot_fcs(ml_classifier, threshold_for_negative_classification, c1, c2, results_folder=\"results_renamed\"):\n",
    "    results_df = aqplot.get_diffresult_dataframe(c1, c2, results_folder=results_folder)\n",
    "    tree = aqplot.read_condpair_tree(c1, c2, results_folder)\n",
    "    tree.type = \"condpair\"\n",
    "    nodes = anytree.findall(tree, filter_= lambda x : x.type == 'mod_seq_charge')\n",
    "    ion2isincluded = get_ion2classification(ml_classifier, nodes, threshold_for_negative_classification)\n",
    "    results_df = results_df[[ion2isincluded.get(x) for x in results_df[\"protein\"]]]\n",
    "\n",
    "    aqplot.volcano_plot(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_ion2classification(ml_classifier, nodes, threshold_for_positive_classification):\n",
    "    df_precursor_features = aqclass.collect_node_parameters(nodes, w_annot=False)\n",
    "    df_precursor_features = df_precursor_features.astype('float')\n",
    "    df_precursor_features = df_precursor_features.replace(np.nan, -1)\n",
    "    X = df_precursor_features.to_numpy()\n",
    "    ionnames = [x.name for x in nodes]\n",
    "    probabilities_positive_example = [x[1] for x in ml_classifier.predict_proba(X)]\n",
    "    ion2included = {ion:(prob>threshold_for_positive_classification ) for ion, prob in zip(ionnames, probabilities_positive_example)}\n",
    "    return ion2included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def balance_classes(df_precursor_features):\n",
    "    df_pos_examples = df_precursor_features[df_precursor_features[\"positive_example\"] ==1]\n",
    "    df_neg_examples = df_precursor_features[df_precursor_features[\"positive_example\"] ==0]\n",
    "    min_length = min(len(df_pos_examples.index), len(df_neg_examples.index))\n",
    "    df_pos_examples = resample(df_pos_examples, replace = False, n_samples = int(min_length), random_state = 123)\n",
    "    df_neg_examples = resample(df_neg_examples, replace = False, n_samples = min_length, random_state = 123)\n",
    "    df_downsampled = pd.concat([df_pos_examples, df_neg_examples], ignore_index=True)\n",
    "    print(sum(df_downsampled[\"positive_example\"]))\n",
    "    print(len(df_downsampled.index))\n",
    "    return df_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import resample\n",
    "def load_feature_df(c1, c2, neg_thresholds =[1.5, 3], pos_threshold =  0.2, features_to_exclude = []):\n",
    "    tree = aqplot.read_condpair_tree(c1, c2, results_folder=\"results\")\n",
    "    pos_negs = get_tp_fp_nodes(tree, 'mod_seq_charge', neg_thresholds, pos_threshold)\n",
    "    df_precursor_features = aqclass.collect_node_parameters(pos_negs)\n",
    "    df_precursor_features = df_precursor_features.drop(columns = features_to_exclude)\n",
    "    return df_precursor_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_precision_recall(classfier, X_train, y_train, X_test, y_test):\n",
    "      y_score = classfier.decision_function(X_train)\n",
    "      average_precision = average_precision_score(y_train, y_score)\n",
    "\n",
    "      print('Average precision-recall score: {0:0.2f}'.format(\n",
    "            average_precision))\n",
    "\n",
    "\n",
    "      disp = plot_precision_recall_curve(classfier, X_test, y_test)\n",
    "      disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "                        'AP={0:0.2f}'.format(average_precision))\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_precursor2fc(cond1, cond2, results_folder):\n",
    "    result_df = aqviz.get_diffresult_dataframe(cond1, cond2, results_folder)\n",
    "    result_dict = dict(zip(result_df[\"protein\"], result_df[\"log2fc\"]))\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import alphaquant.diffquant_utils as aqutils\n",
    "\n",
    "def get_nodes_of_type(cond1, cond2, results_folder, node_type = 'mod_seq_charge'):\n",
    "\n",
    "    tree_sn = aqutils.read_condpair_tree(cond1, cond2, results_folder=results_folder)\n",
    "    tree_sn.type = \"asd\"\n",
    "    return anytree.findall(tree_sn, filter_= lambda x : (x.type == node_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import alphaquant.classify_ions as aqclass\n",
    "import alphaquant.visualizations as aqplot\n",
    "import anytree\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "def get_fc_normalized_nodes(nodes_fclevel, type_lowerlevel, min_nums_lowerlevel = 2, fc_cutoff = 1.0, distort_precursor_modulo = np.inf):\n",
    "    \"\"\"\"get nodes of type lowerlevel which are normalized by the fclevel fold change\"\"\"\n",
    "    randnr_generator = random.Random(42)\n",
    "    normalized_lowerlevels = [] #the normalized lowerlevels are copied values used for training, better change to different variable names to be used\n",
    "    all_lowerlevels = []\n",
    "    count_precursors = 0\n",
    "    for prot in nodes_fclevel:\n",
    "        precursors = anytree.findall(prot, filter_= lambda x : (x.type == type_lowerlevel))\n",
    "        all_lowerlevels.extend(precursors)\n",
    "        if len(precursors)<min_nums_lowerlevel:\n",
    "            continue\n",
    "        fc_prot = prot.fc\n",
    "        if abs(fc_prot) < fc_cutoff:\n",
    "            continue\n",
    "        for precursor in precursors: #shift every peptide by the protein fold change, this should make peptides from different proteins comparable and enable analyses/ml on this dataset\n",
    "            precursor = copy.copy(precursor)\n",
    "            precursor.fc = precursor.fc - fc_prot\n",
    "            if count_precursors%distort_precursor_modulo==0:\n",
    "                perturbation = randnr_generator.uniform(-2, 2)\n",
    "                precursor.fc=precursor.fc + perturbation\n",
    "                precursor.perturbation_added = perturbation\n",
    "            normalized_lowerlevels.append(precursor)\n",
    "            count_precursors+=1\n",
    "\n",
    "    return normalized_lowerlevels, all_lowerlevels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def get_intersect_sn_diann_precursors(cond1, cond2, sn_folder, diann_folder):\n",
    "    precursor_nodes_diann = get_nodes_of_type(cond1, cond2, diann_folder, node_type = 'mod_seq_charge')\n",
    "    precursor_nodes_sn = get_nodes_of_type(cond1, cond2, sn_folder, node_type = 'mod_seq_charge')\n",
    "    precursors_diann = {x.name.replace(\"_MOD_\", \"_MOD__\").replace(\"_CHARGE\", \"__CHARGE\") for x in precursor_nodes_diann}\n",
    "    precursors_sn = {x.name for x in precursor_nodes_sn}\n",
    "\n",
    "    intersect = precursors_diann.intersection(precursors_sn)\n",
    "\n",
    "    return intersect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_nans_feature_dependent(df_precursor_features):\n",
    "    colums_lower_is_worse = [x for x in  df_precursor_features.columns if 'frac_mainclust' in x]\n",
    "    columns_higher_is_worse = [x for x in df_precursor_features.columns if x not in colums_lower_is_worse]\n",
    "    df_precursor_features[colums_lower_is_worse] = df_precursor_features[colums_lower_is_worse].replace(np.nan, 0)\n",
    "    df_precursor_features[columns_higher_is_worse] = df_precursor_features[columns_higher_is_worse].replace(np.nan, 10)\n",
    "\n",
    "    return df_precursor_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def generate_ml_input_regression(df_precursor_features, nodes, replace_nans = False):\n",
    "    ion2fc = {x.name: x.fc for x in nodes}\n",
    "    if replace_nans:\n",
    "        df_precursor_features = replace_nans_feature_dependent(df_precursor_features)\n",
    "    else:\n",
    "        df_precursor_features = df_precursor_features.dropna()\n",
    "    df_precursor_features = df_precursor_features[[(x in ion2fc.keys()) for x in df_precursor_features[\"ion\"]]]\n",
    "    ionnames = list(df_precursor_features[\"ion\"])\n",
    "    df_precursor_features = df_precursor_features.drop(columns=[\"ion\"])\n",
    "    X = df_precursor_features.to_numpy()\n",
    "    y = np.array([ion2fc.get(ion) for ion in ionnames])\n",
    "    featurenames = list(df_precursor_features.columns)\n",
    "    return X, y, featurenames, ionnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import alphaquant.visualizations as aqplot\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import alphaquant.classify_ions as aqclass\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "\n",
    "def do_linear_regression(X, y, featurenames, ionnames, prediction_cutoff):\n",
    "    regr = linear_model.LinearRegression()\n",
    "    X = scale_input(X)\n",
    "    # Split the data into training/testing sets\n",
    "    fc2ion = {x:y for x,y in zip(y, ionnames)}\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    y_pred = regr.predict(X_test)\n",
    "\n",
    "    # The coefficients\n",
    "    print('Coefficients: \\n', regr.coef_)\n",
    "    # The mean squared error\n",
    "    print('Mean squared error: %.2f'\n",
    "        % mean_squared_error(y_test, y_pred))\n",
    "    # The coefficient of determination: 1 is perfect prediction\n",
    "    print('Coefficient of determination: %.2f'\n",
    "        % r2_score(y_test, y_pred))\n",
    "\n",
    "    # Plot outputs\n",
    "    #plt.plot(X_test[:,-1], y_pred,  color='black')\n",
    "    plt.scatter(y_test, y_pred, color='blue')\n",
    "    plt.show()\n",
    "    aqclass.plot_feature_importances(regr.coef_,featurenames, 10)\n",
    "    plt.show()\n",
    "    aqplot.plot_predicted_fc_histogram(y_test, y_pred, prediction_cutoff, show_filtered=False)\n",
    "    plt.show()\n",
    "    aqplot.plot_predicted_fc_histogram(y_test, y_pred, prediction_cutoff, show_filtered=True)\n",
    "    plt.show()\n",
    "    print_good_predicitions(y_test, y_pred, fc2ion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "\n",
    "def do_random_forest_regression(X, y, featurenames, prediction_cutoff):\n",
    "      # Split the data into training/testing sets\n",
    "      X = scale_input(X)\n",
    "      X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "      # Create linear regression object\n",
    "      regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "\n",
    "      # Train the model using the training sets\n",
    "      regr.fit(X_train, y_train)\n",
    "\n",
    "      # Make predictions using the testing set\n",
    "      y_pred = regr.predict(X_test)\n",
    "\n",
    "\n",
    "      # The mean squared error\n",
    "      print('Mean squared error: %.2f'\n",
    "            % mean_squared_error(y_test, y_pred))\n",
    "      # The coefficient of determination: 1 is perfect prediction\n",
    "      print('Coefficient of determination: %.2f'\n",
    "            % r2_score(y_test, y_pred))\n",
    "\n",
    "      # Plot outputs\n",
    "      #plt.plot(X_test[:,-1], y_pred,  color='black')\n",
    "      plt.scatter(y_test, y_pred, color='blue')\n",
    "      plt.show()\n",
    "      aqclass.plot_feature_importances(regr.feature_importances_,featurenames, 10)\n",
    "      plt.show()\n",
    "      aqplot.plot_predicted_fc_histogram(y_test, y_pred, prediction_cutoff, show_filtered=False)\n",
    "      plt.show()\n",
    "      aqplot.plot_predicted_fc_histogram(y_test, y_pred, prediction_cutoff, show_filtered=True)\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import sklearn.preprocessing\n",
    "\n",
    "def scale_input(X):\n",
    "    scaler = sklearn.preprocessing.StandardScaler().fit(X)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    return X_scaled\n",
    "\n",
    "def scale_input_minmax(X):\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    X_transformed = scaler.transform(X)\n",
    "    return X_transformed\n",
    "\n",
    "\n",
    "def print_good_predicitions(y_test, y_pred, fc2ionnames,top_n = 10):\n",
    "    tuples = list(zip(y_test, y_pred))\n",
    "    tuples.sort(key = lambda x: min(abs(np.array([x[0], x[1]]))), reverse=True)\n",
    "    print('printing names of some well predicted ions')\n",
    "    for idx in range(top_n):\n",
    "        print(f\"{tuples[idx]}\\t{fc2ionnames.get(tuples[idx][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import random\n",
    "import math\n",
    "\n",
    "def get_indices_for_cross_predict(y, number_splits):\n",
    "    length_subset = math.floor(len(y)/number_splits)\n",
    "    y_idxs = list(range(len(y)))\n",
    "    random.shuffle(y_idxs)\n",
    "    chunks_excluded = []\n",
    "    chunks_included = []\n",
    "    for i in range(0, len(y), length_subset):\n",
    "        idxs_excluded = y_idxs[i:i + length_subset]\n",
    "        idxs_included = [idx for idx in y_idxs if idx not in idxs_excluded]\n",
    "        chunks_excluded.append(idxs_excluded)\n",
    "        chunks_included.append(idxs_included)\n",
    "    return chunks_included, chunks_excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def get_trained_predictor_and_predicted_trainset(X, y, ionnames, number_splits, regr):\n",
    "    y = np.array(y)\n",
    "    X = scale_input(X)\n",
    "\n",
    "    chunks_included, chunks_excluded = get_indices_for_cross_predict(y, number_splits)\n",
    "    y_test_all = []\n",
    "    y_pred_all = []\n",
    "    ionnames_all = []\n",
    "    all_excluded = {}\n",
    "    for i in range(len(chunks_included)):\n",
    "        idxs_in = chunks_included[i]\n",
    "        idxs_out = chunks_excluded[i]\n",
    "\n",
    "        if len(set(idxs_out).intersection(all_excluded))>0:\n",
    "            raise Exception('train set in test set!')\n",
    "\n",
    "        X_train = X[idxs_in,:]\n",
    "        y_train = y[idxs_in]\n",
    "        X_test = X[idxs_out, :]\n",
    "        y_test = y[idxs_out]\n",
    "        regr.fit(X_train, y_train)\n",
    "        # Make predictions using the testing set\n",
    "        y_pred = regr.predict(X_test)\n",
    "        print(f\"round {i} w. {len(idxs_in)} ions used for training\")\n",
    "        y_test_all.extend(y_test)\n",
    "        y_pred_all.extend(y_pred)\n",
    "        ionnames_all.extend(ionnames[idxs_out])\n",
    "\n",
    "    return y_test_all, y_pred_all, ionnames_all, regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import alphaquant.diffquant_utils as aqutils\n",
    "\n",
    "def predict_remaining_dataset(all_precursors, regr, y_pred_cp, ionnames_cp, acquisition_info_df):\n",
    "    precursors_to_predict = [x for x in all_precursors if x.name not in ionnames_cp]\n",
    "    df_precursor_features = collect_node_parameters(precursors_to_predict)\n",
    "    merged_df = aqutils.merge_acquisition_df_parameter_df(acquisition_info_df, df_precursor_features)\n",
    "    X_remaining  = generate_ml_input_regression(merged_df, precursors_to_predict, replace_nans=True)[0]\n",
    "    y_pred_remaining = regr.predict(X_remaining)\n",
    "    ionnames_remaining = [x.name for x in precursors_to_predict]\n",
    "    y_pred_total = list(y_pred_cp) + list(y_pred_remaining)\n",
    "    ionnames_total = list(ionnames_cp) + list(ionnames_remaining)\n",
    "\n",
    "    return y_pred_total, ionnames_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import scipy.stats\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import alphaquant.visualizations as aqplot\n",
    "import alphaquant.normalization as aqnorm\n",
    "\n",
    "def find_nearest(array, values):\n",
    "    indices = np.abs(np.subtract.outer(array, values)).argmin(0)\n",
    "    return indices\n",
    "\n",
    "def find_mean_and_cutoffs(y_pred, visualize = False):\n",
    "    data = np.array(y_pred)\n",
    "    gmm = GaussianMixture(n_components = 2).fit(data.reshape(-1, 1))\n",
    "    if visualize:\n",
    "        aqplot.visualize_gaussian_mixture_fit(gmm,y_pred)\n",
    "        plt.hist(data, bins=50, histtype='step', density=True, alpha=0.5, label='before shift')\n",
    "    weight_mean_cov = list(zip(gmm.weights_, gmm.means_, gmm.covariances_))\n",
    "    weight_mean_cov.sort(key = lambda x : x[0], reverse=True)\n",
    "    weight_mean_cov = weight_mean_cov[0] # get the ND with the largest weight\n",
    "    mean = weight_mean_cov[1][0]\n",
    "    stdev = np.sqrt(weight_mean_cov[2][0][0])\n",
    "    val_neg = scipy.stats.norm.ppf(0.01,  scale = stdev)\n",
    "    val_pos = scipy.stats.norm.ppf(0.99,  scale = stdev)\n",
    "    print(f\"the mean is {mean}\")\n",
    "    if visualize:\n",
    "        plt.hist(y_pred-mean, bins=50, histtype='step', density=True, alpha=0.5, label= 'after_shift')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return mean ,val_neg, val_pos\n",
    "\n",
    "\n",
    "def fit_gaussian_to_dist_around_mode(dist, visualize = False):\n",
    "    #cut out the \"core\" ~50% of the distribution, which I assume to be gaussian, then fit a gaussian to this core and determine mean and stdev\n",
    "    dist = np.sort(np.array(dist))\n",
    "    mode = aqnorm.determine_mode_iteratively(dist)\n",
    "    mode_idx = find_nearest(dist, mode)\n",
    "\n",
    "    cumsum_rel = np.array(list(range(len(dist))))/len(dist)\n",
    "    mode_quantile = cumsum_rel[mode_idx]\n",
    "    subrange = min([mode_quantile, 0.34, 1-mode_quantile]) #if the mode is close to the edge, make the interval smaller\n",
    "\n",
    "    print(f\"subrange: {subrange}\")\n",
    "    quantile1 = mode_quantile-subrange\n",
    "    quantile2 = mode_quantile+subrange\n",
    "    idx_start = find_nearest(cumsum_rel, quantile1)\n",
    "    idx_end = find_nearest(cumsum_rel, quantile2)\n",
    "    print(f'before_adjust: start {idx_start} mode {mode_idx} end {idx_end}')\n",
    "    #try to fit around the same \"widht\" around the mode\n",
    "    difference_start_mode = mode_idx - idx_start\n",
    "    difference_mode_end = idx_end - mode_idx\n",
    "    if difference_start_mode>difference_mode_end:\n",
    "        idx_end = mode_idx+difference_start_mode\n",
    "    if difference_mode_end> difference_start_mode:\n",
    "        idx_start = mode_idx-difference_mode_end\n",
    "    print(f'after adjust: start {idx_start} mode {mode_idx} end {idx_end}')\n",
    "\n",
    "    dist_subset = dist[idx_start:idx_end]\n",
    "    gmm = GaussianMixture(n_components = 1).fit(dist_subset.reshape(-1, 1))\n",
    "    mean = gmm.means_[0]\n",
    "    var = gmm.covariances_[0]\n",
    "    empirical_stdev = (dist[idx_end] - dist[idx_start])/2\n",
    "    if visualize:\n",
    "        aqplot.visualize_gaussian_nomix_subfit(mode, (empirical_stdev)**2,dist,dist_subset)\n",
    "        plt.hist(dist, bins=50, histtype='step', density=True, alpha=0.5, label='before shift')\n",
    "        plt.hist(dist_subset,  bins=25, histtype='step', density=True, alpha=0.5, label='before shift')\n",
    "        plt.show()\n",
    "    mean = gmm.means_[0]\n",
    "    stdev = np.sqrt(gmm.covariances_[0])\n",
    "    val_neg = scipy.stats.norm.ppf(1e-5,  scale = empirical_stdev)\n",
    "    val_pos = scipy.stats.norm.ppf(1-1e-5,  scale = empirical_stdev)\n",
    "    return mode, val_neg, val_pos\n",
    "\n",
    "\n",
    "def gauss(x, a, x0, sigma):\n",
    "    return a * np.exp(-(x - x0)**2 / (2 * sigma**2))\n",
    "\n",
    "\n",
    "def gaussian(x, amp, cen, wid):\n",
    "    \"\"\"1-d gaussian: gaussian(x, amp, cen, wid)\"\"\"\n",
    "    return (amp / (np.sqrt(2*np.pi) * wid)) * np.exp(-(x-cen)**2 / (2*wid**2))\n",
    "\n",
    "\n",
    "from lmfit import Model\n",
    "from scipy.stats import norm\n",
    "def fit_gaussian_to_subdist(dist, visualize, results_dir = None):\n",
    "    #cut out the middle of the data\n",
    "    dist = np.sort(np.array(dist))\n",
    "    dist_hist = np.histogram(dist, bins = int(len(dist)/3))\n",
    "    counts= dist_hist[0]\n",
    "    bins = dist_hist[1]\n",
    "    bins_middled = [(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)]\n",
    "    \n",
    "    mode = aqnorm.determine_mode_iteratively(dist)\n",
    "    mode_idx = find_nearest(bins_middled, mode)\n",
    "\n",
    "    cumsum_rel = np.cumsum(counts)/sum(counts)\n",
    "\n",
    "    mode_quantile = cumsum_rel[mode_idx]\n",
    "    subrange = min([mode_quantile, 0.4, 1-mode_quantile]) #if the mode is close to the edge, make the interval smaller\n",
    "\n",
    "    print(f\"subrange: {subrange}\")\n",
    "    quantile1 = mode_quantile-subrange\n",
    "    quantile2 = mode_quantile+subrange\n",
    "    idx_start = 0#find_nearest(cumsum_rel, quantile1)\n",
    "    idx_end = len(cumsum_rel)-1#find_nearest(cumsum_rel, quantile2)\n",
    "    x = bins_middled[idx_start:idx_end]\n",
    "    y = counts[idx_start:idx_end]\n",
    "\n",
    "\n",
    "    gmodel = Model(gaussian)\n",
    "    result = gmodel.fit(y, x=x, amp=5, cen=0, wid=1)\n",
    "    fit_results = result.__dict__[\"best_values\"]\n",
    "    mean = fit_results[\"cen\"]\n",
    "    stdev = fit_results[\"wid\"]\n",
    "    weight = fit_results[\"amp\"]\n",
    "\n",
    "    if visualize:\n",
    "        plt.hist(dist, bins=int(len(dist)/3), alpha = 0.5)\n",
    "        fit_results = weight*norm.pdf(bins_middled, mean, stdev).ravel()\n",
    "        plt.plot(bins_middled, fit_results, '-', label = 'best fit',linewidth = 1.5, c = 'black')\n",
    "        #plt.plot(x, result.best_fit, '-', label='best fit')\n",
    "        plt.legend()\n",
    "        if results_dir is not None:\n",
    "            plt.savefig(f'{results_dir}/ml_offsets_gaussian_fit.pdf')\n",
    "        plt.show()\n",
    "    \n",
    "    print(stdev)\n",
    "    #val_neg = scipy.stats.norm.ppf(0.001,  scale = stdev)\n",
    "    #val_pos = scipy.stats.norm.ppf(0.999,  scale = stdev)\n",
    "    val_neg = -3*stdev\n",
    "    val_pos = 3*stdev\n",
    "    print(mean, val_neg, val_pos)\n",
    "    return mean, val_neg, val_pos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def annotate_precursor_nodes(cutoff_neg, cutoff_pos, y_pred_total, ionnames_total, precursor_nodes):\n",
    "    precursor2predscore = {x:y for x, y in zip(ionnames_total, y_pred_total)}\n",
    "    for precursor in precursor_nodes:\n",
    "        predscore = precursor2predscore.get(precursor.name)\n",
    "        ml_excluded = not ((predscore>cutoff_neg) & (predscore<cutoff_pos))\n",
    "        precursor.predscore = predscore\n",
    "        precursor.ml_excluded = bool(ml_excluded)\n",
    "        precursor.cutoff = cutoff_pos #cutoff_pos is -cutoff_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import alphaquant.visualizations as aqplot\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def balance_small_and_strong_fcs(normed_nodes, cutoff):\n",
    "    smaller_pep_idxs = []\n",
    "    larger_peps_idxs = []\n",
    "    for idx in range(len(normed_nodes)):\n",
    "        fc = normed_nodes[idx].fc\n",
    "        if abs(fc)< cutoff:\n",
    "            smaller_pep_idxs.append(idx)\n",
    "        else:\n",
    "            larger_peps_idxs.append(idx)\n",
    "    if len(larger_peps_idxs)< len(smaller_pep_idxs):\n",
    "        print(\"balancing subsets\")\n",
    "        print(f\"{len(larger_peps_idxs)} of large vs {len(smaller_pep_idxs)} of small fcs\")\n",
    "        smaller_pep_idxs = random.sample(smaller_pep_idxs, len(larger_peps_idxs))\n",
    "    else:\n",
    "        print(\"skip balancing subsets\")\n",
    "        return normed_nodes\n",
    "    new_idxs = smaller_pep_idxs+larger_peps_idxs\n",
    "    return [normed_nodes[i] for i in range(len(new_idxs))]\n",
    "\n",
    "\n",
    "def random_forest_iterative_cross_predict(X, y, ionnames, number_splits, regr, balancing_cutoff = np.inf):\n",
    "    X = scale_input(X)\n",
    "    ionnames = np.array(ionnames)\n",
    "    #X_balanced, y_balanced, ionnames_balanced = balance_small_and_strong_fcs(X, y, ionnames,balancing_cutoff)\n",
    "    #X_not_balanced, y_not_balanced  = get_balance_excluded_subset(y_balanced, y, X)\n",
    "\n",
    "    chunks_included, chunks_excluded = get_indices_for_cross_predict(y, number_splits)\n",
    "    y_test_all = []\n",
    "    y_pred_all = []\n",
    "    ionnames_all = []\n",
    "    all_excluded = {}\n",
    "    for i in range(len(chunks_included)):\n",
    "        idxs_in = chunks_included[i]\n",
    "        idxs_out = chunks_excluded[i]\n",
    "\n",
    "        if len(set(idxs_out).intersection(all_excluded))>0:\n",
    "            print(\"overfitting alarm!\")\n",
    "            all_excluded.update(set(idxs_out))\n",
    "\n",
    "        X_train = X[idxs_in,:]\n",
    "        y_train = y[idxs_in]\n",
    "        X_test = X[idxs_out, :]\n",
    "        y_test = y[idxs_out]\n",
    "        regr.fit(X_train, y_train)\n",
    "        if i==0:\n",
    "            regr_export = regr\n",
    "        # Make predictions using the testing set\n",
    "        y_pred = regr.predict(X_test)\n",
    "        \n",
    "        y_test_all.extend(y_test)\n",
    "        y_pred_all.extend(y_pred)\n",
    "        ionnames_all.extend(ionnames[idxs_out])\n",
    "\n",
    "    return y_test_all, y_pred_all, ionnames_all, regr_export\n",
    "    #aqplot.get_error_and_scatter_ml_regression(y_test_all, y_pred_all, scatter_filt)\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "def get_indices_for_cross_predict(y, number_splits):\n",
    "    length_subset = math.floor(len(y)/number_splits)\n",
    "    y_idxs = list(range(len(y)))\n",
    "    random.Random(42).shuffle(y_idxs) #set seed 42\n",
    "    chunks_excluded = []\n",
    "    chunks_included = []\n",
    "    for i in range(0, len(y), length_subset):\n",
    "        idxs_excluded = y_idxs[i:i + length_subset]\n",
    "        idxs_included = [idx for idx in y_idxs if idx not in idxs_excluded]\n",
    "        chunks_excluded.append(idxs_excluded)\n",
    "        chunks_included.append(idxs_included)\n",
    "    return chunks_included, chunks_excluded\n",
    "\n",
    "\n",
    "def get_balance_excluded_subset(y_balanced, y, X):\n",
    "    idxs_y_balance_excluded = [x for x in range(len(y)) if (y[x] not in y_balanced)]\n",
    "    y_not_balanced = np.array(y)[idxs_y_balance_excluded]\n",
    "    X_not_balanced = X[idxs_y_balance_excluded,:]\n",
    "    return X_not_balanced, y_not_balanced\n",
    "\n",
    "\n",
    "def calculate_log_loss_scores_for_prediction(y_test, y_pred):\n",
    "    loss_scores = abs(np.log2(y_test) - np.log2(y_pred))\n",
    "    return loss_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 21, 87, 5, 49, 13, 121, 129, 141, 143, 92, 157, 23, 97, 45, 108, 27, 63, 55, 173]\n",
      "[176, 4, 103, 61, 111, 184, 163, 181, 75, 131, 78, 107, 198, 123, 47, 35, 1, 91, 33, 195]\n",
      "[51, 62, 118, 167, 31, 72, 151, 25, 155, 8, 89, 65, 169, 179, 36, 178, 137, 194, 159, 12]\n",
      "[88, 140, 191, 142, 18, 160, 76, 125, 161, 41, 73, 77, 58, 93, 3, 114, 166, 113, 124, 15]\n",
      "[149, 37, 133, 7, 71, 146, 139, 53, 44, 164, 126, 60, 119, 70, 153, 68, 85, 57, 115, 152]\n"
     ]
    }
   ],
   "source": [
    "import numpy.random\n",
    "import copy\n",
    "def test_iterative_cross_predict():\n",
    "    X = numpy.random.rand( 200, 3)\n",
    "    y = numpy.random.rand(200)\n",
    "    ionnames = numpy.random.rand(200)\n",
    "    select_idxs = numpy.random.randint(low = 0, high = 199, size = 15)\n",
    "    control_ionnames = copy.copy(ionnames[select_idxs])\n",
    "    \n",
    "\n",
    "    y_test_all, _, ionnames_all, _ = random_forest_iterative_cross_predict(X, y, ionnames, 5, RandomForestRegressor())\n",
    "\n",
    "    idxs_ionnames = [x for x in range(len(ionnames_all)) if ionnames_all[x] in control_ionnames]\n",
    "    control_idxs_y = [x for x in select_idxs if ionnames[x] in ionnames_all]\n",
    "    control_ys = y[control_idxs_y]\n",
    "\n",
    "    y_test_all_control = np.array(y_test_all)[idxs_ionnames]\n",
    "\n",
    "    assert set(control_ys).intersection(set(y_test_all_control)) == set(y_test_all_control)\n",
    "\n",
    "\n",
    "test_iterative_cross_predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import alphaquant.diffquant_utils as aqutils\n",
    "import alphaquant.visualizations as aqplot\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def assign_predictability_scores(protein_nodes, results_dir, name, samples_used,precursor_cutoff=2, fc_cutoff = 1.0, number_splits = 5, plot_predictor_performance = False, replace_nans = False, distort_precursor_modulo = np.inf, performance_metrics = {}):\n",
    "\n",
    "    #add predictability scores to each precursor\n",
    "    #prepare the input table with all the relevant features for machine learning\n",
    "    dfhandler = aqutils.AcquistionDataFrameHandler(results_dir=results_dir,samples=samples_used)\n",
    "    acquisition_info_df = get_acquisition_info_df(dfhandler)\n",
    "    node_level = get_node_level_from_dfhandler(dfhandler)\n",
    "    protein_nodes = list(sorted(protein_nodes, key  = lambda x : x.name))\n",
    "    normalized_precursors, all_precursors = get_fc_normalized_nodes(protein_nodes, node_level, precursor_cutoff, fc_cutoff, distort_precursor_modulo=distort_precursor_modulo)\n",
    "    df_precursor_features = collect_node_parameters(normalized_precursors)\n",
    "    merged_df = aqutils.merge_acquisition_df_parameter_df(acquisition_info_df, df_precursor_features)\n",
    "    \n",
    "    #transform into ML input\n",
    "    X, y, featurenames, ionnames = generate_ml_input_regression(merged_df, normalized_precursors, replace_nans=replace_nans)\n",
    "\n",
    "    test_fc_name_mapping(y, ionnames, normalized_precursors)\n",
    "\n",
    "    #predict the subset of peptides that is accessible to protein shifting (only the ones with at least 2 peps per protein)\n",
    "    regr = RandomForestRegressor()\n",
    "    y_test_cp, y_pred_cp, ionnames_cp, regr = random_forest_iterative_cross_predict(X, y, ionnames, number_splits, regr)\n",
    "    print(\"performed RF prediction\")\n",
    "    performance_metrics[\"r2_score\"] = r2_score(y_test_cp, y_pred_cp)\n",
    "    test_fc_name_mapping(y_test_cp, ionnames_cp, normalized_precursors)\n",
    "\n",
    "    #define plot outdir\n",
    "    results_dir_plots =f\"{results_dir}/{name}\" \n",
    "    aqutils.make_dir_w_existcheck(results_dir_plots)\n",
    "    if plot_predictor_performance:\n",
    "        plt.hist(y_test_cp, 60, density=True, histtype='stepfilled',cumulative=False, alpha = 0.5)\n",
    "        plt.xlim(-1.8, 1.8)\n",
    "        plt.show()\n",
    "        \n",
    "        aqplot.scatter_ml_regression_perturbation_aware(y_test=y_test_cp, y_pred = y_pred_cp, ionnames=ionnames_cp, nodes=normalized_precursors, results_dir=results_dir_plots)\n",
    "        aqplot.plot_ml_fc_histograms(y_test_cp, y_pred_cp, 0.5, results_dir_plots)\n",
    "        aqplot.plot_feature_importances(regr.feature_importances_,featurenames, 10, results_dir_plots)\n",
    "    \n",
    "    #mean, cutoff_neg, cutoff_pos = find_mean_and_cutoffs(y_pred_cp, visualize= plot_predictor_performance)\n",
    "    mean, cutoff_neg, cutoff_pos = fit_gaussian_to_subdist(y_pred_cp, visualize=plot_predictor_performance,results_dir = results_dir_plots)\n",
    "    #use the trained model to predict the remaining ions and stitch together with the already predicted ions\n",
    "    y_pred_total, ionnames_total = predict_remaining_dataset(all_precursors, regr, y_pred_cp, ionnames_cp, acquisition_info_df)\n",
    "\n",
    "    y_pred_normed = y_pred_total-mean\n",
    "\n",
    "    #add quality scores to nodes\n",
    "\n",
    "    add_quality_scores_to_node(acquisition_info_df, all_precursors)\n",
    "\n",
    "    #annotate the precursor nodes\n",
    "    annotate_precursor_nodes(cutoff_neg, cutoff_pos, y_pred_normed, ionnames_total, all_precursors) #two new variables added to each node:\n",
    "\n",
    "\n",
    "def get_acquisition_info_df(dfhandler):\n",
    "    acquisition_df = dfhandler.get_acquisition_info_df()\n",
    "    if not dfhandler.already_formatted:\n",
    "        dfhandler.save_allsample_dataframe_as_new_acquisition_dataframe()\n",
    "        dfhandler.update_ml_file_location_in_method_parameters_yaml()\n",
    "    return acquisition_df\n",
    "\n",
    "from alphaquant.cluster_ions import globally_initialized_typefilter\n",
    "def get_node_level_from_dfhandler(dfhandler):\n",
    "    return globally_initialized_typefilter.mapping_dict.get(dfhandler.last_ion_level_to_use)\n",
    "\n",
    "\n",
    "def add_quality_scores_to_node(acquisition_info_df, nodes):\n",
    "    #check if the data belongs to Spectronaut or DIANN\n",
    "    if \"FG.ShapeQualityScore\" in acquisition_info_df.columns:\n",
    "        param = \"FG.ShapeQualityScore\"\n",
    "    elif \"Quantity.Quality\" in acquisition_info_df.columns:\n",
    "        param = \"Quantity.Quality\"\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    df_avged = acquisition_info_df.groupby(\"ion\").mean().reset_index()\n",
    "    ion2param = dict(zip(df_avged[\"ion\"], df_avged[param]))\n",
    "    \n",
    "    for node in nodes:\n",
    "        node.default_quality_score = ion2param.get(node.name)\n",
    "\n",
    "def test_fc_name_mapping(fcs, ionnames, nodes):\n",
    "    number_samplings = min(math.ceil(len(fcs)/5), 30)\n",
    "    sample_idxs = random.sample(range(len(fcs)), number_samplings)\n",
    "    name2node = {x.name : x for x in nodes}\n",
    "    for idx in sample_idxs:\n",
    "        fc_ml_input = fcs[idx]\n",
    "        name_ml_input = ionnames[idx]\n",
    "        fc_node = name2node.get(name_ml_input).fc\n",
    "        assert fc_ml_input == fc_node\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
