{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp diffquant_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array/Dict reformatting and transformation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_condpairname(condpair):\n",
    "    return f\"{condpair[0]}_VS_{condpair[1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_middle_elem(sorted_list):\n",
    "    nvals = len(sorted_list)\n",
    "    if nvals==1:\n",
    "        return sorted_list[0]\n",
    "    middle_idx = nvals//2\n",
    "    if nvals%2==1:\n",
    "        return sorted_list[middle_idx]\n",
    "    return 0.5* (sorted_list[middle_idx] + sorted_list[middle_idx-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "def get_nonna_array(array_w_nas):\n",
    "    res = []\n",
    "    isnan_arr = np.isnan(array_w_nas)\n",
    "\n",
    "    for idx in range(len(array_w_nas)):\n",
    "        sub_res = []\n",
    "        sub_array = array_w_nas[idx]\n",
    "        na_array = isnan_arr[idx]\n",
    "        for idx2 in range(len(sub_array)):\n",
    "            if not na_array[idx2]:\n",
    "               sub_res.append(sub_array[idx2])\n",
    "        res.append(np.array(sub_res))\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "def get_non_nas_from_pd_df(df):\n",
    "    return {\n",
    "        pep_name: sub_vals[~np.isnan(sub_vals)] for pep_name, sub_vals in\n",
    "        zip( df.index.values, df.values)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "def get_ionints_from_pd_df(df):\n",
    "    return {\n",
    "        pep_name: sub_vals for pep_name, sub_vals in\n",
    "        zip( df.index.values, df.values)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def invert_dictionary(my_map):\n",
    "    inv_map = {}\n",
    "    for k, v in my_map.items():\n",
    "        inv_map[v] = inv_map.get(v, []) + [k]\n",
    "    return inv_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from scipy.stats import norm\n",
    "\n",
    "def get_z_from_p_empirical(p_emp,p2z):\n",
    "    p_rounded = np.format_float_scientific(p_emp, 1)\n",
    "    if p_rounded in p2z:\n",
    "        return p2z.get(p_rounded)\n",
    "    z = norm.ppf(float(p_rounded))\n",
    "    p2z[p_rounded] = z\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import anytree\n",
    "def get_levelnodes_from_nodeslist(nodeslist, level):\n",
    "    levelnodes = []\n",
    "    for node in nodeslist:\n",
    "        precursors = anytree.findall(node, filter_= lambda x : (x.type == level))\n",
    "        levelnodes.extend(precursors)\n",
    "    return levelnodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "def set_logger(\n",
    "    *,log_path = None,\n",
    "    log_file_name=\"\",\n",
    "    stream: bool = True,\n",
    "    log_level: int = logging.INFO,\n",
    "    overwrite: bool = False,\n",
    ") -> str:\n",
    "    \"\"\"Set the log stream and file.\n",
    "    All previously set handlers will be disabled with this command.\n",
    "    Parameters\n",
    "    ----------\n",
    "    log_file_name : str, None\n",
    "        The file name to where the log is written.\n",
    "        Folders are automatically created if needed.\n",
    "        This is relative to the current path. When an empty string is provided,\n",
    "        a log is written to the AlphaTims \"logs\" folder with the name\n",
    "        \"log_yymmddhhmmss\" (reversed timestamp year to seconds).\n",
    "        If None, no log file is saved.\n",
    "        Default is \"\".\n",
    "    stream : bool\n",
    "        If False, no log data is sent to stream.\n",
    "        If True, all logging can be tracked with stdout stream.\n",
    "        Default is True.\n",
    "    log_level : int\n",
    "        The logging level. Usable values are defined in Python's \"logging\"\n",
    "        module.\n",
    "        Default is logging.INFO.\n",
    "    overwrite : bool\n",
    "        If True, overwrite the log_file if one exists.\n",
    "        If False, append to this log file.\n",
    "        Default is False.\n",
    "    Returns\n",
    "    -------\n",
    "    : str\n",
    "        The file name to where the log is written.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    global PROGRESS_CALLBACK\n",
    "    log_path = os.path.join(BASE_PATH, \"logs\")\n",
    "    root = logging.getLogger()\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s> %(message)s', \"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "    root.setLevel(log_level)\n",
    "    while root.hasHandlers():\n",
    "        root.removeHandler(root.handlers[0])\n",
    "    if stream:\n",
    "        stream_handler = logging.StreamHandler(sys.stdout)\n",
    "        stream_handler.setLevel(log_level)\n",
    "        stream_handler.setFormatter(formatter)\n",
    "        root.addHandler(stream_handler)\n",
    "    if log_file_name is not None:\n",
    "        if log_file_name == \"\":\n",
    "            if not os.path.exists(log_path):\n",
    "                os.makedirs(log_path)\n",
    "            log_file_name = log_path\n",
    "        log_file_name = os.path.abspath(log_file_name)\n",
    "        if os.path.isdir(log_file_name):\n",
    "            current_time = time.localtime()\n",
    "            current_time = \"\".join(\n",
    "                [\n",
    "                    f'{current_time.tm_year:04}',\n",
    "                    f'{current_time.tm_mon:02}',\n",
    "                    f'{current_time.tm_mday:02}',\n",
    "                    f'{current_time.tm_hour:02}',\n",
    "                    f'{current_time.tm_min:02}',\n",
    "                    f'{current_time.tm_sec:02}',\n",
    "                ]\n",
    "            )\n",
    "            log_file_name = os.path.join(\n",
    "                log_file_name,\n",
    "                f\"log_{current_time}.txt\"\n",
    "            )\n",
    "        directory = os.path.dirname(log_file_name)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        if overwrite:\n",
    "            file_handler = logging.FileHandler(log_file_name, mode=\"w\")\n",
    "        else:\n",
    "            file_handler = logging.FileHandler(log_file_name, mode=\"a\")\n",
    "        file_handler.setLevel(log_level)\n",
    "        file_handler.setFormatter(formatter)\n",
    "        root.addHandler(file_handler)\n",
    "    return log_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import yaml\n",
    "import os.path\n",
    "\n",
    "def load_method_parameters(results_dir):\n",
    "    params_file = f\"{results_dir}/aq_parameters.yaml\"\n",
    "    return load_config(params_file)\n",
    "\n",
    "def store_method_parameters(local_vars, results_dir):\n",
    "    method_params = {x : local_vars[x] for x in local_vars.keys() if ((\"_df\" not in x) and ('condpair' not in x))}\n",
    "    params_file = f\"{results_dir}/aq_parameters.yaml\"\n",
    "    if os.path.exists(params_file):\n",
    "        previous_params = load_method_parameters(results_dir)\n",
    "        method_params.update(previous_params)\n",
    "    if not os.path.exists(f\"{results_dir}/\"):\n",
    "        os.makedirs(f\"{results_dir}/\")\n",
    "    with open(params_file, 'w') as outfile:\n",
    "        yaml.dump(method_params, outfile, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Parsers\n",
    "The Alphaquant pipeline is run using a generic wide-table input format, as specified in the documentation. The following parsers convert long format tables as provided e.g. by Spectronaut or DIA-NN into this generic format. The configuration for the parsers is set by a yaml file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert long format to wide format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse .yaml file\n",
    "The relevant parameters for reading and reformatting the long table are stored in the \"intable_config.yaml\" file. The functions below are for reading and reformating the config info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import yaml\n",
    "import itertools\n",
    "\n",
    "def get_relevant_columns(protein_cols, ion_cols, sample_ID, quant_ID, filter_dict):\n",
    "    filtcols = []\n",
    "    for filtconf in filter_dict.values():\n",
    "        filtcols.append(filtconf.get('param'))\n",
    "    relevant_cols = protein_cols + ion_cols + [sample_ID] + [quant_ID] + filtcols\n",
    "    relevant_cols = list(set(relevant_cols)) # to remove possible redudancies\n",
    "    return relevant_cols\n",
    "\n",
    "\n",
    "def get_relevant_columns_config_dict(config_typedict):\n",
    "    filtcols = []\n",
    "    dict_ioncols = []\n",
    "    for filtconf in config_typedict.get('filters', {}).values():\n",
    "        filtcols.append(filtconf.get('param'))\n",
    "\n",
    "    if 'ion_hierarchy' in config_typedict.keys():\n",
    "        for headr in config_typedict.get('ion_hierarchy').values():\n",
    "            ioncols = list(itertools.chain.from_iterable(headr.get(\"mapping\").values()))\n",
    "            dict_ioncols.extend(ioncols)\n",
    "    quantID = config_typedict.get(\"quant_ID\")\n",
    "    if type(quantID) ==type(\"string\"):\n",
    "        quant_ids = [config_typedict.get(\"quant_ID\")]\n",
    "    elif quantID == None:\n",
    "        quant_ids = []\n",
    "    else:\n",
    "        quant_ids = list(config_typedict.get(\"quant_ID\").values())\n",
    "    relevant_cols = config_typedict.get(\"protein_cols\") + config_typedict.get(\"ion_cols\", []) + [config_typedict.get(\"sample_ID\")] + quant_ids + filtcols + dict_ioncols\n",
    "    relevant_cols = list(set(relevant_cols)) # to remove possible redudancies\n",
    "    return relevant_cols\n",
    "\n",
    "\n",
    "def get_config_columns(config_dict):\n",
    "    protein_cols = config_dict.get(\"protein_cols\")\n",
    "    ion_cols = config_dict.get(\"ion_cols\")\n",
    "    sample_ID = config_dict.get(\"sample_ID\")\n",
    "    quant_ID = config_dict.get(\"quant_ID\")\n",
    "    filter_dict = config_dict.get(\"filters\", {})\n",
    "    relevant_cols = get_relevant_columns(protein_cols, ion_cols, sample_ID, quant_ID, filter_dict)\n",
    "    return relevant_cols, protein_cols, ion_cols, sample_ID, quant_ID, filter_dict\n",
    "\n",
    "\n",
    "def load_config(config_yaml):\n",
    "    stream = open(config_yaml, 'r')\n",
    "    config_all = yaml.safe_load(stream)\n",
    "    return config_all\n",
    "\n",
    "def get_type2relevant_cols(config_all):\n",
    "    type2relcols = {}\n",
    "    for type in config_all.keys():\n",
    "        config_typedict = config_all.get(type)\n",
    "        relevant_cols = get_relevant_columns_config_dict(config_typedict)\n",
    "        type2relcols[type] = relevant_cols\n",
    "    return type2relcols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and reformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def filter_input(filter_dict, input):\n",
    "    for filtname,filterconf in filter_dict.items():\n",
    "        param = filterconf.get('param')\n",
    "        comparator = filterconf.get('comparator')\n",
    "        value = filterconf.get('value')\n",
    "\n",
    "        if comparator not in [\">\",\">=\", \"<\", \"<=\", \"==\", \"!=\"]:\n",
    "            raise TypeError(f\"cannot identify the filter comparator of {filtname} given in the longtable config yaml!\")\n",
    "\n",
    "        if comparator==\"==\":\n",
    "            input = input[input[param] ==value]\n",
    "            continue\n",
    "        try:\n",
    "            input = input.astype({f\"{param}\" : \"float\"})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if comparator==\">\":\n",
    "            input = input[input[param].astype(type(value)) >value]\n",
    "\n",
    "        if comparator==\">=\":\n",
    "            input = input[input[param].astype(type(value)) >=value]\n",
    "\n",
    "        if comparator==\"<\":\n",
    "            input = input[input[param].astype(type(value)) <value]\n",
    "\n",
    "        if comparator==\"<=\":\n",
    "            input = input[input[param].astype(type(value)) <=value]\n",
    "\n",
    "        if comparator==\"!=\":\n",
    "            input = input[input[param].astype(type(value)) !=value]\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def merge_protein_and_ion_cols(input_df, config_dict):\n",
    "    protein_cols =  config_dict.get(\"protein_cols\")\n",
    "    ion_cols = config_dict.get(\"ion_cols\")\n",
    "    input_df['protein'] = input_df.loc[:, protein_cols].astype('string').sum(axis=1)\n",
    "    input_df['ion'] = input_df.loc[:, ion_cols].astype('string').sum(axis=1)\n",
    "    input_df = input_df.rename(columns = {config_dict.get('quant_ID') : \"quant_val\"})\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import copy\n",
    "def merge_protein_cols_and_ion_dict(input_df, config_dict):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        input_df ([pandas dataframe]): longtable containing peptide intensity data\n",
    "        confid_dict ([dict[String[]]]): nested dict containing the parse information. derived from yaml file\n",
    "\n",
    "    Returns:\n",
    "        pandas dataframe: longtable with newly assigned \"protein\" and \"ion\" columns\n",
    "    \"\"\"\n",
    "    protein_cols = config_dict.get(\"protein_cols\")\n",
    "    ion_hierarchy = config_dict.get(\"ion_hierarchy\")\n",
    "    splitcol2sep = config_dict.get('split_cols')\n",
    "    quant_id_dict = config_dict.get('quant_ID')\n",
    "\n",
    "    ion_dfs = []\n",
    "    input_df['protein'] = input_df.loc[:, protein_cols].astype('string').sum(axis=1)\n",
    "\n",
    "    input_df = input_df.drop(columns = [x for x in protein_cols if x!='protein'])\n",
    "    for hierarchy_type in ion_hierarchy.keys():\n",
    "        df_subset = input_df.copy()\n",
    "        ion_hierarchy_local = ion_hierarchy.get(hierarchy_type).get(\"order\")\n",
    "        ion_headers_merged, ion_headers_grouped = get_ionname_columns(ion_hierarchy.get(hierarchy_type).get(\"mapping\"), ion_hierarchy_local) #ion headers merged is just a helper to select all relevant rows, ionheaders grouped contains the sets of ionstrings to be merged into a list eg [[SEQ, MOD], [CH]]\n",
    "        quant_columns = get_quantitative_columns(df_subset, hierarchy_type, config_dict, ion_headers_merged)\n",
    "        headers = list(set(ion_headers_merged + quant_columns + ['protein']))\n",
    "        if \"sample_ID\" in config_dict.keys():\n",
    "            headers+=[config_dict.get(\"sample_ID\")]\n",
    "        df_subset = df_subset[headers].drop_duplicates()\n",
    "\n",
    "        if splitcol2sep is not None:\n",
    "            if quant_columns[0] in splitcol2sep.keys(): #in the case that quantitative values are stored grouped in one column (e.g. msiso1,msiso2,msiso3, etc.), reformat accordingly\n",
    "                df_subset = split_extend_df(df_subset, splitcol2sep)\n",
    "            ion_headers_grouped = adapt_headers_on_extended_df(ion_headers_grouped, splitcol2sep)\n",
    "\n",
    "        #df_subset = df_subset.set_index(quant_columns)\n",
    "\n",
    "        df_subset = add_merged_ionnames(df_subset, ion_hierarchy_local, ion_headers_grouped, quant_id_dict, hierarchy_type)\n",
    "        ion_dfs.append(df_subset)\n",
    "    input_df = pd.concat(ion_dfs, ignore_index=True)\n",
    "    return input_df\n",
    "\n",
    "\n",
    "def get_quantitative_columns(input_df, hierarchy_type, config_dict, ion_headers_merged):\n",
    "    naming_columns = ion_headers_merged + ['protein']\n",
    "    if config_dict.get(\"format\") == 'longtable':\n",
    "        quantcol = config_dict.get(\"quant_ID\").get(hierarchy_type)\n",
    "        return [quantcol]\n",
    "\n",
    "    if config_dict.get(\"format\") == 'widetable':\n",
    "        quantcolumn_candidates = [x for x in input_df.columns if x not in naming_columns]\n",
    "        if \"quant_prefix\" in config_dict.keys():\n",
    "            return [x for x in quantcolumn_candidates if x.startswith(config_dict.get(\"quant_prefix\"))] # in the case that the quantitative columns have a prefix (like \"Intensity \" in MQ peptides.txt), only columns with the prefix are filtered\n",
    "        else:\n",
    "            return quantcolumn_candidates #in this case, we assume that all non-ionname/proteinname columns are quantitative columns\n",
    "\n",
    "\n",
    "def get_ionname_columns(ion_dict, ion_hierarchy_local):\n",
    "    ion_headers_merged = []\n",
    "    ion_headers_grouped = []\n",
    "    for lvl in ion_hierarchy_local:\n",
    "        vals = ion_dict.get(lvl)\n",
    "        ion_headers_merged.extend(vals)\n",
    "        ion_headers_grouped.append(vals)\n",
    "    return ion_headers_merged, ion_headers_grouped\n",
    "\n",
    "\n",
    "def adapt_headers_on_extended_df(ion_headers_grouped, splitcol2sep):\n",
    "    #in the case that one column has been split, we need to designate the \"naming\" column\n",
    "    ion_headers_grouped_copy = copy.deepcopy(ion_headers_grouped)\n",
    "    for vals in ion_headers_grouped_copy:\n",
    "        if splitcol2sep is not None:\n",
    "            for idx in range(len(vals)):\n",
    "                if vals[idx] in splitcol2sep.keys():\n",
    "                    vals[idx] = vals[idx] + \"_idxs\"\n",
    "    return ion_headers_grouped_copy\n",
    "\n",
    "def split_extend_df(input_df, splitcol2sep, value_threshold=10):\n",
    "    \"\"\"reformats data that is stored in a condensed way in a single column. For example isotope1_intensity;isotope2_intensity etc. in Spectronaut\n",
    "\n",
    "    Args:\n",
    "        input_df ([type]): [description]\n",
    "        splitcol2sep ([type]): [description]\n",
    "        value_threshold([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        Pandas Dataframe: Pandas dataframe with the condensed items expanded to long format\n",
    "    \"\"\"\n",
    "    if splitcol2sep==None:\n",
    "        return input_df\n",
    "\n",
    "    for split_col, separator in splitcol2sep.items():\n",
    "        idx_name = f\"{split_col}_idxs\"\n",
    "        split_col_series = input_df[split_col].str.split(separator)\n",
    "        input_df = input_df.drop(columns = [split_col])\n",
    "\n",
    "        input_df[idx_name] = [list(range(len(x))) for x in split_col_series]\n",
    "        exploded_input = input_df.explode(idx_name)\n",
    "        exploded_split_col_series = split_col_series.explode()\n",
    "\n",
    "        exploded_input[split_col] = exploded_split_col_series.replace('', 0) #the column with the intensities has to come after to column with the idxs\n",
    "\n",
    "        exploded_input = exploded_input.astype({split_col: float})\n",
    "        exploded_input = exploded_input[exploded_input[split_col]>value_threshold]\n",
    "        #exploded_input = exploded_input.rename(columns = {'var1': split_col})\n",
    "    return exploded_input\n",
    "\n",
    "\n",
    "\n",
    "def add_merged_ionnames(df_subset, ion_hierarchy_local, ion_headers_grouped, quant_id_dict, hierarchy_type):\n",
    "    all_ion_headers = list(itertools.chain.from_iterable(ion_headers_grouped))\n",
    "    columns_to_index = [x for x in df_subset.columns if x not in all_ion_headers]\n",
    "    df_subset = df_subset.set_index(columns_to_index)\n",
    "\n",
    "    rows = df_subset[all_ion_headers].to_numpy()\n",
    "    ions = []\n",
    "\n",
    "    for row in rows: #iterate through dataframe\n",
    "        count = 0\n",
    "        ionstring = \"\"\n",
    "        for lvl_idx in range(len(ion_hierarchy_local)):\n",
    "            ionstring += f\"{ion_hierarchy_local[lvl_idx]}\"\n",
    "            for sublvl in ion_headers_grouped[lvl_idx]:\n",
    "                ionstring+= f\"_{row[count]}_\"\n",
    "                count+=1\n",
    "        ions.append(ionstring)\n",
    "    df_subset['ion'] = ions\n",
    "    df_subset = df_subset.reset_index()\n",
    "    if quant_id_dict!= None:\n",
    "        df_subset = df_subset.rename(columns = {quant_id_dict.get(hierarchy_type) : \"quant_val\"})\n",
    "    return df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def reformat_longtable_according_to_config_new(input_file, config_dict, conditions_subset = None, sep = \"\\t\",decimal = \".\"):\n",
    "    \"\"\"Reshape a long format proteomics results table (e.g. Spectronaut or DIA-NN) to a wide format table.\n",
    "    :param file input_file: long format proteomic results table\n",
    "    :param string input_type: the configuration key stored in the config file (e.g. \"diann_precursor\")\n",
    "    \"\"\"\n",
    "    relevant_cols = get_relevant_columns_config_dict(config_dict)\n",
    "    input_df_it = pd.read_csv(input_file, sep = sep, decimal=decimal, usecols = relevant_cols, encoding ='latin1', chunksize=1000000)\n",
    "    input_df_list = []\n",
    "    for input_df_subset in input_df_it:\n",
    "        input_df_subset = filter_input(config_dict.get(\"filters\", {}), input_df_subset)\n",
    "        if conditions_subset !=None:\n",
    "            input_df_subset = input_df_subset[[(x in conditions_subset) for x in input_df_subset[config_dict.get(\"sample_id\")]]]\n",
    "            if len(input_df_subset.index)==0:\n",
    "                continue\n",
    "        if \"ion_hierarchy\" in config_dict.keys():\n",
    "            input_df_subset = merge_protein_cols_and_ion_dict(input_df_subset, config_dict)\n",
    "        else:\n",
    "            input_df_subset = merge_protein_and_ion_cols(input_df_subset, config_dict)\n",
    "        input_df_list.append(input_df_subset)\n",
    "\n",
    "\n",
    "    input_df = pd.concat(input_df_list)\n",
    "\n",
    "    input_df = input_df.astype({'quant_val': 'float'})\n",
    "    input_reshaped = pd.pivot_table(input_df, index = ['protein', 'ion'], columns = config_dict.get(\"sample_ID\"), values = 'quant_val', fill_value=0)\n",
    "    if input_reshaped.iloc[:,0].replace(0, np.nan).median() <100: #when values are small, rescale by a constant factor to prevent rounding errors in the subsequent aq analyses\n",
    "        input_reshaped = input_reshaped *10000\n",
    "\n",
    "    input_reshaped = input_reshaped.reset_index()\n",
    "\n",
    "    return input_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_wideformat_table(peptides_tsv, config_dict):\n",
    "    input_df = pd.read_csv(peptides_tsv,sep=\"\\t\", encoding ='latin1')\n",
    "    filter_dict = config_dict.get(\"filters\")\n",
    "    protein_cols = config_dict.get(\"protein_cols\")\n",
    "    ion_cols = config_dict.get(\"ion_cols\")\n",
    "    input_df = filter_input(filter_dict, input_df)\n",
    "    #input_df = merge_protein_and_ion_cols(input_df, config_dict)\n",
    "    input_df = merge_protein_cols_and_ion_dict(input_df, config_dict)\n",
    "    if 'quant_prefix' in config_dict.keys():\n",
    "        quant_prefix = config_dict.get('quant_prefix')\n",
    "        headers = ['protein', 'ion'] + list(filter(lambda x: x.startswith(quant_prefix), input_df.columns))\n",
    "        input_df = input_df[headers]\n",
    "        input_df = input_df.rename(columns = lambda x : x.replace(quant_prefix, \"\"))\n",
    "\n",
    "    input_df = input_df.reset_index()\n",
    "\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_mq_peptides_table(peptides_tsv, pepheader = \"Sequence\", protheader = \"Leading razor protein\"):\n",
    "    peps = pd.read_csv(peptides_tsv,sep=\"\\t\", encoding ='latin1')\n",
    "    peps = peps[peps[\"Reverse\"] != \"+\"]\n",
    "    peps = peps[peps[\"Potential contaminant\"] != \"+\"]\n",
    "    if pepheader != None:\n",
    "        peps = peps.rename(columns = {pepheader : \"ion\"})\n",
    "    if protheader != None:\n",
    "        peps = peps.rename(columns = {protheader: \"protein\"})\n",
    "    headers = ['protein', 'ion'] + list(filter(lambda x: x.startswith(\"Intensity \"), peps.columns))\n",
    "    peps = peps[headers]\n",
    "    peps = peps.rename(columns = lambda x : x.replace(\"Intensity \", \"\"))\n",
    "\n",
    "    return peps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from anytree.importer import JsonImporter\n",
    "import os\n",
    "\n",
    "def read_condpair_tree(cond1, cond2, results_folder = os.path.join(\".\", \"results\")):\n",
    "    \"\"\"reads the merged and clustered iontree for a given condpair\"\"\"\n",
    "    condpairname = get_condpairname([cond1, cond2])\n",
    "    tree_file =os.path.join(results_folder, f\"{condpairname}.iontrees.json\")\n",
    "    if not os.path.isfile(tree_file):\n",
    "        return None\n",
    "    importer = JsonImporter()\n",
    "    filehandle = open(tree_file, 'r')\n",
    "    jsontree = importer.read(filehandle)\n",
    "    filehandle.close()\n",
    "    return jsontree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import numpy as np\n",
    "import alphaquant.diffquant_utils as aqutils\n",
    "import os\n",
    "\n",
    "def merge_ptmsite_mappings_write_table(spectronaut_file, mapped_df, modification_type, ptm_type_config_dict = 'spectronaut_ptm_fragion_isotopes'):\n",
    "    config_dict = aqutils.import_config_dict()\n",
    "    config_dict_ptm = config_dict.get(ptm_type_config_dict)\n",
    "    relevant_columns = aqutils.get_relevant_columns_config_dict(config_dict_ptm)\n",
    "    specnaut_df_it = pd.read_csv(spectronaut_file, sep = \"\\t\", chunksize=10000)\n",
    "    ptmmapped_table_filename = f'{spectronaut_file.replace(\".tsv\", \"\")}_ptmsite_mapped.tsv'\n",
    "    header = True\n",
    "    for specnaut_df in specnaut_df_it:\n",
    "        specnaut_df_annot = add_ptmsite_info_to_subtable(specnaut_df, mapped_df, modification_type, relevant_columns)\n",
    "        write_chunk_to_file(specnaut_df_annot, ptmmapped_table_filename, header)\n",
    "        header = False\n",
    "    \n",
    "\n",
    "def add_ptmsite_info_to_subtable(spectronaut_df, mapped_df, modification_type, relevant_columns):\n",
    "    labelid2ptmid, labelid2site = get_ptmid_mappings(mapped_df) #get precursor+experiment to site mappings\n",
    "    labelid_spectronaut = spectronaut_df[\"R.Label\"].astype('str').to_numpy() + spectronaut_df[\"FG.Id\"].astype('str').to_numpy() #derive the id to map from Spectronaut\n",
    "    spectronaut_df[\"ptm_id\"] = np.array([labelid2ptmid.get(x, np.nan) for x in labelid_spectronaut]) #add the ptm_id row to the spectronaut table\n",
    "    modseq_typereplaced = np.array([str(x.replace(modification_type, \"\")) for x in spectronaut_df[\"EG.ModifiedSequence\"]]) #EG.ModifiedSequence already determines a localization of the modification type. Replace all localizations and add the new localizations below\n",
    "    sites = np.array([str(labelid2site.get(x)) for x in labelid_spectronaut])\n",
    "    spectronaut_df[\"ptm_mapped_modseq\"] = np.char.add(modseq_typereplaced, sites)\n",
    "    spectronaut_df = spectronaut_df.dropna(subset=[\"ptm_id\"]) #drop peptides that have no ptm\n",
    "    spectronaut_df = spectronaut_df[relevant_columns]\n",
    "    return spectronaut_df\n",
    "\n",
    "\n",
    "def get_ptmid_mappings(mapped_df):\n",
    "    labelid = mapped_df[\"R.Label\"].astype('str').to_numpy() + mapped_df[\"FG.Id\"].astype('str').to_numpy()\n",
    "    ptm_ids = mapped_df[\"ptm_id\"].to_numpy()\n",
    "    site = mapped_df[\"site\"].to_numpy()\n",
    "    labelid2ptmid = dict(zip(labelid, ptm_ids))\n",
    "    labelid2site = dict(zip(labelid, site))\n",
    "    return labelid2ptmid, labelid2site\n",
    "\n",
    "\n",
    "def write_chunk_to_file(chunk, filepath ,write_header):\n",
    "    chunk.to_csv(filepath, header=write_header, mode='a', sep = \"\\t\", index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check for already processed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "def check_for_processed_runs_in_results_folder(results_folder):\n",
    "    contained_condpairs = []\n",
    "    folder_files = os.listdir(results_folder)\n",
    "    result_files = list(filter(lambda x: \"results.tsv\" in x ,folder_files))\n",
    "    for result_file in result_files:\n",
    "        res_name = result_file.replace(\".results.tsv\", \"\")\n",
    "        if ((f\"{res_name}.normed.tsv\" in folder_files) & (f\"{res_name}.results.ions.tsv\" in folder_files)):\n",
    "            contained_condpairs.append(res_name)\n",
    "    return contained_condpairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "def import_data(input_file, input_type_to_use = None, conditions_subset = None):\n",
    "    \"\"\"\n",
    "    Function to import peptide level data. Depending on available columns in the provided file,\n",
    "    the function identifies the type of input used (e.g. Spectronaut, MaxQuant, DIA-NN), reformats if necessary\n",
    "    and returns a generic wide-format dataframe\n",
    "    :param file input_file: quantified peptide/ion -level data\n",
    "    :param file results_folder: the folder where the AlphaQuant outputs are stored\n",
    "    \"\"\"\n",
    "    if \"aq_reformat\" in input_file:\n",
    "        data = pd.read_csv(input_file, sep = \"\\t\", encoding ='latin1')\n",
    "        return data\n",
    "\n",
    "    input_type, config_dict_type, sep = get_input_type_and_config_dict(input_file, input_type_to_use)\n",
    "    format = config_dict_type.get('format')\n",
    "\n",
    "    if format == \"longtable\":\n",
    "        data = reformat_longtable_according_to_config_new(input_file, config_dict_type, conditions_subset, sep = sep)\n",
    "    elif format == \"widetable\":\n",
    "        data = read_wideformat_table(input_file, config_dict_type)\n",
    "    else:\n",
    "        raise Exception('Format not recognized!')\n",
    "\n",
    "    if conditions_subset !=None:\n",
    "        conditions_subset = sorted(conditions_subset)\n",
    "        conditions_subset_string = \"_\".join(conditions_subset)\n",
    "        input_type+=conditions_subset_string\n",
    "    data.to_csv(f\"{input_file}.{input_type}.aq_reformat.tsv\",  index = False,sep = \"\\t\")\n",
    "\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import pathlib\n",
    "\n",
    "def get_input_type_and_config_dict(input_file, input_type_to_use = None):\n",
    "    #parse the type of input (e.g. Spectronaut Fragion+MS1Iso) out of the input file\n",
    "\n",
    "\n",
    "    config_file = os.path.join(pathlib.Path(__file__).parent.absolute(), \"..\", \"intable_config.yaml\") #the yaml config is located one directory below the python library files\n",
    "    config_dict = load_config(config_file)\n",
    "    type2relevant_columns = get_type2relevant_cols(config_dict)\n",
    "\n",
    "    input_file = input_file.replace(\".aq_reformat.tsv\", \"\")\n",
    "\n",
    "    filename = str(input_file)\n",
    "    if '.csv' in filename:\n",
    "        sep=','\n",
    "    if '.tsv' in filename:\n",
    "        sep='\\t'\n",
    "    if '.txt' in filename:\n",
    "        sep='\\t'\n",
    "\n",
    "    if 'sep' not in locals():\n",
    "        raise TypeError(f\"neither of the file extensions (.tsv, .csv, .txt) detected for file {input_file}! Your filename has to contain one of these extensions. Please modify your file name accordingly.\")\n",
    "\n",
    "\n",
    "\n",
    "    uploaded_data_columns = set(pd.read_csv(input_file, sep=sep, nrows=1, encoding ='latin1').columns)\n",
    "\n",
    "    for input_type in type2relevant_columns.keys():\n",
    "        if (input_type_to_use is not None) and (input_type!=input_type_to_use):\n",
    "            continue\n",
    "        relevant_columns = type2relevant_columns.get(input_type)\n",
    "        relevant_columns = [x for x in relevant_columns if x] #filter None values\n",
    "        print(f\"recols {input_type}\\t {relevant_columns}\")\n",
    "        if set(relevant_columns).issubset(uploaded_data_columns):\n",
    "            config_dict_type =  config_dict.get(input_type)\n",
    "            return input_type, config_dict_type, sep\n",
    "    raise TypeError(\"format not specified in intable_config.yaml!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def import_config_dict():\n",
    "    config_file = os.path.join(pathlib.Path(__file__).parent.absolute(), \"..\", \"intable_config.yaml\") #the yaml config is located one directory below the python library files\n",
    "    config_dict = load_config(config_file)\n",
    "    return config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "\n",
    "def get_samplenames(data):\n",
    "    \"\"\"extracts the names of the samples of the AQ input dataframe\"\"\"\n",
    "    names = list(data.columns)\n",
    "    names.remove('protein')\n",
    "    names.remove('ion')\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_samplemap(samplemap_file):\n",
    "    file_ext = os.path.splitext(samplemap_file)[-1]\n",
    "    if file_ext=='.csv':\n",
    "        sep=','\n",
    "    if (file_ext=='.tsv') | (file_ext=='.txt'):\n",
    "        sep='\\t'\n",
    "\n",
    "    if 'sep' not in locals():\n",
    "        print(f\"neither of the file extensions (.tsv, .csv, .txt) detected for file {samplemap_file}! Trying with tab separation. In the case that it fails, please add the appropriate extension to your file name.\")\n",
    "        sep = \"\\t\"\n",
    "\n",
    "    return pd.read_csv(samplemap_file, sep = sep, encoding ='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def prepare_loaded_tables(data_df, samplemap_df):\n",
    "    \"\"\"\n",
    "    Integrates information from the peptide/ion data and the samplemap, selects the relevant columns and log2 transforms intensities.\n",
    "    \"\"\"\n",
    "    samplemap_df = samplemap_df[samplemap_df[\"condition\"]!=\"\"] #remove rows that have no condition entry\n",
    "    filtvec_not_in_data = [(x in data_df.columns) for x in samplemap_df[\"sample\"]] #remove samples that are not in the dataframe\n",
    "    samplemap_df = samplemap_df[filtvec_not_in_data]\n",
    "    headers = ['protein'] + samplemap_df[\"sample\"].to_list()\n",
    "    data_df = data_df.set_index(\"ion\")\n",
    "    for sample in samplemap_df[\"sample\"]:\n",
    "        data_df[sample] = np.log2(data_df[sample].replace(0, np.nan))\n",
    "    return data_df[headers], samplemap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def import_acquisition_info_df(results_dir, last_ion_level_to_use = \"CHARGE\", header_filter = lambda x : (\"EG.\" in x) | (\"FG.\" in x), sep = \"\\t\",decimal = \".\"):\n",
    "    \"\"\"import tables containing details on the acquisition (e.g. the Spectronaut input table)\n",
    "    \"\"\"\n",
    "    method_params_dict = load_method_parameters(results_dir)\n",
    "    input_file = method_params_dict.get('input_file')\n",
    "    config_dict = get_input_type_and_config_dict(input_file)[1]\n",
    "\n",
    "    df_sample = pd.read_csv(input_file, sep = sep, decimal = decimal, encoding='latin1', nrows=3000)\n",
    "\n",
    "    df_sample = df_sample.replace({False: 0, True: 1})\n",
    "    ion_headers, ion_levels = get_ion_headers_from_config_dict(config_dict, last_ion_level_to_use)\n",
    "    \n",
    "    numeric_headers =  list(df_sample.select_dtypes(include=np.number).columns)\n",
    "\n",
    "    filtered_numeric_headers = [x for x in numeric_headers if header_filter(x)]\n",
    "    header_subset = get_all_ion_headers(ion_headers) + filtered_numeric_headers\n",
    "\n",
    "    input_df_it = pd.read_csv(input_file, sep = sep, decimal=decimal, usecols = header_subset, encoding ='latin1', chunksize=1000000)\n",
    "    input_df_list = []\n",
    "    for input_df_subset in input_df_it:\n",
    "        input_df_subset = input_df_subset.drop_duplicates()\n",
    "\n",
    "        input_df_list.append(input_df_subset)\n",
    "\n",
    "    input_df = pd.concat(input_df_list)\n",
    "    input_df = add_merged_ionnames(input_df, ion_levels, ion_headers, None, None)\n",
    "    #input_df[\"ion\"] = get_ion_row(input_df, ion_headers, ion_levels)\n",
    "\n",
    "\n",
    "    return input_df\n",
    "\n",
    "def get_ion_headers_from_config_dict(config_dict, last_ion_level_to_use):\n",
    "    ion_headers = []\n",
    "    ion_levels = []\n",
    "    ion_hierarchy = config_dict.get(\"ion_hierarchy\")\n",
    "    hier_key = 'fragion' if 'fragion' in ion_hierarchy.keys() else list(ion_hierarchy.keys())[0]\n",
    "    ion_hierarchy = ion_hierarchy.get(hier_key) #chose the first ion hierarchy object at random\n",
    "    for idx in range(len(ion_hierarchy.get('order'))):\n",
    "        ion_level = ion_hierarchy.get('order')[idx]\n",
    "        ion_levels.append(ion_level)\n",
    "        ion_headers.append(ion_hierarchy.get('mapping').get(ion_level))\n",
    "        if ion_level == last_ion_level_to_use:\n",
    "            break\n",
    "    return ion_headers, ion_levels\n",
    "\n",
    "def get_all_ion_headers(ion_headers):\n",
    "    #ion headers is a dict of dict, merge all content of the ion headers\n",
    "    all_ion_headers = []\n",
    "    for ion_header in ion_headers:\n",
    "        all_ion_headers.extend(ion_header)\n",
    "    return all_ion_headers\n",
    "\n",
    "def get_ion_row(df, ion_headers, ion_levels):\n",
    "    all_ion_headers = get_all_ion_headers(ion_headers)\n",
    "    df_subset_np = df[all_ion_headers].to_numpy()\n",
    "    ionnames = [get_ion_header(row, ion_headers, ion_levels) for row in df_subset_np]\n",
    "    return ionnames\n",
    "\n",
    "def get_ion_header(df_row_np, ion_headers ,ion_levels):\n",
    "    ion = \"\"\n",
    "    idx_column = 0\n",
    "    for idx_level in range(len(ion_levels)):\n",
    "        level = str(ion_levels[idx_level]) + \"_\"\n",
    "        name = \"\"\n",
    "        for idx_sublevel in range(len(ion_headers[idx_level])): #in the case that multiple columns determine one level, e.g. the level \"FRAGION\" is determined by the columns for fragment ion, losses, and fragion charge state\n",
    "            name += str(df_row_np[idx_column]) + \"_\"\n",
    "            idx_column+=1\n",
    "        ion+=level + name\n",
    "    return ion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def merge_acquisition_df_parameter_df(acquisition_df, parameter_df, groupby_merge_type = 'mean'):\n",
    "    \"\"\"acquisition df contains details on the acquisition, parameter df are the parameters derived from the tree\n",
    "    \"\"\"\n",
    "    merged_df = parameter_df.merge(acquisition_df, how = 'left', on = 'ion')\n",
    "    if groupby_merge_type == 'mean':\n",
    "        merged_df = merged_df.groupby('ion').mean().reset_index()\n",
    "    if groupby_merge_type == 'min':\n",
    "        merged_df = merged_df.groupby('ion').min().reset_index()\n",
    "    if groupby_merge_type == 'max':\n",
    "        merged_df = merged_df.groupby('ion').max().reset_index()\n",
    "    merged_df = merged_df.dropna(axis=1, how='all')\n",
    "    return merged_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
